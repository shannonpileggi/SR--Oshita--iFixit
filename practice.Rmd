---
title: "Practicing with the data"
author: "Lisa Oshita"
date: "June 29, 2017"
output:
  html_document: default
  pdf_document: default
---
#### Importing the data/Setting up time to event variable
```{r}

dir <- file.path(getwd(),"data")
out <- read.csv(file.path(dir, "answers_data.csv"))

library(dplyr)
out_english <- out %>% 
                tbl_df() %>%
                filter(langid == "en")

out_english$time_until_answer <- (out_english$first_answer_date - out_english$post_date)/3600
empty <- which(is.na(out_english$time_until_answer))
for (i in empty) {
  out_english$time_until_answer[i] <- (out_english$download_date[i] - out_english$post_date[i])/3600
}

proportion_answered <- sum(out_english$answered)/nrow(out_english)
```

#### About the data set
* `r nrow(out_english)` observations
* `r proportion_answered * 100` % of questions were answered 

#### time until answer 
* minimum: `r summary[1]` hrs
* maximum: `r summary[6]` hrs
* mean: `r summary[4]` hrs
* median: `r summary[3]` hrs
* first quartile: `r summary[2]`hrs
    + 25% of the questions were answered by `r summary[2]` hrs
* third quartile: `r summary[5]` hrs
    + 75% of the questions were answered by `r summary[5]` hrs
* IQR range: `r summary[5] - summary[2]` hrs

```{r}
summary <- summary(out_english$time_until_answer)

library(ggplot2)

ggplot(out_english, aes(x = time_until_answer)) + 
  geom_histogram(bins = 10) + 
  scale_x_continuous("Time (hours)", labels = comma) 
```

### investigating whether a question mark is present or not
```{r}
library(stringr)
library(rebus)

#variable indicating if title contains questionmark 
out_english$contain_questionmark <- str_detect(out_english$title, pattern = QUESTION)
sum(out_english$contain_questionmark)

#view average time_until_answered grouped by whether or not question contains "?", for each category
#seems like for a majority of the categories, questions with question marks get answered faster
out_english %>%
  group_by(category, contain_questionmark) %>%
  summarise(avg_time = mean(time_until_answer, na.rm = TRUE), median =
              median(time_until_answer, na.rm = TRUE), num = n()) %>%
  arrange(category, avg_time)

#calculating proportion of questions that contain a question mark that were answered
q <- out_english %>%
        filter(contain_questionmark == T)
sum(q$answered)/nrow(q)

#proportion of questions that contain ? for each category
prop.table(table(out_english$category, out_english$contain_questionmark), 1)
```

### searching for "tried" and "searched" in text variable
```{r}
out_english$tried <- str_detect(as.character(out_english$text), pattern = or("tried", "searched"))
sum(out_english$tried)/nrow(out_english)

out_english %>%
  group_by(tried) %>%
  summarise(avg_time = mean(time_until_answer), avg_daily = mean(daily_views), n = n()) %>%
  arrange(avg_time)

```


### exploring the categories 
```{r}
table(out_english$category)

#view average time until answered grouped by each category 
out_english%>%
  group_by(category) %>%
  summarise(avg_time = mean(time_until_answer, na.rm = TRUE), median_time = median(time_until_answer, na.rm = TRUE), num = n()) %>%
  arrange(avg_time)

#looking at time_until_answer distributions according to Mac, PC, Phone, showing whether or not the question contains a question mark
ggplot(subset(out_english, category %in% c("Mac", "PC", "Phone")), aes(x = time_until_answer, fill = as.factor(contain_questionmark))) +
  geom_histogram(bins = 10) + 
  facet_grid(.~category)

#looking at only data for phones category (nothing interesting here)
phones_only <- out_english %>% filter(category == "Phone")
ggplot(phones_only, aes(x = str_length(title), y = time_until_answer)) + 
  geom_point(size = 3, alpha = 0.5)
```

### exploring title length
```{r}
#creating title_length variable
out_english$title_length <- str_length(out_english$title)
hist(out_english$title_length)
summary(out_english$title_length)

#viewing average/median title length for each category and comparing it to average time until answer
out_english %>%
  group_by(category) %>%
  summarise(avg_length = mean(title_length), median_length = median(title_length), avg_time = mean(time_until_answer), n = n()) %>%
  arrange(avg_time)

#creating title length intervals
# out_english$title_length_intervals <- cut(out_english$title_length, breaks = 5)
# #don't really see anything interesting here 
# out_english %>%
#   group_by(title_length_intervals) %>%
#   summarise(avg_time = mean(time_until_answer, na.rm = TRUE), num = n())

#plot with title length 
#randomly subset half of the data set to make easier to read
#don't see any association 
ggplot(sample_frac(out_english, size = 0.50, replace = FALSE), aes(x = title_length, y = time_until_answer)) + 
  geom_point()
```

### exploring text length
```{r}
out_english$text_length <- str_length(out_english$text)
hist(out_english$text_length, xlim = c(0,3000))
summary(out_english$text_length)

#examining avg/median text length and comparing it to avg time_until_answer for each category
out_english %>%
  group_by(category) %>%
  summarise(avg = mean(text_length), median = median(text_length), median_time = median(time_until_answer), n = n()) %>%
  arrange(median_time)
```

### exploring device length 
*observation 7548 has a 136 character string for it's device
```{r}
out_english$device_length <- str_length(out_english$device)
hist(out_english$device_length)
summary(out_english$device_length)

#don't see anything interesting here
out_english %>% 
  group_by(category) %>%
  summarise(avg = mean(device_length), median = median(device_length), avg_time = mean(time_until_answer), n = n()) %>%
  arrange(avg_time)

```

### examining the number of tags for a question
```{r}
summary(out_english$n_tags)

#proportion of questions that don't have any tags
sum(out_english$n_tags == 0)/nrow(out_english)
table(out_english$n_tags)

#viewing average time_until_answer grouped by number of tags and category
out_english %>%
  group_by(as.factor(n_tags)) %>%
  summarise(avg_time = mean(time_until_answer), n = n()) %>%
  arrange(avg_time)
  
#plot of number of tags and time_until_answer (nothing interesting here)
ggplot(out_english, aes(x = factor(n_tags), y = time_until_answer)) + 
  geom_point(position = "jitter")
```

### number of pictures included 
```{r}
# summary(out_english$n_images)
# hist(out_english$n_images)
# table(out_english$n_images)
# 
# #proportion of questions that include any images
# sum(out_english$n_images != 0)/nrow(out_english)
# 
# #looking at mean time_until_answer grouped by n_images
# out_english %>% 
#   group_by(as.factor(n_images)) %>%
#   summarise(avg_time = mean(time_until_answer), avg_daily = mean(daily_views), n = n()) %>%
#   arrange(avg_time)
# 
# #looking at mean time_until_answer grouped by categories and n_images
# out_english %>%
#   group_by(category, as.factor(n_images)) %>%
#   summarise(avg_time = mean(time_until_answer), median = median(time_until_answer), n = n()) %>%
#   arrange(category, avg_time)
# 
# #looking at whether or not a question included at least one pic
# #creating the variable
# sum(out_english$pic_included)/length(out_english$pic_included)
# 
# out_english$pic_included <- 0
# out_english$pic_included[out_english$n_images != 0] <- 1
# 
# out_english %>%
#   group_by(as.factor(pic_included)) %>%
#   summarise(avg_time = mean(time_until_answer), avg_daily = mean(daily_views), n = n()) %>%
#   arrange(avg_time)

```

### examining if being a new user makes a difference in time_until_answer
```{r}
prop.table(table(as.factor(out_english$new_user)))

#looking at average/median time until answer for new/continuing users
out_english %>% 
  group_by(as.factor(new_user)) %>%
  summarise(avg_time = mean(time_until_answer), median = median(time_until_answer), n = n()) %>%
  arrange(avg_time)

#viewing histogram for new/continuing users
ggplot(out_english, aes(x = time_until_answer)) +
  geom_histogram(bins = 10) + 
  facet_grid(.~as.factor(new_user))
```

### examining total views for a question
```{r}
summary(out_english$views)
hist(out_english$views) #very skewed right
summary(out_english$daily_views)

#looking at avg time_until_answer, avg total view count, and median views grouped by number of tags
out_english %>% 
  tbl_df %>%
  group_by(as.factor(n_tags)) %>%
  summarise(avg_views = mean(views), median_views = median(views), avg_time = mean(time_until_answer)) %>%
  arrange(avg_views)

#which product categories get the most views
out_english %>%
  tbl_df() %>%
  group_by(category) %>%
  summarise(avg_views = mean(views), median_views = median(views), n = n()) %>%
  arrange(desc(avg_views))
```

### examining daily_views
```{r}
hist(out_english$daily_views)
summary(out_english$daily_views)
#determining whcih categories have the most avg daily views 
out_english %>%
  tbl_df() %>%
  group_by(category) %>%
  summarise(avg_dailyviews = mean(daily_views), median = median(daily_views), n = n()) %>%
  arrange(desc(avg_dailyviews))

out_english %>% 
  tbl_df %>%
  group_by(as.factor(n_tags)) %>%
  summarise(avg_dailyviews = mean(daily_views), median = median(daily_views), avg_time = mean(time_until_answer), n = n()) %>%
  arrange(desc(avg_dailyviews))

#indicator variable for whether or not the question was categorized correctly
# out_english$categorized <- 0
# out_english$categorized[!is.na(out_english$category)] <- 1
# out_english %>%
#   tbl_df %>%
#   group_by(as.factor(categorized)) %>%
#   summarise(avg_dailyviews = mean(daily_views), median = median(daily_views), avg_time = mean(time_until_answer), n = n()) %>%
#   arrange(desc(avg_dailyviews))
```


### practicing with text mining
```{r}
# library(dplyr)
# #working only with the 2000 questions that were answered the fastest
# arranged <- out_english %>% arrange(time_until_answer)
# fastest <- arranged[1:2000,]
# 
# library(qdap)
# library(tm)
# RWEKA NOT INSTALLING!!
#library(RWeka)
# 
# titles <- fastest$title
# 
# #clean titles 
# qdap_clean <- function(x) {
#   x <- replace_abbreviation(x)
#   x <- replace_contraction(x)
#   x <- replace_number(x)
#   x <- replace_ordinal(x)
#   x <- replace_symbol(x)
#   x <- tolower(x)
#   return(x)
# }
# tm_clean <- function(corpus) {
#   tm_clean <- tm_map(corpus, removePunctuation)
#   corpus <- tm_map(corpus, stripWhitespace)
#   corpus <- tm_map(corpus, removeWords, stopwords("en"))
#   return(corpus)
# }
# 
# tokenizer <- function(x) {
#   NGramTokenizer(x, Weka_control(min = 2, max = 3)) }
# 
# titles <- qdap_clean(titles)
# titles_cor <- VCorpus(VectorSource(titles))
# titles_corpus <- tm_clean(titles_cor)
# 
# titles_tdm <- TermDocumentMatrix(titles_corpus, )
```










