---
title: "New variables"
author: "Lisa Oshita"
date: "July 14, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### importing the data/setting up variables
```{r}
dir <- file.path(getwd(),"data")
out <- read.csv(file.path(dir, "answers_data.csv"))


out$time_until_answer <- out$first_answer_date - out$post_date
#filling in the NAs
empty <- which(is.na(out$time_until_answer))
for (i in empty) {
  out$time_until_answer[i] <- out$download_date[i] - out$post_date[i]
}


library(dplyr)
out_english <- out %>% 
                tbl_df() %>% 
                filter(langid == "en")
```

### finding most frequently used terms
```{r}
library(qdap)
library(tm)
#library(RWeka)

#function to clean the corpus
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "can", "will", "cant", "wont", "works", "get", "help", "need"))
  return(corpus)
}

freq_terms <- function(vec) {
  source <- VectorSource(vec)
  corpus <- VCorpus(source)
  cleaned_corpus <- clean_corpus(corpus) 
  dtm <- DocumentTermMatrix(cleaned_corpus)
  m <- as.matrix(dtm) 
  freq <- colSums(m)
  freq <- sort(freq, decreasing = TRUE)
  return(freq)
}

titles_freq <- freq_terms(out_english$title)
head(titles_freq)
#this function changes "turn on" to just "turn", in our case "turn on" might be a significant term? -- do something about this in the function? like do bigrams? (observation 300)

top100 <- titles_freq[1:100]
top50 <- titles_freq[1:50]
top10 <- titles_freq[1:10]
plot(term_freq)
plot(top50)
```

### variable indicating if title contains at least one word in top50 words list
```{r}
library(stringr)
library(rebus)

out_english$contain_top10 <- str_detect(as.character(out_english$title), pattern = or1(names(top10)))
```

### exploring contain_top10
```{r}
sum(out_english$contain_top10)/nrow(out_english)
#41% of words contain at least one of the top 10 words

library(dplyr)
#not working correctly???
out_english %>%
  group_by(contain_top10) %>%
  summarise(median_time = median(time_until_answer), 
            avg_daily = mean(daily_views)) %>%
  arrange(median_time)


hist(unname(titles_freq))
hist(log(titles_freq))
hist(log(out_english$time_until_answer))

t.test(log(time_until_answer) ~ contain_top10, data = out_english)
```

### survival curves
```{r}
library(survival) 

surv_object <- Surv(out_english$time_until_answer, out_english$answered, type = "right")

options(scipen=5)
KM_top10 <- survfit(surv_object ~ contain_top10, data = out_english)
plot(KM_top10, lty = 1:2, xlab = "Time", ylab = "Survival Probability", main = "KM Curves for questions with and without top 10 terms")
legend("topright", c("False", "True"), lty = 1:2)

survdiff(surv_object~contain_top10, data = out_english)
```

### trying out TfIdf (counts occurrences of words but penalizes for appearing in too many documents) 
```{r}
freq_terms_weighted <- function(vec) {
  source <- VectorSource(vec)
  corpus <- VCorpus(source)
  cleaned_corpus <- clean_corpus(corpus) 
  dtm <- DocumentTermMatrix(cleaned_corpus, control = list(weighting = function(x)
    weightTfIdf(x, normalize = TRUE)))
  m <- as.matrix(dtm) 
  freq <- colSums(m)
  freq <- sort(freq, decreasing = TRUE)
  return(freq)
}

titles_freq_weighted <- freq_terms_weighted(out_english$title)
top10_weighted <- term_freq_weighted[1:10]
top10_weighted
top50_weighted <- term_freq_weighted[1:50]

hist(titles_freq_weighted)

#weighting didn't do much
```

### visualizing the frequency of the words 

also look at the distribution of how often the appear (plot of 100 words verses percent of posts appeared in) 
top 20 wordsâ€”might be in 80% of the words
add up words cumulatively? use a cumulative function in r, make own table manually (word number vs percent post appears in , then use cumulative function on that table to create plot) 
```{r}

plot(x = c(1:50), unname(top50)/nrow(out_english), xlab = "Top 10 terms", ylab = "Percentage of posts appeared in", xaxt = "n")
axis(1, at=1:50, labels=names(top50), las = 2)
#screen appears in 12% of questions

#cumulative distribution
cum_percent <- cumsum(unname(top10))
plot(x = c(1:10), cum_percent/nrow(out_english), xlab = "Top 10 Terms", ylab = "Cumulative Percentage of posts appeared in", xaxt = "n")
axis(1, at=1:10, labels=names(top10), las = 2)
# this is "double counting" questions? 
```

* find most frequent terms among answered/unanswered questions
* if any of these terms are present in a question, can use to predict the time_until_answer (think that questions that contain terms in frequent terms list in answered questions might get answered quicker, questions that contain terms in list for unanswered questions might get answered slower)
* noticed that there were shared common words among answered/unanswered questions (ex: screen, phone)
* wanted to find a way to find words unique to answered/unanswered 
* calculating proportion of times that each word in the most freq list occurs in its answered or unanswered category 
* finding the difference in these proportions to see if the word is more likely to be in answered/unanswered category 
*use these difference in proportions to determine which list the term should be included in (answered/unanswered)

```{r}
library(tidyverse) #contains all packages
library(plyr)
answered <- out_english %>%
              tbl_df() %>% 
              filter(answered == 1)
unanswered <- out_english %>%
              tbl_df() %>% 
              filter(answered == 0)
  
answered_m <- freq_terms(answered$title)
unanswered_m <- freq_terms(unanswered$title)

answered_m[1:100]
unanswered_m[1:100]

str(answered_m)

#proportion of times each term occurs in answered/unanswered questions
answered_freq <- data.frame(term = names(answered_m), a_freq = unname(answered_m), a_prop = unname(answered_m)/length(answered_m))

unanswered_freq <- data.frame(term = names(unanswered_m), u_freq = unname(unanswered_m), u_prop = unname(unanswered_m)/length(unanswered_m))

combined <- join(answered_freq, unanswered_freq, by = "term", match = "all")
head(combined)
dim(combined)
```

```{r}
combined$difference <- abs(combined$a_prop - combined$u_prop)
combined$ratio <- combined$a_prop/combined$u_prop
head(combined)

combined %>%
  arrange(desc(ratio)) %>%
  select(-difference)
# ssd is 28.8 times more likely to appear in answered questions than it is in unanswered questions, only because it appears 40 times in a_freq questions and only once in unanswered 
# need to come up with min and max thresholds for which terms to include freq_words list

combined %>%
  arrange(desc(u_prop)) 

library(ggthemes)
ggplot(combined, aes(x = u_prop)) +
  geom_histogram(bins = 50) + 
  scale_x_continuous(limits = c(0, 0.05), labels = scales::percent) + 
  scale_y_continuous(limits = c(0,500)) 

ggplot(combined, aes(x = a_prop)) +
  geom_histogram(bins = 50) + 
  scale_x_continuous(limits = c(0, 0.05), labels = scales::percent) + 
  scale_y_continuous(limits = c(0,200)) 
```
* how many words to consider
* minimum/max occurence thresholds (how often show up to be able to use it) 
* create a function that takes in as parameter how many words to consider, and then 
*function to take in vector of text, return frequencies of top words, input = number of words to return 
*ggplot!!!!






