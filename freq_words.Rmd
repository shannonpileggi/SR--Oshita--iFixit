---
title: "New variables"
author: "Lisa Oshita"
date: "July 14, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Importing the data/Setting up time to event variable
```{r}
dir <- file.path(getwd(),"data")
out <- read.csv(file.path(dir, "answers_data.csv"))

library(dplyr)
out_english <- out %>% 
                tbl_df() %>%
                filter(langid == "en")

out_english$time_until_answer <- (out_english$first_answer_date - out_english$post_date)/3600
empty <- which(is.na(out_english$time_until_answer))
for (i in empty) {
  out_english$time_until_answer[i] <- (out_english$download_date[i] - out_english$post_date[i])/3600
}

```


### Finding most frequently used terms
```{r}

library(qdap)
library(tm)

#function to clean corpus
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "can", "will", "cant", "wont", "works", "get", "help", "need"))
  return(corpus)
}

#function that returns vector of terms sorted from most to least frequent
freq_terms <- function(vec) {
  source <- VectorSource(vec)
  corpus <- VCorpus(source)
  cleaned_corpus <- clean_corpus(corpus) 
  dtm <- DocumentTermMatrix(cleaned_corpus)
  m <- as.matrix(dtm) 
  freq <- colSums(m)
  freq <- sort(freq, decreasing = TRUE)
  return(freq)
}

titles_freq <- freq_terms(out_english$title)
head(titles_freq)
hist(titles_freq)
hist(log(titles_freq))

top50 <- titles_freq[1:50]
top10 <- titles_freq[1:10]
plot(top50)

```


### Variable indicating if title contains at least one word in top50 most frequent
* no significant difference between questions containing these words and questions that don't
```{r}

library(stringr)
library(rebus)

out_english$contain_top10 <- str_detect(as.character(out_english$title), pattern = or1(names(top10)))

sum(out_english$contain_top10)/nrow(out_english)

out_english %>%
  group_by(contain_top10) %>%
  summarise(median_time = median(time_until_answer), 
            avg_daily = mean(daily_views), n = n()) %>%
  arrange(median_time)

```

### Survival curves for questions with/without top10 terms
```{r}

library(survival) 
library(survminer)

surv_object <- Surv(out_english$time_until_answer, out_english$answered, type = "right")
KM_top10 <- survfit(surv_object ~ contain_top10, data = out_english)

ggsurvplot(KM_top10, data = out_english, risk.table = TRUE, pval = TRUE)

survdiff(surv_object~contain_top10, data = out_english)

```


### Trying out TfIdf (no difference)
```{r}

freq_terms_weighted <- function(vec) {
  source <- VectorSource(vec)
  corpus <- VCorpus(source)
  cleaned_corpus <- clean_corpus(corpus) 
  dtm <- DocumentTermMatrix(cleaned_corpus, control = list(weighting = function(x)
    weightTfIdf(x, normalize = TRUE)))
  m <- as.matrix(dtm) 
  freq <- colSums(m)
  freq <- sort(freq, decreasing = TRUE)
  return(freq)
}

titles_weighted <- freq_terms_weighted(out_english$title)
titles_freq_weighted[1:10]

hist(titles_freq_weighted)

```

### Visualizing word frequency 
* Distribution of how often words appear (plot of top 100 most frequently used words vs. percent of posts appeared in) 
```{r}

plot(x = c(1:50), unname(top50)/nrow(out_english), xlab = "Top 10 terms", ylab = "Percentage of posts appeared in", xaxt = "n")
axis(1, at=1:50, labels=names(top50), las = 2)

```

### Most frequent words within answered/unanswered questions
```{r}

library(tidyverse)

answered <- out_english %>%
              tbl_df() %>% 
              filter(answered == 1)
unanswered <- out_english %>%
              tbl_df() %>% 
              filter(answered == 0)
  
terms_a <- freq_terms(answered$title)
terms_u <- freq_terms(unanswered$title)

ans <- data.frame(term = names(terms_a), a_freq = unname(terms_a), a_prop = unname(terms_a) / length(terms_a))
unans <- data.frame(term = names(terms_u), u_freq = unname(terms_u), u_prop = unname(terms_u) / length(terms_u))

combined <- full_join(ans, unans, by = "term")
combined$ratio <- combined$a_prop / combined$u_prop
head(combined)

#interpretation: ssd is 28.8 times more likely to show up in answered questions than it is in unanswered questions (but it only shows up .7% of the time in answered questions, and .02% of the time in unanswered questions- probably not helpful)
combined %>% 
  arrange(desc(ratio))

combined %>%
  arrange(desc(u_prop)) 

ggplot(combined, aes(x = a_prop)) +
  geom_histogram(bins = 50) + 
  scale_x_continuous("Proportion in answered questions",limits = c(0, 0.05), labels = scales::percent) + 
  scale_y_continuous(limits = c(0,200)) 

ggplot(combined, aes(x = u_prop)) +
  geom_histogram(bins = 50) + 
  scale_x_continuous("Proportion in unanswered questions", limits = c(0, 0.05), labels = scales::percent) + 
  scale_y_continuous(limits = c(0,200)) 

```


### Finding proportion and ratio thresholds 
* find a set of words that show up frequently enough to be used as a set of predictors
* want the ratio to be large enough to allow us to decide whether or not that word is more likely to show up in an answered/unanswered question
```{r}

#unanswered questions

p_threshold <- 0.01
ratio_threshold <- 1

freq_terms_u <- combined %>%
                  filter(u_prop > p_threshold) %>%
                  filter(ratio < ratio_threshold)
freq_terms_u


#answered questions

freq_terms_a <- combined %>%
                  filter(a_prop > p_threshold) %>%
                  filter(ratio > ratio_threshold)
freq_terms_a

```


### Means/medians/survival curves for freq_terms_u
```{r}

out_english$contain_u <- str_detect(as.character(out_english$title), pattern = or1(freq_terms_u$term))

sum(out_english$contain_u)/nrow(out_english) #only 13% of questions contain these terms

out_english %>%
  group_by(contain_u) %>%
  summarise(median_time = median(time_until_answer), avg_time = mean(time_until_answer), n = n(), avg_views = 
              mean(daily_views)) %>%
  arrange(median_time)


KM_u <- survfit(surv_object ~ contain_u, data = out_english) 
survdiff(surv_object ~ contain_u, data = out_english)

ggsurvplot(KM_u, data = out_english, risk.table = TRUE, pval = TRUE)

```


### Means/medians/survival curves for freq_terms_a
```{r}
out_english$contain_a <- str_detect(as.character(out_english$title), pattern = or1(freq_terms_a$term))

sum(out_english$contain_a)/nrow(out_english) #63% of the questions contain these terms

out_english %>%
  group_by(contain_a) %>%
  summarise(median_time = median(time_until_answer), avg_time = mean(time_until_answer), n = n(), avg_views = 
              mean(daily_views)) %>%
  arrange(median_time)


KM_a <- survfit(surv_object ~ contain_freq_answered, data = out_english)
survdiff(surv_object~contain_freq_answered, data = out_english)

ggsurvplot(KM_a, data = out_english, risk.table = TRUE, pval = TRUE)

```


### Looking at only device terms within freq_terms_u
```{r}

unanswered_devices <- freq_terms_u %>%
                        filter(term %in% c("camera", "sound", "tablet", "light", "wifi", "speaker", "sound"))

out_english$contain_u_dev <- str_detect(as.character(out_english$title), pattern = or1(unanswered_devices$term))

sum(out_english$contain_u_dev)/nrow(out_english) #only 5.6% of titles contain these terms

KM_u_dev <- survfit(surv_object ~ contain_u_dev, data = out_english)

survdiff(surv_object ~ contain_u_dev, data = out_english)

ggsurvplot(KM_u_dev, data = out_english, risk.table = TRUE, pval = TRUE)

```

### Looking at only device terms within freq_terms_a
```{r}

freq_terms_a

answered_devices <- freq_terms_answered %>%
                      filter(term %in% c("screen", "iphone", "phone", "battery", "button", "display", "macbook", "lcd",
                                         "board", "keyboard", "laptop", "home", "pro", "drive"))

out_english$contain_dev_a <- str_detect(as.character(out_english$title), pattern = or1(answered_devices$term))
sum(out_english$contain_dev_a)/nrow(out_english) #33% of all questions contain these words

KM_dev_a <- survfit(surv_object ~ contain_dev_a, data = out_english)

survdiff(surv_object ~ contain_dev_a, data = out_english)

ggsurvplot(KM_dev_a, data = out_english, risk.table = TRUE, pval = TRUE)

```

### Function to return top n most frequently used words from a character vector
```{r}

n_freq_terms <- function(vector, n) {
  source <- VectorSource(vector)
  corp <- VCorpus(source)
  cleaned <- clean_corpus(corp) 
  dtm <- DocumentTermMatrix(cleaned)
  m <- as.matrix(dtm) 
  frequency <- colSums(m)
  frequency <- sort(frequency, decreasing = TRUE)
  terms <- frequency[1:n]
}

#testing
answered <- out_english %>%
              tbl_df() %>% 
              filter(answered == 1)
n_freq_terms(answered$title, 10)

```

### Plotting KM objects with ggplot
*convert survfit object to data frame with fortify function, plot with ggplot()
```{r}

library(ggfortify)

plot_surv <- function(survfit, data) {
  
  df <- fortify(survfit, data = data)
  
  if (!("strata" %in% names(df))) {
    ggplot(df, aes(x = time, y = surv)) + 
    geom_line() + 
    ggtitle("Survival Curve")
  }

  if ("strata" %in% names(df)) {
    ggplot(df, aes(x = time, y = surv, color = strata)) + 
      geom_line() + 
      ggtitle("Survival Curves")
  }
}

plot_surv(KM_top10, data = out_english)

```





