---
title: "Notes from studies/dissertations"
author: "Lisa Oshita"
date: "July 20, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### "Understanding and Classifying the Quality of Technical Forum Questions"
* published in: 2014 14th International Conference on Quality Software
* tries to model/predict the quality of questions on Stack Overflow

##### Metrics 
* Stack Overflow Metrics 
    + Body length
    + Emails count (number of email addresses found in the question)
    + Lowercase percentage (percentage of lowercase letters throughout the question)
    + Spaces count (total number of spaces throughout question)
    + Number of tags
    + Text Speak Count (number of text speak within the question, ex: doesnt', wat, rotfl)
    + Textual similarity between question and title
    + Capital title 
    + Uppercase percentage
* Readabiity metrics
    + Metric entropy = shannon entropy/length of the text: represents the randomness of the info in the question
    + Average terms entropy = average of the entropy for each term in questions text
    + Computed standardized readibility indexes- represent comprehension difficulty Stanford NLP Parser7 to extract sentences/words, TeX hyphenation [16] for syllables
* User popularity metrics
    + badges received concerning question and answers
    + coverage of author’s badges with respect to the tags assigned by the author to the new question
    + maybe consider including a variable about the reputation of an asker? 

##### Data Analysis
* used decision trees to classify good/bad questions
    + found that the popularity of the author is more important than textual features to determine quality
* Learned quality function findings
    + found that users who received a lot of down votes were more likely to post good questions to up their reputation
    + having produced good answers in the past was indicative of good quality
    + up votes recieved in the past didn't influence quality
    + text speak indicates bad quality (low number of sentences in the question) (maybe do ratio of number of sentences to length of question)????

##### References to check
* M. Allamanis and C. Sutton. Why, when, and what: Analyzing stack overflow questions by topic, type, and code. In Proceedings of MSR2013 (10th Working Conference on Mining Software Repositories), pages 53–56. IEEE Press, 2013.
* K. Arai and A. N. Handayani. Prediting Quality of Answer in Collaborative Q\A Community. International Journal of Advanced Research in Artificial Intelligence, 2(3):21–25, 2013.
* M. Coleman and T. L. Liau. A computer readability formula designed for machine scoring. Journal of Applied Psychology, 60(2):283–284, April 1975.
* D. Correa and A. Sureka. Chaff from the Wheat : Characterization and Modeling of Deleted Questions on Stack Overflow. In Proceedings of WWW 2014 (23rd international conference on World Wide Web. ACM, 2014.
* M. J. David Blei, Andrew Ng. Latent Dirichlet Allocation. Journal of machine Learning research, 3:993–1022, 2003.
* J. Jeon, W. B. Croft, J. H. Lee, and S. Park. A framework to predict the quality of answers with non-textual features. In Proceedings of SIGIR 2006 (29th Annual International ACM SIGIR Conference on Research & Development on Information Retrieval), pages 228–235. ACM, 2006.
* C. Treude, O. Barzilay, and M.-A. Storey. How do programmers ask and answer questions on the web? (nier track). In Proceedings of ICSE 2011 (33rd International Conference on Software Engineering), pages 804–807. ACM, 2011.
* A. Barua, S. W. Thomas, and A. E. Hassan. What are developers talking about? an analysis of topics and trends in stack overflow. Empirical Software Engineering, 19(3):619-654, June 2014.

##### References from "Quality Questions Need Quality Code: Classifying Code Fragments on Stack Overflow" to look into
* V. Bhat, A. Gokhale, R. Jadhav, J. Pudipeddi, and L. Akoglu, “Min(e)d your tags: Analysis of question response time in stackoverflow,” in Proc. of ASONAM 2014, 2014, pp. 328–335.
*  J. Yang, C. Hauff, A. Bozzon, and G.-J. Houben, “Asking the right question in collaborative Q&A systems,” Proc. of Hypertext 2014, pp. 179–189.

### "Automatically Assessing the Post Quality in Online Discussions on Software" References
* Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard Hovya. 2006a. Mining and assessing discussions on the web
through speech act analysis. In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.
* Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.