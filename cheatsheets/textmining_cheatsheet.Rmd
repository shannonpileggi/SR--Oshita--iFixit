---
title: "text mining"
author: "Lisa Oshita"
date: "July 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Basics
* text mining: process of deriving actionable insights from text
* bag of words text mining represents a way to count terms, or n-grams, across a collection of documents
* corpus: collection of documents
* 2 kinds of corpus data types: permanent corpus (PCorpus) and volatile corpus (VCorpus)
```{r}
library(qdap)
text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses."
#finds 10 most frequent terms

#finds the 10 most frequent terms (doesn't mattern what kind of word, all words treated equally)
term_count <- freq_terms(text, 10)
plot(term_count)
```

### tm package and creating volatile corpus
* to make a volatile corpus, need to have R interpret each element in vector of text as a document, us tm package and its source functions to do this: VectorSource(), which outputs a source object
* example: coffee_source <- VectorSource(coffee_tweets) (to convert vector to a source object)
* then pass source to VCorpus(), which is a list of lists: at each element of the corpus, there is a plain text document, which is essentially a list that contains the actual text data (since it's a list of lists, need to subset twice to get to the actual text data)
* use DataframeSource() when making corpus from dataframe: treats entire row as a complete document (careful not to pick up non-text data)
```{r}
library(tm)
num  <- c(1,2,3)
Author1 <- c("Text mining is a great time.", "Text analysis provides insights", "qdap and tm are used in text mining")
Author2 <- c("R is a great language", "R has many uses", "DataCamp is cool!")
example_text <- cbind(num, Author1, Author2)

#create source based off of columns 2 and 3 (don't want 1)
df_source <- DataframeSource(example_text[, 2:3])
#convert to corpus
df_corpus <- VCorpus(df_source)
df_corpus

#create another source based off of only column 3 (a vector)
vec_source <- VectorSource(example_text[,3])
#convert to a corpus
vec_corpus <- VCorpus(vec_source)
vec_corpus
```

### common cleaning functions
* cleaning helps aggregate terms
* tolower(): Make all characters lowercase (from base r, others are from tm) 
* removePunctuation(): Remove all punctuation marks
* removeNumbers(): Remove numbers
* stripWhitespace(): Remove excess whitespace
* from qdap package: 
* bracketX(): Remove all text within brackets (e.g. "It's (so) cool" becomes "It's cool")
* replace_number(): Replace numbers with their word equivalents (e.g. "2" becomes "two")
* replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. "Sr" becomes "Senior")
* replace_contraction(): Convert contractions back to their base words (e.g. "shouldn't" becomes "should not")
* replace_symbol() Replace common symbols with their word equivalents (e.g. "$" becomes "dollar")

### stop words
* words that are frequent but provide little/no information
* tm package contains a list of 174 stop words
```{r}
library(tm)
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."

#list of standard English stop words
stopwords("en")

#print text without standard english stopwords
removeWords(text, stopwords("en"))
#add coffee and bean to the stopwords list
new_stops <- c("coffee", "bean", stopwords("en"))
removeWords(text, new_stops)
```

### word stemming and stem completion
* stemDocument function gets to a words root, takes in either a character vector (outputs character vector) or PlainTextDocument (outputs PlainTextDocument)
```{r}
complicate <- c("complicated", "complication", "complicatedly")
#word stemming saved in stem_doc
library(SnowballC)
stem_doc <- stemDocument(complicate)
stem_doc
#completion dictionary
comp_dict <- "complicate"
#stem completion
complete_text <- stemCompletion(stem_doc, comp_dict)
complete_text
```

```{r}
#trying to apply stemDocument would treat text_data as one character, need to remove punctuation and split it up
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)
# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))
# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)
stem_doc
# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)
complete_doc
```

### preprocessing a corpus
* tm_map() applies preprocessing steps to a corpus
```{r}
#use custom functions to save time
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  return(corpus)
}
```

### tdm vs dtm
* document term matrix: use when you want each document represented as a row (useful if want to compare authors within rows, or want to preserve time series)
* make DTM: use DocumentTermMatrix(corpus), and then save as a matrix with as.matrix()
* term document matrix: transpose of DTM
* make TDM: use TermDocumentMatrix(corpus), save as matrix with as.matrix()

### frequent terms with tm
* can use rowSums(tdm) to find frequency of terms, arrange in decreasing order to find the words that appear the most, create barplot of top frequency words

### frequent terms with qdap
* fast way to get frequent terms (and give up some control over preprocessing steps), use freq_terms() function from qdap 
* accepts a text variable, top = top number of terms to show, stopwords = vector of stopwords to remove, at.least = minimum character length of a word to be included

### word clouds
* size is related to individual word frequency
* ex: wordcloud(words, frequencies, max.words = 500, colors = "blue")
```{r}
library(wordcloud)
library(RColorBrewer)
#stopwords and word clouds
#added chardonnay, wine, and glass to stopwords list
#apply function to corpus, convert to tdm, convert to matrix, 
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, 
                   c(stopwords("en"), "amp", "chardonnay", "wine", "glass"))
  return(corpus)
}
```

### adding colors to wordclouds
* wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, color = c("grey80", "darkgoldenrod1", "tomato")) - will put most frequent terms in colors 
* using brewer.pal function in RColorBrewer allows you to select colors from a palette
```{r}
#specify number of colors to draw
green_pal <- brewer.pal(8, "Greens")
#most of the times the first couple colors are too faint to see 
green_pal <- green_pal[-(1:2)]
#then add green_pal to wordcloud function
```

### Visualizing common words (process for visualizing dissimilar words is similar to this)
* visualize common words across multiple documents: commonality_cloud()
* use paste function and collapse = " " to create a vector containing all documents
* use VectorSource() to convert vector to vector source
* use VCorpus() on the vector source to create corpus 
* then clean corpus, convert it to TDM, convert it to matrix, then use commonality.cloud() with max.words and colors arguments

### other visualizations 
* create word networks with word_associate() function
* can create dendrograms 
* word associations: findAssocs(tdm, "word", 0.25), returns a list, for any given word, calculates its correlation with every other word in a TDM or DTM

### term frequency-inverse document frequency
* if a term appears frequently it must be important, but if it appears in all documents- not likely to be important:
* TfIdf score increases by term occurrence but is penalized by the frequency of appearance among all documents
* use: control = list(weighting = weightTfIdf) in TermDocumentMatrix function


