---
title: "Final Model Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
dir <- file.path(getwd(),"data")
out <- read.csv(file.path(dir, "answers_data.csv"))
list <- oshitar::variable_setup(out)
x <- list[[1]]
x1 <- as.data.frame(x)

library(survival); library(dplyr); library(rms)
```

##### Cross validation 

```{r, cross validation plan, echo = FALSE}
# cross validation plan with k = 5
set.seed(444)
splitPlan <- vtreat::kWayCrossValidation(nrow(x), 5, NULL, NULL)

train1 <- x[splitPlan[[1]]$train, ]; test1 <- x[splitPlan[[1]]$app, ]
train2 <- x[splitPlan[[2]]$train, ]; test2 <- x[splitPlan[[2]]$app, ]
train3 <- x[splitPlan[[3]]$train, ]; test3 <- x[splitPlan[[3]]$app, ]
train4 <- x[splitPlan[[4]]$train, ]; test4 <- x[splitPlan[[4]]$app, ]
train5 <- x[splitPlan[[5]]$train, ]; test5 <- x[splitPlan[[5]]$app, ]
```


```{r, functions for cv, include = FALSE}
# function for cross validation, vars should be string: "new_category + new_user..."
crossval <- function(vars, train, test) {
  formula <- paste("Surv(time_until_answer, answered) ~ ", vars, sep = "")
  model <- rms::cph(as.formula(formula), data = train)
  
  train[["predictions"]] <- exp(predict(model, type = "lp"))
  metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = train)
  train_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2),
                              R2 = round(metric$stats[8], 2),
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = train)$concordance)
  
  # predicting on test data
  test[["predictions"]] <- exp(predict(model, newdata = test, type = "lp"))
  
  # computing performance metrics 
  metric1 <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = test)
  test_metrics <- data.frame(HR = exp(metric1$coefficients), 
                             LR = round(metric1$stats[3],2), 
                             pval = round(metric1$stats[5],2), 
                             R2 = round(metric$stats[8], 2),
                             AIC = stats::AIC(metric1, k = 2),
                             Dxy = round(metric1$stats[9],2), 
                             Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = test)$concordance)
  
  # returns data frame with train/test metrics 
  statistics <- rbind(train_metrics, test_metrics)
  rownames(statistics) <- c("Training Data", "Test Data")
  return(statistics)
}

# function to average performance metrics, takes list of output from crossval
get_avgmetrics <- function(list) {
  avg <- rbind(train_avg = colMeans(purrr::map_df(1:length(list), ~rbind(list[[.]][1,]))), test_avg = colMeans(purrr::map_df(1:length(list), ~rbind(list[[.]][2,]))))
  return(avg)
}

# function to get average differences between training and test data metrics for each cv iteration
get_avgdiff <- function(list) {
  list_diff <- purrr::map(1:length(list), ~diff(as.matrix(list[[.]], lag = 1)))
  df <- plyr::ldply(list_diff, data.frame) %>%
    select(HR, Dxy, Concordance) %>%
    colMeans()
  return(df)
}
```

```{r, echo = FALSE}
vars <- "new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strat(ampm) + sqrt(avg_tag_score) + pol(text_length, 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4)"

vars3 <- "new_category + new_user + contain_unanswered + contain_answered + title_questionmark + text_contain_punct + text_all_lower + update + prior_effort + weekday + rcs(avg_tag_score, 5) + rcs(text_length, 4) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4)"

vars1 <- "new_category + new_user + contain_unanswered + contain_answered + title_questionmark + text_contain_punct + text_all_lower + update + prior_effort + weekday + avg_tag_score + text_length + device_length + avg_tag_length + newline_ratio"

trains <- list(train1, train2, train3, train4, train5)
trains <- purrr::map(trains, ~as.data.frame(.))
tests <- list(test1, test2, test3, test4, test5)
tests <- purrr::map(tests, ~as.data.frame(.))

(cv_results <- purrr::map2(trains, tests, ~crossval(vars1, .x, .y)))
avg_1 <- get_avgmetrics(cv_results)
avg_1

(cv_results1 <- purrr::map2(trains, tests, ~crossval(vars1, .x, .y)))
avg_2 <- get_avgmetrics(cv_results1)
avg_2

avg_1; avg_2
```

##### Validation with rms::validate for Somers’ Dxy rank correlation between predicted log hazard and observed survival time + slope shrinkage

###### output: 

* Dxy = Somers' Dxy (2 ∗ (C − 0.5), where C is the concordance probability)
* R2: RN2 index
* Slope: Calibration slope (slope of predicted log odds vs true log odds) optimism value is a value of overfitting
* D: Discrimination index — likelihood ratio χ2 divided by the sample size
* U: Unreliability index — unitless index of how far the logit calibration curve intercept and slope are from (0, 1)
* Q: Logarithmic accuracy score - scaled version of the log-likelihood achieved by the predictive model
* g: g index (?)

###### interpreting:

* optimism is small- model is a good fit (?)
* slope shrinkage = 0.0163, indicates over-fitting not present

```{r, echo = FALSE}
dd <- datadist(x1)
options(datadist = "dd")
final1 <- rms::cph(Surv(time_until_answer, answered) ~ new_category + new_user + 
                               contain_unanswered + contain_answered + title_questionmark + 
                               title_beginwh + text_contain_punct + text_all_lower + update + 
                               greeting + gratitude + prior_effort + weekday + strat(ampm) + 
                               sqrt(avg_tag_score) + pol(text_length, 2) + rcs(device_length, 5) + 
                               rcs(avg_tag_length, 4) + rcs(newline_ratio, 4), 
                             data = x, x = TRUE, 
                             y = TRUE, surv = TRUE)
final1
AIC(final1, k = 2) # 69945.32
(val <- rms::validate(final1, method = "crossvalidation", bw = F, dxy = TRUE, u = 0.5))

final <- rms::cph(Surv(time_until_answer, answered) ~ new_category + new_user + 
                  contain_unanswered + contain_answered + title_questionmark + 
                  text_contain_punct + text_all_lower + update + prior_effort + weekday + 
                  rcs(avg_tag_score, 5) + rcs(text_length, 4) + rcs(device_length, 5) + 
                  rcs(avg_tag_length, 4) + rcs(newline_ratio, 4), 
                  data = x1, x = TRUE, 
                  y = TRUE, surv = TRUE)
final
AIC(final, k = 2)
```


##### Validating the model for calibration accuracy with rms::calibrate

* outputs mean absolute error in predictions, mean squared error, and the 0.9 quantile of the absolute error
* error =  difference between the predicted values and the corresponding bias-corrected calibrated values

```{r, echo = FALSE}
# calibration accuracy in predicting probability at 0.5 hours (predicts and calibrates at 0.5 hours for each iteration of the bootstrap?)
calibration <- rms::calibrate(final, method = "crossvalidation", B = 100, bw = FALSE, u = 0.5)
calibration
plot(calibration)
```

```{r, not using these functions, include = FALSE}
# not using these functions
# function to delete overly influential questions (used in function below)
delete_influential <- function(var) {
  thresh <- mean(score[[var]]) + (sd(score[[var]]) * 30)
  delete <- which(abs(score[[var]]) > thresh)
  return(delete)
}

# fits model on training data, identifies overly influential questions, removes questions according to threshold, refits model to that data, uses that model to make predictions and computes performance metrics 
newcrossval <- function(vars, train, test) {
  formula <- paste("Surv(time_until_answer, answered) ~ ", vars, sep = "")
  model <- survival::coxph(as.formula(formula), data = train)
  
  score <- as.data.frame(residuals(model, type = "score"))
  
  to_delete <- unique(unlist(purrr::map(names(score), ~delete_influential(.))))
  newdata <- train[-to_delete,]
  
  newmodel <- survival::coxph(as.formula(formula), data = newdata)
  
  newdata[["predictions"]] <- predict(newmodel, type = "risk")
  metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = newdata)
  train_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2), 
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = newdata)$concordance)
  
  # predicting on test data
  test[["predictions"]] <- predict(newmodel, newdata = test, type = "risk")
  
  # computing performance metrics 
  metric1 <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = test)
  test_metrics <- data.frame(HR = exp(metric1$coefficients), 
                             LR = round(metric1$stats[3],2), 
                             pval = round(metric1$stats[5],2), 
                             AIC = stats::AIC(metric1, k = 2),
                             Dxy = round(metric1$stats[9],2), 
                             Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = test)$concordance)
  
  # returns data frame with train/test metrics 
  statistics <- rbind(train_metrics, test_metrics)
  rownames(statistics) <- c("Training Data", "Test Data")
  return(statistics)
}
```

##### Assessing model fit on full data

* significant predictors (pvalues < 0.05):
    + new_category: Apple Product, Camera, game console, other, pc, vehicle
    + new_user
    + contain_unanswered
    + text_all_lower
    + update
    + prior_effort 
    + weekday: Thursday
    + avg_tag_score (sqrt)
    + spline on avg_tag_length
* Interpretations of signficant predictors
    + Apple Product: (beta = 0.93) log hazard of getting answered for apple products is 0.93 higher than for android/othe phones, controlling for other predictors. (HR = exp(beta) = 2.554) Hazard of getting answered is 155% higher for apple products compared to android/other phones, controlling for other predictors 
    + avg_tag_score (sqrt): (beta = 2.8764) log hazard of getting answered increases by 2.88 for every 1 unique increase in the square root of avg_tag_score, adjusting for other predictors. HR = 17.75, the hazard of getting answered increases by 167.5% for each one unit increase in the square root of the average tag score, adjusting for other predictors

```{r, echo = FALSE}
x$predictions <- exp(predict(final, type = "lp"))

metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = x)
AIC(metric, k = 2) # 83125.55

final_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2),
                              R2 = round(metric$stats[8], 2),
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = x)$concordance)
final_metrics # metrics on the full data set are similar to the metrics from cross validation
```

##### Getting estimated survival probabilities 

```{r, echo = FALSE}
surv_prob <- final$surv # list of 4 (one list for each stratification level)

# allows you to get estimated survival probability at specific time (for each strat level)
summary(survfit(final), time = c(0.5, 1, 3))

# either specify newdata, linear.predictors or x
# number on the rows of the surv matrix is the index 
# function is very slow if full data set is used
rms::survest(final, newdata = test1[1:3,], times = c(0.5, 1, 10), conf.int = FALSE)
rms::survest(final, times = c(0.5, 1, 10), conf.int = FALSE)

# using different function
pec::predictSurvProb(final, newdata = test1[1:3,], times = c(0.5, 1, 10))
# interpretation:
#   for question 380, the probability that it remains unanswered beyond 30 minutes is 0.53
#   for question 2195, the probability that it remains unanswered byond 30 minutes is 0.85
```

##### Estimated survival probability curves

* predicted survival curve + curves for certain variables (rms functions keep outputting errors about data dist)
* nomograms? 

```{r}
# plots log hazard for individual predictors 
dd = datadist(x)
options(datadist="dd")

ggplot(Predict(final, new_category))  
ggplot(Predict(final, device_length))
ggplot(Predict(final, text_length))
ggplot(Predict(final, avg_tag_length))
ggplot(Predict(final, newline_ratio))

times <- sort(unique(x$time_until_answer))
pec::plotPredictSurvProb(final, newdata = x[1:4,], times = c(1, 10, 12, 13, 14)) # ugly output

# must specify a variable 
rms::survplot(final, ampm, time.inc = 100, col = 1:4, n.risk = TRUE, xlab = "Hours")
rms::survplot(final, new_category, time.inc = 100, col = 1:length(levels(x$new_category)), xlab = "hour", label.curves=list(keys=1:length(levels(x$new_category))))
rms::survplot(final, new_user = 0, conf.int = .95, time.inc = 100) # can change variable it plots 
```

##### Assessing residuals/PH assumption

```{r, echo = FALSE}
#----Martingale residuals----------------------------------------------
train1$mart <- residuals(trainmodel, type = "martingale")

ggplot(train1, aes(x = sqrt(avg_tag_score), y = mart)) + 
  geom_point() + 
  geom_smooth(method = "loess") + 
  ggtitle(paste("Martingale Residuals for sqrt(average_tag_score)")) + 
  scale_x_continuous("Average Tag Score (square root)") 
# functional form seems adequate 

#----Deviance residuals------------------------------------------------
train1$deviance <- residuals(trainmodel, type = "deviance")
ggplot(train1, aes(x = 1:nrow(train1), y = deviance)) + 
  geom_point() + 
  scale_x_continuous("Question index") + 
  scale_y_continuous(breaks = seq(-4, 4, by = 0.5)) + 
  ggtitle("Deviance Residuals")
# identify questions with residuals > |2.5| (questions poorly predicted by the model)

# examining all questions with > |2.5|
outliers <- train1 %>%
  dplyr::filter(abs(deviance) > 2.5)
# 117 questions in training data set have deviance residuals > 2.5 
# all were answered with time_until_answer < 1 hr (except for 2 which have time_until_answer > 1050 hours)

#----Schoenfeld residuals----------------------------------------------
schoen <- as.data.frame(residuals(trainmodel, type = "schoenfeld"))
schoen$comp_times <- sort(train1[train1$answered != 0,]$time_until_answer)

# function to plot schoenfeld residuals for each predictor
plot_schoen <- function(var) {
  ggplot(schoen, aes(x = comp_times, y = schoen[[var]])) + 
    geom_point() + 
    geom_smooth() + 
    ggtitle(paste("Schoenfeld Residuals for", var)) + 
    scale_y_continuous("Residuals")
}
purrr::map(names(schoen)[-ncol(schoen)], plot_schoen)

# formal test of PH assumption 
options(scipen=999)
ph_test <- data.frame(predictors = rownames(cox.zph(finalz)$table), cox.zph(finalz)$table) 
rownames(ph_test) = NULL
ph_test <- ph_test %>% arrange(p) 
# first term of device_length, Apple Product, Camera, Game Console violate PH (stratifying on new_category decreases predictive ability) 

#----Score residuals--------------------------------------------------
# check for influential observations
score <- as.data.frame(residuals(trainmodel, type = "score"))

# function to plot score residuals
plot_score <- function(column) {
  ggplot(score, aes(x = 1:nrow(score), y = score[[column]])) + 
    geom_point() + 
    geom_smooth() + 
    ggtitle(paste("Score residuals for", column)) + 
    scale_y_continuous("Score Residuals") + 
    scale_x_continuous("Question index")
}

purrr::map(names(score), ~plot_score(.))
# tried removing influential questions and doing cross validation: models fit on data with influential questions removed didn't perform as well on the test data compared to models fit on full data

```
