---
title: "Final Model Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
x <- as.data.frame(oshitar::setup())
library(survival); library(dplyr); library(rms)
```

```{r, cross validation plan, echo = FALSE}
# cross validation plan with k = 5
set.seed(444)
splitPlan <- vtreat::kWayCrossValidation(nrow(x), 5, NULL, NULL)

train1 <- x[splitPlan[[1]]$train, ]; test1 <- x[splitPlan[[1]]$app, ]
train2 <- x[splitPlan[[2]]$train, ]; test2 <- x[splitPlan[[2]]$app, ]
train3 <- x[splitPlan[[3]]$train, ]; test3 <- x[splitPlan[[3]]$app, ]
train4 <- x[splitPlan[[4]]$train, ]; test4 <- x[splitPlan[[4]]$app, ]
train5 <- x[splitPlan[[5]]$train, ]; test5 <- x[splitPlan[[5]]$app, ]
```

#### Variables: 

* new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strata(ampm) + sqrt(avg_tag_score) + poly(text_length, degree = 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4)
* device_length has 5 knots since hypothesized that it's a better predictor, others only have 4 (reccomended in Regression Modelling Strategies text) 

```{r, echo = FALSE}
# building model on train1
trainmodel <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strat(ampm) + sqrt(avg_tag_score) + poly(text_length, 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4), data = train1, x = TRUE, y = TRUE)
AIC(trainmodel, k = 2) # 54428.62
trainmodel
```

##### Assessing residuals/PH assumption

```{r, echo = FALSE}
#----Martingale residuals----------------------------------------------
train1$mart <- residuals(trainmodel, type = "martingale")

ggplot(train1, aes(x = sqrt(avg_tag_score), y = mart)) + 
  geom_point() + 
  geom_smooth(method = "loess") + 
  ggtitle(paste("Martingale Residuals for sqrt(average_tag_score)")) + 
  scale_x_continuous("Average Tag Score (square root)") 
# functional form seems adequate 

#----Deviance residuals------------------------------------------------
train1$deviance <- residuals(trainmodel, type = "deviance")
ggplot(train1, aes(x = 1:nrow(train1), y = deviance)) + 
  geom_point() + 
  scale_x_continuous("Question index") + 
  scale_y_continuous(breaks = seq(-4, 4, by = 0.5)) + 
  ggtitle("Deviance Residuals")
# identify questions with residuals > |2.5| (questions poorly predicted by the model)

# examining all questions with > |2.5|
outliers <- train1 %>%
  dplyr::filter(abs(deviance) > 2.5)
# 117 questions in training data set have deviance residuals > 2.5 
# all were answered with time_until_answer < 1 hr (except for 2 which have time_until_answer > 1050 hours)

#----Schoenfeld residuals----------------------------------------------
schoen <- as.data.frame(residuals(trainmodel, type = "schoenfeld"))
schoen$comp_times <- sort(train1[train1$answered != 0,]$time_until_answer)

# function to plot schoenfeld residuals for each predictor
plot_schoen <- function(var) {
  ggplot(schoen, aes(x = comp_times, y = schoen[[var]])) + 
    geom_point() + 
    geom_smooth() + 
    ggtitle(paste("Schoenfeld Residuals for", var)) + 
    scale_y_continuous("Residuals")
}
purrr::map(names(schoen)[-ncol(schoen)], plot_schoen)

# formal test of PH assumption 
options(scipen=999)
ph_test <- data.frame(predictors = rownames(cox.zph(trainmodel)$table), cox.zph(trainmodel)$table) 
rownames(ph_test) = NULL
ph_test <- ph_test %>% arrange(p) 
# first term of device_length, Apple Product, Camera, Game Console violate PH (stratifying on new_category decreases predictive ability) 

#----Score residuals--------------------------------------------------
# check for influential observations
score <- as.data.frame(residuals(trainmodel, type = "score"))

# function to plot score residuals
plot_score <- function(column) {
  ggplot(score, aes(x = 1:nrow(score), y = score[[column]])) + 
    geom_point() + 
    geom_smooth() + 
    ggtitle(paste("Score residuals for", column)) + 
    scale_y_continuous("Score Residuals") + 
    scale_x_continuous("Question index")
}

purrr::map(names(score), ~plot_score(.))
# tried removing influential questions and doing cross validation: models fit on data with influential questions removed didn't perform as well on the test data compared to models fit on full data

```

##### Cross validation 

```{r, functions for cv, echo = FALSE}
# function to perform cross validation, vars should be string: "new_category + new_user..."
crossval <- function(vars, train, test) {
  formula <- paste("Surv(time_until_answer, answered) ~ ", vars, sep = "")
  model <- rms::cph(as.formula(formula), data = train)
  
  train[["predictions"]] <- exp(predict(model, type = "lp"))
  metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = train)
  train_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2),
                              R2 = round(metric$stats[8], 2),
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = train)$concordance)
  
  # predicting on test data
  test[["predictions"]] <- exp(predict(model, newdata = test, type = "lp"))
  
  # computing performance metrics 
  metric1 <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = test)
  test_metrics <- data.frame(HR = exp(metric1$coefficients), 
                             LR = round(metric1$stats[3],2), 
                             pval = round(metric1$stats[5],2), 
                             AIC = stats::AIC(metric1, k = 2),
                             Dxy = round(metric1$stats[9],2), 
                             Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = test)$concordance)
  
  # returns data frame with train/test metrics 
  statistics <- rbind(train_metrics, test_metrics)
  rownames(statistics) <- c("Training Data", "Test Data")
  return(statistics)
}

# function to average performance metrics, takes list of output from crossval
get_avgmetrics <- function(list) {
  avg <- rbind(train_avg = colMeans(purrr::map_df(1:length(list), ~rbind(list[[.]][1,]))), test_avg = colMeans(purrr::map_df(1:length(list), ~rbind(list[[.]][2,]))))
  return(avg)
}

# function to get average differences between training and test data metrics for each cv iteration
get_avgdiff <- function(list) {
  list_diff <- purrr::map(1:length(list), ~diff(as.matrix(list[[.]], lag = 1)))
  df <- plyr::ldply(list_diff, data.frame) %>%
    select(HR, Dxy, Concordance) %>%
    colMeans()
  return(df)
}
```

```{r, echo = FALSE}
vars <- "new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strat(ampm) + sqrt(avg_tag_score) + poly(text_length, 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4)"

trains <- list(train1, train2, train3, train4, train5)
tests <- list(test1, test2, test3, test4, test5)

(cv_results <- purrr::map2(trains, tests, ~crossval(vars, .x, .y)))

get_avgmetrics(cv_results)
get_avgdiff(cv_results) # metrics don't change that much between fitting model on training and test data sets
```

##### Validation with rms::validate for Somers’ Dxy rank correlation between pre-dicted log hazard and observed survival time, + slope shrinkage

###### output: 
* Dxy = Somers' Dxy (2 ∗ (C − 0.5), where C is the concordance probability)
* R2: RN2 index
* Slope: Calibration slope (slope of predicted log odds vs true log odds) optimism value is a value of overfitting
* D: Discrimination index — likelihood ratio χ2 divided by the sample size
* U: Unreliability index — unitless index of how far the logit calibration curve intercept and slope are from (0, 1)
* Q: Logarithmic accuracy score - scaled version of the log-likelihood achieved by the predictive model
* g: g index (?)

###### interpreting:
* optimism is small- model is a good fit (?)
* slope shrinkage = 0.0163, indicates over-fitting not present

```{r, echo = FALSE}
valmodel <- rms::cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strat(ampm) + sqrt(avg_tag_score) + poly(text_length, 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4), data = x, x = TRUE, y = TRUE, surv = TRUE, time.inc = 0.5, dxy = TRUE)

validation <- rms::validate(valmodel, method = "crossvalidation", B = 100, bw = F, dxy = TRUE, u = 0.5)
validation

```

##### Validating the model for calibration accuracy with rms::calibrate

* outputs mean absolute error in predictions, mean squared error, and the 0.9 quantile of the absolute error
* error =  difference between the predicted values and the corresponding bias-corrected calibrated values

```{r, echo = FALSE}
# calibration accuracy in predicting probability at 0.5 hours (predicts and calibrates at 0.5 hours for each iteration of the bootstrap?)
calibration <- rms::calibrate(calmodel, method = "crossvalidation", B = 100, bw = FALSE, u = 0.5)
calibration
plot(calibration)
```

```{r, not using these functions}
# not using these functions
# function to delete overly influential questions (used in function below)
delete_influential <- function(var) {
  thresh <- mean(score[[var]]) + (sd(score[[var]]) * 30)
  delete <- which(abs(score[[var]]) > thresh)
  return(delete)
}

# fits model on training data, identifies overly influential questions, removes questions according to threshold, refits model to that data, uses that model to make predictions and computes performance metrics 
newcrossval <- function(vars, train, test) {
  formula <- paste("Surv(time_until_answer, answered) ~ ", vars, sep = "")
  model <- survival::coxph(as.formula(formula), data = train)
  
  score <- as.data.frame(residuals(model, type = "score"))
  
  to_delete <- unique(unlist(purrr::map(names(score), ~delete_influential(.))))
  newdata <- train[-to_delete,]
  
  newmodel <- survival::coxph(as.formula(formula), data = newdata)
  
  newdata[["predictions"]] <- predict(newmodel, type = "risk")
  metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = newdata)
  train_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2), 
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = newdata)$concordance)
  
  # predicting on test data
  test[["predictions"]] <- predict(newmodel, newdata = test, type = "risk")
  
  # computing performance metrics 
  metric1 <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = test)
  test_metrics <- data.frame(HR = exp(metric1$coefficients), 
                             LR = round(metric1$stats[3],2), 
                             pval = round(metric1$stats[5],2), 
                             AIC = stats::AIC(metric1, k = 2),
                             Dxy = round(metric1$stats[9],2), 
                             Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = test)$concordance)
  
  # returns data frame with train/test metrics 
  statistics <- rbind(train_metrics, test_metrics)
  rownames(statistics) <- c("Training Data", "Test Data")
  return(statistics)
}
```

##### Fitting the model to the full data set

```{r, echo = FALSE}
model <- rms::cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strat(ampm) + sqrt(avg_tag_score) + poly(text_length, 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4), data = x, x = TRUE, y = TRUE, surv = TRUE)

x$predictions <- exp(predict(model, type = "lp"))

metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = x)
AIC(metric, k = 2) # 83023

final_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2),
                              R2 = round(metric$stats[8], 2),
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = x)$concordance)
final_metrics # metrics on the full data set are similar to the metrics from cross validation
```

##### Getting estimated survival probabilities 

```{r, echo = FALSE}
surv_prob <- model$surv # list of 4 (one list for each stratification level)

# allows you to get estimated survival probability at specific time (for each strat level)
summary(survfit(model), time = c(0.5, 1, 3))

# either specify newdata, linear.predictors or x
# number on the rows of the surv matrix is the index 
# function is very slow if full data set is used
rms::survest(model, newdata = test1[1:3,], times = c(0.5, 1, 10), conf.int = FALSE)

# using different function
pec::predictSurvProb(model, newdata = test1[1:3,], times = c(0.5, 1, 10))

times <- sort(unique(x$time_until_answer))

pec::plotPredictSurvProb(model, newdata = x[1:4,], times = times) # this is ugly, figure out how to do this in ggplot?
```

##### Describing the final model

* predicted survival curve + curves for certain variables (rms functions to do this keep outputting errors about datadist)
* nomograms? 

##### Function to output survival probabilities

* input data set with any combination of predictors + times to predict on 
* output corresponding survival probabilities 

```{r}
# input vector of times to predict on + data set to be used in predictions
# function runs really slowly because it has to set up the data + fit the model everytime 
# outputs matrix of predicted survival 
# also include some kind of performance metric? measure of how accurately its predicting 
# sets up variables needed in the new data 
get_survival <- function(times, data) {
  newdata <- oshitar::variable_setup(data)
  x <- oshitar::setup()
  model <- rms::cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + title_beginwh + text_contain_punct + text_all_lower + update + greeting + gratitude + prior_effort + weekday + strat(ampm) + sqrt(avg_tag_score) + poly(text_length, 2) + rcs(device_length, 5) + rcs(avg_tag_length, 4) + rcs(newline_ratio, 4), data = x, x = TRUE, y = TRUE, surv = TRUE)
  surv_probs <- pec::predictSurvProb(model, newdata = newdata, times)
  return(surv_probs)
}


get_survival(times = c(0.5, 1, 10), data = out)

```


