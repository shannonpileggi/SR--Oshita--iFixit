---
title: "Predictive modeling with cross validation"
author: "Lisa Oshita"
date: "8/10/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, data setup, echo = FALSE}
x <- oshitar::setup()
library(survival)
library(stringr)
library(rebus)
library(dplyr)
library(stats)
library(rms)
```

### Creating cross-validation plan with k = 5

```{r, echo = FALSE}
set.seed(444)
splitPlan <- vtreat::kWayCrossValidation(nrow(x), 5, NULL, NULL)
str(splitPlan)

train1 <- x[splitPlan[[1]]$train, ]
test1 <- x[splitPlan[[1]]$app, ]

train2 <- x[splitPlan[[2]]$train, ]
test2 <- x[splitPlan[[2]]$app, ]

train3 <- x[splitPlan[[3]]$train, ]
test3 <- x[splitPlan[[3]]$app, ]

train4 <- x[splitPlan[[4]]$train, ]
test4 <- x[splitPlan[[4]]$app, ]

train5 <- x[splitPlan[[5]]$train, ]
test5 <- x[splitPlan[[5]]$app, ]
```

### Building the model on one of the training data sets

##### Fitting model with all significant variables

* univariate analysis p-value < 0.01
* using stepAIC to perform backwards and forwards stepwise variable selection with AIC criteria 
* transformed text_length + avg_tag_length + newline_ratio all insignificant (don't include in model)

```{r, echo = FALSE}
fit <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + log10(text_length) + title_questionmark + title_beginwh + sqrt(newline_ratio) + sqrt(avg_tag_score) + text_contain_punct + text_all_lower + update + greeting + as.factor(n_tags) + gratitude + prior_effort + ampm + n_images + weekday, data = train1)

model <- MASS::stepAIC(fit, direction = "both", k = 2)

#results of stepAIC (including contain_answered even though it was dropped)
results <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday, data = train1)
summary(results)
AIC(results, k = 2) # 64969.63
```

##### Adding in splines 

* using penalized spline smoothing with AIC criteria 
    + pspline function from survival package 
* splines on device_length and avg_tag_length decreased the AIC the most 
    + it seems like adding in any spline decreases the AIC (is the AIC valid to use) 
* also fit using corrected AIC as criteria - no difference 

```{r}
# adding in spline on device_length
model_spline <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(device_length, df = 0, method = "AIC"), data = train1)
AIC(model_spline, k = 2) # 64907.21
summary(model_spline)

# adding in spline on text_length
model_spline1 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(text_length, df = 0, method = "AIC"), data = train1)
AIC(model_spline1, k = 2) # 64914.27

# adding in spline on avg_tag_length
model_spline2 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(avg_tag_length, df = 0, method = "AIC"), data = train1)
summary(model_spline2)
AIC(model_spline2, k = 2) # 64893.52

# spline = avg_tag_length + device_length
model_spline3 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(device_length, df = 0, method = "AIC") + pspline(avg_tag_length, df = 0, method = "AIC"), data = train1)
summary(model_spline3)
AIC(model_spline3, k = 2) # 64865.95

# adding in spline on newline_ratio
model_spline4 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(newline_ratio, df = 0, method = "AIC"), data = train1)
summary(model_spline3)
AIC(model_spline4, k = 2) # 64915.6

# log10(text_length), spline = avg_tag_length + device_length
model_spline5 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + log10(text_length) + sqrt(newline_ratio) + pspline(device_length, df = 0, method = "AIC") + pspline(avg_tag_length, df = 0, method = "AIC"), data = train1)
AIC(model_spline5, k = 2) # 64865.73

# log10(text_length), sqrt(newline_ratio), spline = avg_tag_length + device_length
model_spline5 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + log10(text_length) + sqrt(newline_ratio) + pspline(device_length, df = 0, method = "AIC") + pspline(avg_tag_length, df = 0, method = "AIC"), data = train1)
AIC(model_spline5, k = 2) # 64864.73

# title_beginwh*title_questionmark, log10(text_length), sqrt(newline_ratio), spline = avg_tag_length + device_length
model_spline5 <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + log10(text_length) + sqrt(newline_ratio) + pspline(device_length, df = 0, method = "AIC") + pspline(avg_tag_length, df = 0, method = "AIC") + title_beginwh + title_questionmark*title_beginwh, data = train1)
AIC(model_spline5, k = 2) # 64858.68
```

##### Restricted cubic splines  

* using rcs (restricted cubic spline) in rms package
* fit knots at quantiles of the predictor 
* recommended in Regression Modeling Strategies to have 5 knots for large data sets 
* estimates less parameters than penalized spline smoothing with AIC criteria 
* **AIC is higher for these splines** 

```{r}
library(rms)
# fitting with spline on device_length
model <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + rcs(device_length, unname(quantile(train1$device_length, c(0.05, 0.275, 0.5, 0.65, 0.95)))), data = train1)
AIC(model, k = 2) # 64968.39

# fitting with spline on text_length
model2 <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + rcs(text_length, unname(quantile(train1$text_length, c(0.05, 0.275, 0.5, 0.65, 0.95)))), data = train1)
AIC(model2, k = 2) # 64967.25

# fitting with spline on avg_tag_length (output keeps giving errors)
# model3 <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + rcs(avg_tag_length, unname(quantile(train1$avg_tag_length, c(0.05, 0.35, 0.65, 0.95)))), data = train1)
# AIC(model3, k = 2) # 64967.25

# fitting with spline on newline_ratio (keep getting errors with this one too)
# model4 <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + rcs(newline_ratio, unname(quantile(train1$newline_ratio, c(0.05, 0.275, 0.5, 0.65, 0.95)))), data = train1)
# AIC(model4, k = 2) # 64967.25

# fitting with splines on device and text length
model5 <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + rcs(device_length, unname(quantile(train1$device_length, c(0.05, 0.275, 0.5, 0.65, 0.95)))) + rcs(text_length, unname(quantile(train1$text_length, c(0.05, 0.275, 0.5, 0.65, 0.95)))), data = train1)
AIC(model5, k = 2) # 64969.7

# fitting model with spline on text_length + sqrt(device_length)
model6 <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + sqrt(device_length) + rcs(text_length, unname(quantile(train1$text_length, c(0.05, 0.275, 0.5, 0.65, 0.95)))), data = train1)
AIC(model6, k = 2) # 64967.95

```

##### Interactions

* probably not going to include any interactions - don't do much for the model 

```{r, echo = FALSE}
# adding interaction between title_questionmark + title_beginwh 
reduced <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + title_beginwh + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday, data = train1) # title_beginwh was not included in result from stepAIC, added in this model to compare it to the model with the interaction 

interaction <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + title_beginwh + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + title_questionmark*title_beginwh, data = train1)
summary(interaction)
AIC(interaction, k = 2) # 64963.33 barely drops 
oshitar::compare_nested(interaction, reduced)
```

### residuals, PH assumption

* Martingale residuals
    + plot for each continuous predictor 
    + assess adequacy of functional form of predictors, or suggest a potential form 
* Deviance residuals
    + check for outliers identify questions poorly predicted by the model 
    + one plot, examine questions with |residuals| > 2.5
* Schoenfeld residuals
    + one plot for each predictor 
    + if loess curve is flat, proportional hazard assumption is not violated, doesn't depend on time. Distinct pattern indicates violation of this assumption
    + **from the plots:** new_user, title_questionmark, num_freq_tags (1), ampm (Morning, Night), weekday (Thursday, Saturday), new_user*AMPM (morning, night) may violate PH assumption 
* Score residuals
    + check for influential observations. ID quesitons who's predictor values highly effect parameter estimates
    + plot for each predictor 
* **figure out how to identify certain points on ggplots**

```{r, residuals, echo = FALSE}
#=============================================
#martingale residuals
train1$mart <- residuals(model_full, type = "martingale")

ggplot(train1, aes(x = device_length, y = mart)) + 
  geom_point() + 
  geom_smooth() + 
  ggtitle(paste("Martingale Residuals for device_length")) + 
  scale_x_continuous("Device_length")

#=============================================
#deviance residuals 
train1$deviance <- residuals(model_full, type = "deviance")
ggplot(train1, aes(x = 1:nrow(train1), y = deviance)) + 
  geom_point() + 
  scale_x_continuous("Question index") + 
  scale_y_continuous(breaks = seq(-4, 4, by = 0.5)) + 
  ggtitle("Deviance Residuals")
# quite a few questions have a deviance residual of larger than 2.5 

#=============================================
#schoenfeld residuals
schoen <- as.data.frame(residuals(model_full, type = "schoenfeld"))
# complete event times sorted from shortest to longest
schoen$comp_times <- sort(train1[train1$answered != 0,]$time_until_answer)

plot_schoen <- function(var) {
  ggplot(schoen, aes(x = comp_times, y = schoen[[var]])) + 
    geom_point() + 
    geom_smooth() + 
    ggtitle(paste("Schoenfeld Residuals for", var)) + 
    scale_y_continuous("Residuals")
}
names <- names(schoen)[-ncol(schoen)]
purrr::map(names, plot_schoen)

#=============================================
# score residuals
score <- as.data.frame(residuals(model_full, type = "score"))
# function to plot score residuals
plot_score <- function(column) {
  ggplot(score, aes(x = 1:nrow(score), y = score[[column]])) + 
    geom_point() + 
    geom_smooth() + 
    ggtitle(paste("Score residuals for", column)) + 
    scale_y_continuous("Score Residuals") + 
    scale_x_continuous("Question index")
}
columns <- names(score)
purrr::map(columns, plot_score)

#=============================================
# working with rms::which.influence to id influential points
c <- rms::cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + title_questionmark + num_freq_tags + contain_answered + text_all_lower + update + ampm + weekday + device_length + title_beginwh + title_questionmark*title_beginwh + new_user*update + new_user*ampm + new_category*new_user, x = TRUE, y = TRUE, data = train1)
influence <- rms::which.influence(c)
rms::show.influence(influence, train1)

#=============================================
# formal test of PH assumption
options(scipen=999)
ph_test <- data.frame(predictors = rownames(cox.zph(model_full)$table), cox.zph(model_full)$table) 
rownames(ph_test) = NULL
ph_test <- ph_test %>%
  arrange(p)

#=============================================
# addression PH assumption violations

# stratifying on new_category
strat_model <- coxph(Surv(time_until_answer, answered) ~ strata(new_category) + new_user + contain_unanswered + title_questionmark + num_freq_tags + contain_answered + text_all_lower + update + ampm + weekday + device_length + title_beginwh + title_questionmark*title_beginwh + new_user*update + new_user*ampm, data = train1)
AIC(strat_model, k = 2) # 49766.8
summary(strat_model)

# stratifying on new_user
strat_model1 <- coxph(Surv(time_until_answer, answered) ~ new_category + strata(new_user) + contain_unanswered + title_questionmark + num_freq_tags + contain_answered + text_all_lower + update + ampm + weekday + device_length + title_beginwh + title_questionmark*title_beginwh, data = train1)
AIC(strat_model1, k = 2) # 60543.75
summary(strat_model1)

# stratifying on ampm 
strat_model2 <- coxph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + title_questionmark + num_freq_tags + contain_answered + text_all_lower + update + strata(ampm) + weekday + device_length + title_beginwh + title_questionmark*title_beginwh, data = train1)
AIC(strat_model2, k = 2) # 54447.8
summary(strat_model2)

# stratifying on ampm, new_user, new_category
strat_model3 <- coxph(Surv(time_until_answer, answered) ~ strata(new_category) + strata(new_user) + contain_unanswered + title_questionmark + num_freq_tags + contain_answered + text_all_lower + update + strata(ampm) + weekday + device_length + title_beginwh + title_questionmark*title_beginwh, data = train1)
AIC(strat_model3, k = 2) # 35370.81
summary(strat_model3)
```


### Performance metric on training data set

* survConcordance: 
    + Concordance is defined as Pr(agreement) for any two randomly chosen observations, where in this case agreement means that the observation with the shorter survival time of the two also has the larger risk score. The predictor (or risk score) will often be the result of a Cox model or other regression
    + some of the pairs are incomparable. For instance a pair of times (5+, 8), the first being a censored value. We do not know whether the first survival time is greater than or less than the second
    + observations that are comparable, pairs may also be tied on survival time (but only if both are uncensored) or on the predictor. The final concondance is (agree + tied/2)/(agree + disagree + tied)
    

```{r}
# working with model_spline2 (model with all splines)
train1$predictions <- predict(object = model_spline2, type = "risk") # should I predict "lp" instead of "risk"
(performance <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = train1, x = TRUE, y = TRUE, se.fit = TRUE, residuals = TRUE))

survConcordance(Surv(time_until_answer, answered) ~ predictions, data = train1) # Concordance = 0.6382697
Hmisc::rcorrcens(Surv(time_until_answer, answered) ~ predictions, data = train1)

```

### Predicting on the test data

```{r, include = FALSE}
# function to perform cross validation
# vars is a string of variables, ex: "new_category + new_user..."
# outputs df with performance metrics (for model on train and test data 
crossval <- function(vars, train, test) {
  formula <- paste("Surv(time_until_answer, answered) ~ ", vars, sep = "")
  model <- survival::coxph(as.formula(formula), data = train)
  
  train[["predictions"]] <- predict(model, type = "risk")
  metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = train)
  train_metrics <- data.frame(HR = exp(metric$coefficients), 
                              LR = round(metric$stats[3],2), 
                              pval = round(metric$stats[5],2), 
                              AIC = stats::AIC(metric, k = 2),
                              Dxy = round(metric$stats[9],2), 
                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = train)$concordance)
  
  # predicting on test data
  test[["predictions"]] <- predict(model, newdata = test, type = "risk")
  
  # computing performance metrics 
  metric1 <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = test)
  test_metrics <- data.frame(HR = exp(metric1$coefficients), 
                             LR = round(metric1$stats[3],2), 
                             pval = round(metric1$stats[5],2), 
                             AIC = stats::AIC(metric1, k = 2),
                             Dxy = round(metric1$stats[9],2), 
                             Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = test)$concordance)
  
  # returns data frame with train/test metrics 
  statistics <- rbind(train_metrics, test_metrics)
  rownames(statistics) <- c("Training Data", "Test Data")
  return(statistics) 
  
}

# function to get average performance metric for each iteration 
# takes list of performance metrics 
get_avgmetrics <- function(list) {
  avg <- rbind(train_avg = colMeans(purrr::map_df(1:length(list), ~rbind(list[[.]][1,]))), test_avg = colMeans(purrr::map_df(1:length(list), ~rbind(list[[.]][2,]))))
  return(avg)
}

```

##### spline on device_length, text_length, avg_tag_length

```{r, echo = FALSE}
trains <- list(train1, train2, train3, train4, train5)
tests <- list(test1, test2, test3, test4, test5)

allvars <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(device_length, df = 0, method = 'AIC') + pspline(text_length, df = 0, method = 'AIC') + pspline(avg_tag_length, df = 0, method = 'AIC')"
allsplines <- purrr::map2(trains, tests, ~crossval(allvars, .x, .y))

allsplines
# 5th iteration: pvalue for loglikelihood ratio test = 0.51 on test data 
```

##### spline on device_length only 

```{r}
varsdev <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(device_length, df = 0, method = 'AIC')"

splinedev <- purrr::map2(trains, tests, ~crossval(varsdev, .x, .y))
splinedev
```

##### spline on text_length only 

```{r}
varstext <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(text_length, df = 0, method = 'AIC')"

splinetext <- purrr::map2(trains, tests, ~crossval(varstext, .x, .y))
```

##### spline on avg_tag_length only

```{r}
varstag <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(avg_tag_length, df = 0, method = 'AIC')"

splinetag <- purrr::map2(trains, tests, ~crossval(varstag, .x, .y))
```

##### spline on newline_ratio only 

```{r}
varsnew <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(newline_ratio, df = 0, method = 'AIC')"

splinenew <- purrr::map2(trains, tests, ~crossval(varsnew, .x, .y))
```

##### with splines on device_length and text_length 

* splines on device_length and text_length ahd the most signficant p-values out of all splines in univariate analysis

```{r}
devtextvars <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(device_length, df = 0, method = 'AIC') + pspline(text_length, df = 0, method = 'AIC')"

devtext <- purrr::map2(trains, tests, ~crossval(devtextvars, .x, .y))

```

```{r}
sqrtdev <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + sqrt(device_length) + pspline(text_length, df = 0, method = 'AIC')"

sqrtdev <- purrr::map2(trains, tests, ~crossval(sqrtdev, .x, .y))
sqrtdev

sqrttext <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + sqrt(text_length) + pspline(device_length, df = 0, method = 'AIC')"

sqrttext <- purrr::map2(trains, tests, ~crossval(sqrttext, .x, .y))

```

##### without splines

```{r}
vars <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday"

nospline <- purrr::map2(trains, tests, ~crossval(vars, .x, .y))
```

##### log10(text_length) + sqrt(newline_ratio) + splines: device_length, avg_tag_length + interaction between titlebeginwh*title_questionmark
```{r}
vars1 <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + log10(text_length) + sqrt(newline_ratio) + pspline(device_length, df = 0, method = 'AIC') + pspline(avg_tag_length, df = 0, method = 'AIC') + title_beginwh + title_questionmark*title_beginwh"

lots <- purrr::map2(trains, tests, ~crossval(vars1, .x, .y))
lots
```


##### viewing all metrics 

```{r}
# turn avg_list into a data frame with rownames???
list <- list(allsplines, splinedev, splinetext, splinetag, splinenew, nospline, devtext, sqrtdev, sqrttext, lots)
avg_list <- purrr::map(list, ~get_avgmetrics(.))
avg_list
```


```{r}
get_avgdiff <- function(list) {
  list_diff <- purrr::map(1:length(list), ~diff(as.matrix(list[[.]], lag = 1)))
  df <- plyr::ldply(list_diff, data.frame) %>%
    select(HR, Dxy, Concordance) %>%
    colMeans()
  return(df)
}

# find a way to put rownames on df_diff in a reproducible way 
l_diff <- purrr::map(list, ~get_avgdiff(.))
df_diff <- plyr::ldply(purrr::map(1:length(l_diff), ~data.frame(as.list(l_diff[[.]]))))
df_diff
#least amount of average change found in model with only the spline on newline_ratio 
```

```{r}
newline_text <- "new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + log10(text_length) + pspline(newline_ratio, df = 0, method = 'AIC')"

sqrtdev <- purrr::map2(trains, tests, ~crossval(newline_text, .x, .y))
sqrtdev
```



##### restricted cubic spline on text_length (had the smallest AIC)

* predictions on test data not working 

```{r}
#  cv_rcstext <- function(train, test) {
# 
#   # building model on training data set
#   model <- cph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + rcs(text_length, unname(quantile(train1$text_length, c(0.05, 0.275, 0.5, 0.65, 0.95)))), data = train)
# 
#   metric <- cph(Surv(time_until_answer, answered) ~ p, data = train)
#   train_metrics
# 
#   # performance metrics for training data
#   train[["predictions"]] <- exp(predict(model, type = "lp"))
#   metric <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = train)
#   train_metrics <- data.frame(HR = exp(metric$coefficients),
#                               LR = round(metric$stats[3],2),
#                               pval = round(metric$stats[5],2),
#                               Dxy = round(metric$stats[9],2),
#                               AIC = stats::AIC(metric, k = 2),
#                               Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = train)$concordance)
# 
#   # predicting on test data
#   test[["predictions"]] <- exp(predict(model, newdata = test, type = "lp"))
# 
#   # computing performance metrics
#   metric1 <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = test)
#   test_metrics <- data.frame(HR = exp(metric1$coefficients),
#                              LR = round(metric1$stats[3],2),
#                              pval = round(metric1$stats[5],2),
#                              Dxy = round(metric1$stats[9],2),
#                              AIC = stats::AIC(metric1, k = 2),
#                              Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = test)$concordance)
# 
#   # returns data frame with train/test metrics
#   statistics <- rbind(train_metrics, test_metrics)
#   rownames(statistics) <- c("Training Data", "Test Data")
#   return(statistics)
# }
# 
# rcstext_models <- purrr::map2(trains, tests, ~cv_rcstext(.x, .y))
# splinenew_models

```






# Fitting model to the full data set

```{r, echo = FALSE}
x <- oshitar::variable_setup(x)

final <- coxph(Surv(time_until_answer, answered) ~ new_category + as.factor(new_user) + contain_unanswered + contain_answered + title_questionmark + sqrt(avg_tag_score) + text_all_lower + update + prior_effort + ampm + weekday + pspline(device_length, df = 0, method = "AIC") + pspline(text_length, df = 0, method = "AIC") + pspline(avg_tag_length, df = 0, method = "AIC"), data = x)
summary(final)
AIC(final, k = 2) # 83051.61

x$predictions <- predict(final, type = "risk")
(performance <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = x))
AIC(performance, k = 2)

# extracting predicted survival probabilities
prob <- pec::predictSurvProb(final, newdata = x, times = c(0.5, 1, 1.5, 2, quantile(x$time_until_answer)))


#==================================================================================
# function to take in a data set, fit the model, output performance metrics, and allow you to specify times to predict survival probabilities 

predict_survprob <- function(dataset, times) {
  # fitting the model 
  data <- oshitar::variable_setup(dataset)
  model <- coxph(Surv(time_until_answer, answered) ~ new_category + new_user + contain_unanswered + title_questionmark + num_freq_tags + contain_answered + text_all_lower + update + ampm + weekday + device_length + title_beginwh + title_questionmark*title_beginwh + new_user*update + new_user*ampm + new_category*new_user, data = data)
  
  # performance metrics
  data[["predictions"]] <- predict(model, type = "risk")
  p <- rms::cph(Surv(time_until_answer, answered) ~ predictions, data = data)
  metrics <- data.frame(HR = exp(p$coefficients), 
                             LR = round(p$stats[3],2), 
                             pval = round(p$stats[5],2), 
                             R2 = round(p$stats[8],2), 
                             Dxy = round(p$stats[9],2), 
                             AIC = stats::AIC(p, k = 2), 
                             Concordance = survConcordance(Surv(time_until_answer, answered) ~ predictions, data = data)$concordance)
  
  # calculating predicted survival probabilities
  probs <- pec::predictSurvProb(model, newdata = data, times)
  
  # returns a list containing performance metrics, and data frame with predicted survival probabilities
  output <- list(metrics, probs)
  return(output)
}
```

