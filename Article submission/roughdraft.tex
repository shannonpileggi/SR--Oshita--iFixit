\documentclass[12pt]{article}
\usepackage{natbib}  % used for citations
\usepackage[parfill]{parskip} %used for formatting style of text

% Sets margins to 1 in
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


%-----------------------------------------------------------------------------------
\title{Rough Draft}
\author{Lisa Oshita}
\date{}
%-----------------------------------------------------------------------------------


\begin{document}

\maketitle

\section{Abstract}

Community-driven online question and answer forums (CQA) are becoming increasingly valuable sources of information. These platforms house an expansive amount of crowd-sourced knowledge in the form of thousands of questions and answers posted everyday. There are forums that cover a broad range of topics, like \textit{Yahoo! Answers}, and forums focused on specific topics, like computer programming-focused \textit{Stack Overflow}. An example of the latter is iFixit's \textit{Answers} forum. The \textit{Answers} forum features questions asked by users specifically related to device repair, which are answered by both repair experts and everyday users, furthering iFixit's mission to enable individuals to repair their own devices. Thus, it is important that questions receive timely answers. This paper presents a survival analysis on the time until a question receives its first answer. We developed a Cox proportional hazards model to predict the failure probability of a question, or the probability that a question receives an answer before a certain time. Though we identified signifcant predictors, the predictive accuracy was low ($R^2 = 0.15$). Our findings indicate that the most important predictors were the device category of the question (questions pertaining to Apple products received answers faster than others (HR = 2.56, 95\% CI = (2.31, 2.82))) and factors related to the question's title (e.g., whether or not it was phrased as a question (HR = 1.31, 95\% CI = (1.22, 1.41))). Future studies could investigate if the factors identified as signifcant in our study can be generalized to other CQAs. 

\section{Introduction}

    Community-driven online question and answer forums (CQA) are becoming widely used sources of information. These online platforms allow for anyone to ask or answer a question, regardless of their background or expertise level. One popular CQA is Yahoo! Answers, featuring questions of all topics and expertise levels. On the other hand of the spectrum is Stack Overflow, the CQA that features strictly computer programming question and answers. Forums like these 2 receive around 40 million visits every month.
    
    The CQA analyzed in this paper is iFixit's \textit{Answers} forum. Founded in 2003, iFixit helps thousands of people repair their broken devices everyday. The company provides over 30,000 free online repair guides with pictures and step-by-step instructions, and sells the specialized tools and parts needed for such repairs---tools like iPhone 7 battery adhesive strips or Nexus 9 LCD digitizers. The mission of iFixit is to enable people to extend the lifetime of their own devices, effectively saving them money and reducing electronic waste. 
    
    As not all possible repairs are covered in the published repair guides, and users may have questions related to repair guides, iFixit's \textit{Answers} forum is another important resource. This platform features over 9,000 device topics and over 100,000 solutions. Questions on this forum range from broken devices like a jammed zipper to shattered iPhone screens. As many rely on this forum for help with repairs, it is important that users receive timely answers. Fast response times will enhance user engagement and generate more web traffic, which is valuable to the reputation and longevity of the \textit{Answers} forum. Analysis of answer times can reveal factors that affect how quickly questions are answered, which can lead to suggestions for how users can ask better questions to minimize answer times, and for how the forum design can be improved. 
    
    However, to the best of our knowledge, analysis and prediction of answer \textit{times} on CQAs have not been thoroughly investigated. A majority of the existing research focuses on assessing and predicting question and answer quality. As such, there is need for further analysis of response times in these forums. This paper presents a survival analysis on the time until a question is answered on iFixit's \textit{Answers} forum. In order to determine factors significantly related to answer time predict the failure probability of a question.

\section{Related Work}

    In regards to investigation of forum response times, \citep{Bhat2014} developed a classification model to analyze response times of questions posted on \textit{Stack Overflow}, and found that tag-based features like the number of tags included or the number of subscribers a certain tag has, were the best predictors of answer time. 

    \citep{Mamykina2011} found that the swift answer times of \textit{Stack Overflow's} community is a result of the reputation system and the strict emphasis on factual and informative questions and answers, rather than discussion-based. 

    \citep{Asaduzzaman2013} analyzed unanswered questions on \textit{Stack Overflow} to determine common characteristics and found that questions that went unanswered shared certain characteristics in that they were too short and vague, or utilized the tagging system incorrectly. 


\section{Materials}

good question ex: id 408124, 406786 (answered)
bad question ex: id 410310, 405882 (unanswered) 

The data analyzed contained 8,025 questions posted from April 8, 2017 (10:14 PM) to July 7, 2017 (9:28 PM) (the date the data was downloaded). Variables in the data included: device name and category, title, text, tags, whether or not the user was a member of iFixit's site for less than one day before the question was posted, date the question was posted, date the first answer was received. Variables derived: 

Categorical Variables: 

\begin{itemize}
  \item Device category the question pertains to. Categories include: Apple Products, Android\/Other Phone, PC, Tablet, Electronics, Camera, Vehicle, Game Console, Home, Other.
  \item Whether or not the question's title contains at least one word that is considered ``frequently used'' among answered questions. See appendix for a complete list of these terms. 
  \item Whether or not the question's title contains at least one word that is considered ``frequently used'' among unanswered question. See appendix for a complete list of these terms. 
  \item Whether or not the question's title ends in a question mark.
  \item Whether or not the question's title begins with a ``Wh'' word (``Why'', ``Which'', ``When'', ``Who'', ``What'', ``Where''). 
  \item Whether or not the question's text contains any end punctuation marks (. ? !). 
  \item Whether or not the question's text is in all lower case. 
  \item Whether or not the asker edited or added information to the questions text sometime after posting it. 
  \item Whether or not the asker included a greeting (e.g. ``Hello'', ``Greetings'') in the question's text.
  \item Whether or not the asker used polite language in the question's text (e.g., ``Thank you'', ``please''). (don't like the way this is worded) 
  \item Whether or not the asker made an effort to solve the problem on their own, prior to asking the question.
  \item The day of the week the question was posted. 
  \item The time of day the question was posted. Times include: Morning, Afternoon, Evening, and Night. 
\end{itemize}

Numeric Variables:

\begin{itemize}
  \item The average frequency, or proportion of times a tag appeared in all of the data, for all of a question's user-defined tags. Questions without tags were assigned a value of 0 for this variable. 
  \item The average number of characters in each question's tags. 
  \item The number of characters in the question's text. 
  \item The number of characters in the user-defined device name. 
  \item The ratio of the number of line breaks to the number of characters in the question's text.
\end{itemize}


\section{Methods}

    Questions analyzed were restricted to those posted in English (97\% of the full data). The time until event variable used in survival analysis is defined as the time since posting until a question receives its first answer. Comments posted on the question are not considered answers, and an answer does not have to be accepted as the ``chosen solution'' to be considered the answer. For questions that remained unanswered by the time the data was downloaded, the time until event value is defined as the time since posting to the time the data was downloaded. These questions were considered right censored, meaning that the exact answer time for these questions is unknown and greater than the recorded answer time (these questions can still receive an answer after the data was downloaded, and would then have a higher answer time than what is recorded in the data). 

As there was a considerable amount of unanswered or right-censored questions, Kaplan-Meier estimates of survival probability, which adjust to the presence of these right censored observations, were assessed prior to model fitting. This method factors in the number of questions that have not been answered on each ordered time interval, based on complete answer times. The estimates of survival probablity is the product of all probabilities up to that intervals time (fix this paragraph!!!!). As such, these estimates are roughly equivalent to the conditional probability that a randomly selected question receives an answer in the next moment in time, given that it has not received an answer yet. From these estimates, curves were constructed to examine the survival experience of questions, and mean and median percentiles of survival times were calculated. 

Five-fold cross-validation was used to identify potentially signficant variables and to assess model perofmrance. (explain cross-validation better, and why 5 fold was chosen) The full data was split into five training and five test data sets. Each training data set contained 6,208 questions, and each test data set contained 1,552 questions. To identify variables potentially associated with answer time, univariate analysis was performed on one of the training data sets. Each variable was entered into a Cox regression model as the single predictor with the answer time as the survival time. Variables with p-values of less than 0.01 for the likelihood ratio test were taken into consideration. 

Variables identified in univariate analysis as signficant were entered into a Cox regression model. To assess predictive performance, cross-validation methods were used. For each iteration, the model was fit to one of the training data sets. The resulting model was then used to calculate predicted hazard ratios on the corresponding test data. To assess prediction performance, the predicted hazard ratios were entered into a separate Cox regression model as the single predictor with the question's answer times as the survival time. The resulting hazard ratio, R-square statistic, partial likelihood ratio and p-value, Somers' Dxy and Concordance statistics for that model were used as indicators of the prediction model's performance. Significant statistics for this model indicate that the predicted hazard ratio is signficantly associated with survival time and that our model performs well. The concept behind these metrics came from \cite{Chen}. These metrics were also computed for predictions on the training data. The metrics taken into consideration were the averages of all metrics computed on the test data. 


The model with the lowest AIC statistic, and most consistent performance metrics (there wasn't significant change across iterations) was taken as the final model. 

To assess the adequacy of functional forms of variables, Martingale residuals were assessed in plots. 

Deviance residuals were assessed. Score residuals 

Applying splines (using frank harrel's method) 

To assess proportional hazards assumption, cox.zph function was used in r, assesses correlation between scaled Schoenfeld residuals and time. 

  
\section{Results} 
(7,760 questions total), percent answered 
Figure a shows the distribution of answer times for all questions in the data set. 
summary stats from KM estimates
    The following statistics were calculated from Kaplan-Meier (explain what KM is) estimates of questions survival probability. As Kaplan-Meier estimates adjust for the presence of censored observations, these statistics are adjusted for the presence of unanswered questions. For questions analyzed, the mean survival time, or the average time until a question receives its first answer is 775.75 hours, or 32.32 days. The median survival time, the time at which 50\% of the questions in the data received an answer is 9.16 hours. Table 1 provides more percentiles of survival time. 
    
        Figure A shows the Kaplan-Meier Curve for all questions in the data. The curve indicates that around 100 hours after question is posted, the probability that a question receives an answer after that time hovers around 0.35. The survival probability never reaches 0 due to the large amount of unanswered questions. 

results from univariate analysis 

results of metrics 
        For each of the five iterations, there was no signficant difference between the metrics computed on the training data prediction, and the metrics computed ont he test data. Table C shows the average metrics for training and test data predictions. The metrics evaluated were the average of all metrics computed on the test data. Although these metrics are not considerably high, they do not vary signicantly from training to test data sets.  (should I explain more about Somers' Dxy and Concordance?) 

final model selected (with variables and transformations) 
The final model included all variables described above, along with the following transformations: 

\begin{itemize}
  \item Stratification on the time of day the question was posted. 
  \item Square root of the average frequency of a question's tags. 
  \item Quadratic polynomial on the length of the question's text. 
  \item Restricted cubic spline with 5 knots on the number of characters in the device name.
  \item Restricted cubic spline with 4 knots on the average number of characters in a question's tags.
  \item Restricted cubic spline with 4 knots on the ratio of newlines to the number of characters in a question's text. 
\end{itemize}
The partial likelihood ratio test statistic was 1307.15 with a p-value of 0. The R-squared value for this model is 0.1555. Significant predictors with a p-value of less than 0.01 are: the Apple product, Camera, Game Console, Home, PC, and Vehicle categories of the device variable, whether or not the question's text contained at least one term considered ``frequently used'' among answered questions, whether or not question's title ended in question mark, whether or not the asker edited the question's text after posting, whether or not the asker made effort prior to posting the question to find their own solution, the average frequency of a question's tags, and average number of characters for a question's tags. Include some interpretations of the hazard ratio? 

While the model is significant overall, it's predictive accuracy is low. Table blah shows metrics for the model fit to the full data set. One possible explanation for this is the high number of categorical variables included in the model. Including more quantitative variables may improve predictive accuracy. 

assessing assumptions 

Assessing the indepenedence between the model's Schoenfeld residuals and time indicated that the ``Apple Product'' category of the device variable violated the proportional hazards assumption. Other variables at risk for violation included the Camera and Game Console categories of devices, and the number of characters in the user-defined device name. As ``Apple Product'' was the only major violation of this assumption, and stratifying on this variable substantially decreased predictive power, these variables were left as is. 
Visually assessing martingale residuals for each quantitative predictor indicated that functional form for each appeared to be adequate. Assessing the deviance residuals indicated that a considerable amount of questions were not predicted well by the model. Similarily, assessing the score residuals showed that some questions might have an influence on the fit of the model. However, fitting the model without questions identified by score and deviance residuals to be either poorly predicted or highly influential, resulted in worse predictive performance. Hence, all questions were left in the data set.

\section{Discussion}

The data we analyzed for this model presented some limitations. Currently, askers have a considerable amount of freedom in the way they ask a question. As a result many have incorrectly specified various input fields when asking their question. For example, a question about a faulty Android tablet screen included as a tag ``someone sat on it :(''. Generally, tags are not more than a couple key words that describe the topic of the question. Many askers on this forum included lengthy and somewhat ambiguous tags in their question. Another issue was the incorrect naming of the device the asker's question was about. A user asking a question about their Turtle Beach Ear Force X device included as the name of their device "Turtle Beach Ear Force Xmy grandson chewed through the wire while we was playing it's brand-new is there anyway I can have it fixedO One". The inconsistencies in this data made it somewhat difficult to analyze, and possibly contributes to the model's low predictive accuracy. 

\section{Conclusion}




\section{Appendix} 

device

The data contained the original category variable as defined by iFixit. This category variable contained NAs (include how many), a result of the asker creating a question for a device not already on the website's data base. Questions that made the device clear were categorized accordingly. However there were a number of questions that did not explicitly declare the device their question pertained too. These questions were categorized as ``Other''. All Apple products (e.g., iPhones, iMacs, Apple watches) were given their own category. After pulling out iPhones from the original ``Phones'' category, the remaining phones were categorized as ``AndroidOther Phone''. Appliance and Household categories of the original variable were merged into ``Home''. ``Car and Truck'' were added to the ``Vehicle''. Lastly, any category that contained less than 2\% of the questions were categorized as ``Other''. All other categories remained the same as the original category. The final categories in the new categorization variable were Apple Product, Android/Other Phone, PC, Tablet, Electronics, Camera, Vehicle, Game Console, Home, and Other. 

contain answered and unanswered

  These variables were created based on the intuition that certain topics might be more popular, and even unpopular, among the answering community, and that questions concerning these topics might recieve an answer faster, or slower, than questions that don't. 
  
  To create these variables, the data was separated between answered and unanswered questions. Using text mining techniques, two lists were created-one for answered questions and another for unanswered questions--containing every word within the question's titles and the frequency or the number of time they occured among answered and unanswered questions. For ``frequently-used'' words in answered questions, that word would have to appear in more than 1\% of answered question's titles, and would have to appear in answered questions more than it appeared in unanswered questions. To determine the latter, a ratio of proportions was assessed. This ratio was calculated as the proportion of times a word occured among answered questions, to the proportion of times a word occured in unanswered questions. As an example, if ``cracked'' appeared in 2\% of answered questions and in 0.1\% of unanswered questions, it would be considered frequently used among answered questions as it occurs in more than 1\% of answered question's titles and occurs 20 times more in answered questions and in unanswered questions (ratio = 0.02/0.001 = 20). As for ``frequently-used'' words in unanswered questions, they must occur in 1\% or more of the question's titles and occur in more unanswered questions than unanswered questions (ratio < 1). Since there was some overlap between the words in each list and the device categories, every word that matched a device name was removed from the list. The resulting list for answered questions was 111 words, and 32 words in uanswered questions. Contain_answered and contain_unasnwered are logical variables, indicating true if a question's title contained at least one of the words in the frequently used words list for answered and unanswered questions, respectively. 
  
111 terms found to be ``frequently-used'' among answered questions (ordered from highest frequency): 
screen, turn, working, replacement, power, work, replace, charging, charge, button, touch, black, turning, broken, start, stuck, new, lcd, upgrade, problem, change, port, replaced, card, open, boot, replacing, remove, reset, back, drive, error, cable, ssd, cracked, hard, one, dropped, logic, lines, white, keeps, pro, dead, now, front, damage, switch, parts, glass, still, charger, issue, sim, turns, digitizer, just, mode, model, backlight, usb, stopped, logo, starting, know, unresponsive, password, factory, call, use, damaged, find, sensor, possible, fixed, side, galaxy, data, ipod, problems, issue, slow, system, connector, without, overheating, code, ram, air, microphone, please, red, much, plugged, getting, booting, left, way, buy, plus, time, loose, lock, coming, got, shuts, says, install, key, door, top

32 terms found to be ``frequently-used'' among unanswered questions (ordered from highest frequency):
sound, light, wifi, speaker, connect, picture, stay, noise, bluetooth, isnt, apps, going, rear, question, play, stop, service, take, hear, lights, showing, network, volume, come, keep, connection, flashing, shut, print, blue, buttons, edit
  
Title\_questionmark and title\_beginwh 

These two variables were created to determine if stating the title in the form of a question is associated with faster answer times. Title_questionmark is a logical variable indicating true if the question's title ends with a question mark. Title_beginwh is a logical variable indicating true if the title begins with a “wh” word (e.g. "what", "when"). These variables were created based off of the findings from \citep{Bhat2014}. 
  
  
Text\_contain\_punct

A logical variable indicating true if the question’s text contains any end punctuation marks (. ? !). This variable was created to investigate if run-on sentences, or sentences with no end punctuation, take longer to receive an answer. 

Prior\_effort 

Logical variable indicating true if the asker used words that indicate prior effort or research was done before asking the question (e.g. ``tried'', ``attempted'', ``tested''). This variable was created based off of findings from \cite{Bhat2014}. 

Ampm (change name)

The time of day the question was posted. If the question was posted between 5am and 12pm, it was categorized as ``morning''. If it was posted between 12pm and 5pm, it was categorized as ``afternoon'', between 5pm and 8am- ``evening'', and 8pm and 5 am, ``night''

Avg\_tag\_score

This variable was created to investigate the idea that some tags are more popular, or widely used than other, and that including such tags might increase the likelihood of that question recieving an answer. This variable is the average frequency, or proportion of times a question's tags appear in all of the data set. If a question has a higher average, than at least one of it's tags are frequently used. This variable was created based off of findings from \cite{Bhat2014}. 


Device\_length

The number of characters in the user-defined device variable. This variable was included to capture when a user incorrectly inputs the device name. For example, the device variable for question 390271 is, “Turtle Beach Ear Force Xmy grandson chewed through the wire while we was playing it's brand-new is there anyway I can have it fixedO One” and is 136 characters long. 11 questions in this data set also didn’t include any device. This variable was created based on the intuition that users who incorrectly define their devices make it harder for answerers to discern the topic of their question, and thus have longer answer times or might not recieve an answer.

Avg\_tag\_length

The average number of characters in each question's user-defined tags. If a question did not include a tag, this variable was set to 0. This variable, like the device-length variable, was created to capture when a user correctly or incorrectly used the tagging system. For example, question 390989 includes tag: “i need to repair the headset because i can not find the bluetoot” and is 64 characters long, while question 410254 includes tags "sound", "sound driver" and "speaker" and has an average tag length of 8 characters. It is hypothesized that users who use the tagging system correctly, as with the latter user, receive answers quicker than the former user. 

Newline\_ratio

This ratio of the number of newlines to the number of characters in the question's text. This variable was based on the intuition that question's that include newlines in the text are easier to read and thus have faster answer times. Questions with long text that also include newlines are generally easier to read than questions that don’t include any. 

\bibliographystyle{ECA_jasa}
\bibliography{questions}


\end{document}
