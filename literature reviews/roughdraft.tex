\documentclass[12pt]{article}
\usepackage{natbib}  % used for citations
\usepackage[parfill]{parskip} %used for formatting style of text

% Sets margins to 1 in
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


%-----------------------------------------------------------------------------------
\title{Literature Review of Online Question Forums}
\author{Lisa Oshita}
\date{}
%-----------------------------------------------------------------------------------


\begin{document}

\maketitle

\section{Introduction}

    Community-driven online question and answer forums (CQA) are becoming widley used sources of information. These online platforms allow for anyone to ask or answer a question. It provides a place for people of all backgrounds and levels of expertise to provide input on a certain question. One popular CQA is Yahoo! Answers. This platform features questions of all topics, from politics, to food and drink, and even mathematics. On the other hand of the spectrum is Stack Overflow, the CQA that features strictly computer programming question and answers. This forum reiceves around 40 million visits every month. This platforms ultimately provide people with a place to share their knowlege, and clearly are becoming increasingly popular sources of information. 
    
    The CQA discussed in this paper is iFixit's Answers forum. iFixit is a company founded in 2003 by two engineering students of the California Polytechnic State University in San Luis Obispo. What began as a small business running out of the college dorms, is now a company that helps thousands of people fix their broken devices everyday, providing over 30,000 free repair guides with pictures and step-by-step instructions. iFixit also sells the specialized tools and parts used in their repair guides, specialized tools like an iPhone 7 batter adhesive strip or a Nexus 9 LCD screen and digitizer. The main goal of this company is to teach people how to fix their own devices, and in extending the lifetime of their own devices, they save money and cut down on electronic waste. As such, this company helps thousands of people fix their devices everyday. 
    
    Along with repair guides and tool sales, another important component of this company is their online question and answer forum called Answers. This platform features questions related to device repair, featuring over 9,000 devices and over 100,000 solutions and answers. Questions on this forum range from broken devices like a jammed zipper on a Patagonia jacket, to a shattered iPhone 6 plus screen. This forum is an integral part in teaching people how to fix their own devices. If people run into problems, they can always trust that they have a large community to turn to for help. Thus it's important, for both the individuals as well as iFixit, that askers get prompt answers. Fast response times will drive up user engagement and generate more traffic, which in turn is great for the reputation and longevity of Answers and iFixit. As such, investigation of response times would be both beneficial and informative. Analysis can reveal factors within the forum that affect how quickly questions get answers. 

    Suggestions for how users can ask questions to minimize answer times, as well as suggestions for how the forum design can be improved, can be derived from such analysis. However, to the best of our knowledge, analysis and prediction of answer times on forums has not been investigated by many researchers. A majority of the research focuses on assessing and predicting the question and answer quality. As such, there is need for further analysis of response times in these forums. This paper presents a survival analysis on the time until a question is answered on iFixit's Answers forum. It attempts to determine the factors that are significantly related to answer time, and create a cox proportional hazards model that can accurately predict the survival probability of a question.

\section{Related Work}

    In regards to investigation of forum response times, \citep{Bhat2014} developed a classification model (? is that what it's called) to analyze response times of questions posted on Stack Overflow, and found that tag-based features like the number of tags included or the number of subscribers a certain tag has, were the best predictors of answer time. 

    \citep{Mamykina2011} found that the swift answer times of Stack Overflow's community is a result of the reputation system and the strict emphasis on factual and informative questions and answers, rather than discussion-based. 

    \citep{Asaduzzaman2013} analyzed unanswered questions on Stack Overflow to determine common characteristics and found that questions that went unanswered shared certain characteristics in that they were too short and vague, or utilized the tagging system incorrectly. 

\section{Materials and Methods}

    The data set analyzed in this paper contained 7,760 questions posted from April 8, 2017 to July 7, 2017. We worked exclusively with questions in English. Of the questions in the data set, 4,951, or 63.8\%, recieved an answer before the data was downloaded. Questions that remained unanswered were considered left censored. The time until event value used in survival analysis methods was defined as the time until a question recieves an answer. Comments posted on the question are not considered answers, and an answer does not have to be accepted as the chosen solution from the asker to be considered the answer. We are only considering the time until the question recieves its first answer. This time until answer variable was calculated as the time from the date it was posted to the date it recieved its first answer, in hours. For left-censored questions, questions that remained unanswered, this value was calculated as the time from the date it was posted to the date the data was downloaded, in hours. Figure a shows the distribution of the answer times for all questions in the data set.  
    
    The following statistics were calculated from Kaplan Meier estimates of the survival probabilities. In our case, survival probability is defined as the probability that a question remains unanswered after a certain time t. The mean survival time, or the average time for questions in the data set to recieve an answer, is 775.75 hours, or 32.32 days. The median survival time, the time at which 50\% of the questions recieved an answer is 9.16 hours. By 0.88 hours, 52.8 minutes, 25\% of the questions received an answer. By 683.19 hrs, 28.47 days, 64\% of the questions have been answered. Beyond that time point, 
    
    
    Variables within the data set included information about the questions, like the title, the body text, device, date it was posted, and information about the user like whether or not the user was considered new (or a member for less than 24 hours) at the time the question was posted. 
    
    Variables created in the model: 

The time until event variable used in the cox regression model is defined as the time until a question receives its first answer. If a question received an answer, this was calculated as the time from the date the question was posted to the date the first answer was received. If the question did not receive an answer, this was calculated as the time from the date the question was posted, to the date the date set was downloaded. The censoring variable needed in a cox regression model, is 0 if the question remained unanswered by the time the data set was downloaded, and 1 if the question received an answer. 

Newcategory 

This variable categorized the devices of the questions based off of the existing category variable as well as several other factors. The original category variable had NAs, which were a result of the asker creating a question for a device not already featured on the website, and were coded to “Other”. All Apple products (e.g., iPhones, iMacs, Apple watches) were given their own category. After pulling out iPhones from the original “Phones” category, the remaining phones were categorized as “Android/Other Phone”. Appliance and Household categories of the original variable were merged into “Home”. “Car and Truck” were added to the “Vehicle”. Lastly, any category that contained less than 2% of the questions were categorized as “Other”. 

Containanswered and contain_unanswered were created based off of the idea that certain key words would be more likely to catch an user’s eye, and would make them get answered more quickly. This was also based on the idea that certain words might also not be as popular, and would be common amongst questions that remained unanswered. As such, contain_answered and contain_unanswered are logical variables indicating true if a question’s title contains a term that is considered frequently used among answered or unanswered questions, respectively. To create these variables, the data set was separated between answered and unanswered questions. For each set, the occurrence of each word in every title was summed to get the frequencies of each word. Thus, we had a two lists of words—one consisting of the words within answered questions’ titles and their frequencies, and the other consisting of the words within unanswered questions’ titles and their frequencies. From these two lists, rules were applied to determine if a word can be considered “frequently used”.  For frequently used words in answered questions, that word would have to appear in more than 1percent of the titles, and would have to have a ratio of greater than 1. For a word to be considered “frequently used” among unanswered questions, that word would have to appear in more than 1percent of the question, and would have to have a ratio of less th san 1. Thus, we ended up with two sets of words, one set contained the most frequently used words in answered questions, and the other set contained the most freuqnelty used words in unanswered questions. Since there was some overlap between the words and the devices and category levels, any words that matched with a category were removed. That resulted in lists of 111 words for answered questions, and 32 words for unanswered questions. Contain_answered indicated true if a question’s title contained at least one of the words from the frequently used words among the answered questions. Contain_unanswered indicated true if a questions title contained at least one of the words from the frequently used words among the answered questions. 

Title_questionmark and title_beginwh 
These two variables were created to determine if stating the title in the form of a question is associated with faster answer times. Title_questionmark is a logical variable indicating true if the question ends in a question mark. Title_beginwh is a logical variable indicating true if the title begins with a “wh” word, like “what” or “when”. These variables were taken from mined your own tags

Text_contain_punct.
This is a logical variable indicating true if the question’s text contains any end punctuation marks (. ? !). This variable was created to investigate if run-on sentences (sentences with no end punctuation marks) take longer to receive an answer. 

Text_all_lower
This is a logical variable indicating true if the text of the question is in all lower case. 

Update 
This is a logical variable indicating true if the asker updated the question (edited or added information about the question) after posting it. 

Greeting
Logical variable indicating true if the asker included a greeting (e.g. “Hello”, “Hi”) as the first word of the text
Gratitude
Logical variable indicating true if the asker used manners in the question of the text (e.g. “Thank you”, “please”, “appreciate”)

Prior_effort 
Logical variable indicating true if the asker used language that indicates that prior effort or research was done before asking the question. (e.g. “tried”, “attempted”, “tested”). This variable was inspired by 

Weekday 
The day of the week the question was posted

Ampm 
The time of day the question was posted. If the question was posted between 5am and 12pm, it was categorieze das morning. If it was posted between 12pm and 5pm, it was categorized as afternoon, between 5pm and 8am- “evening”, and 8pm and 5 am, “night”

Avg_tag_score
The frequency or “score” of a tag is defined as the proportion of times that tag appears in the data set. This variable is the average of all of a question’s tag “scores” or frequencies. This variable was created to investigate if questions that include popular tags, or tags that are used frequently, have faster answer times. This variable was inspired by 

Text_length
The number of characters in the text

Device_length
The number of characters in the user-defined device variable. This variable was included to capture when a user incorrectly inputs their device. For example, question 390271 includes a device that is 136 characters long--“Turtle Beach Ear Force Xmy grandson chewed through the wire while we was playing it's brand-new is there anyway I can have it fixedO One”. 11 questions also didn’t include any device name. this variable was created to capture these user errors

Avg_tag_length
The average number of characters in the user-defined tags. This variable, like the device-length variable, was created to capture when a user incorrectly used the tagging system. An example of an incorrect tag is question 390989 and says: “i need to repair the headset because i can not find the bluetoot” and comes ot 64 charahcters. If a question did not include a tag, this variable was set to 0. 

Newline_ratio
This variable is calculated as the ratio of the number of newlines a question includes, over the number of characters in the questions text. This variable was included on the assumption that questions with text that is more readable get faster answer times. Questions with long text and also include newlines are generally easier to read than questions that don’t include any newlines. 

Variables included in the final model 

The above variables were all entered into the final model. The final model was stratified on ampm, as prior analysis indicated that this variable violated the proproitnal hazard assumption. The square root of the avg_tag_score was entered. Viewing the plot of answer times against the avg_tag_screo variable indicated heavy skewing. Taking the square root of this variable helped pull in the larger values. The quadratic term was included for text_length. This was included, since it was hypothesized that texts that were either too short or too long would have higher answer times. In other words, there might be a sweet spot on the text_lenght. Restricted cubic splines were included for device_length, avg_tag_length, and newline_ratio. The method for fitting these variables was taken from frank harrels text book. These variables were fit as restricted cubic splines with 5 knots initially. The final includes 5, 4, and 4 knots, as this model with this number of knots had the lowest Aic. 

Methods 

Univariate analysis for each of the variables of interest was carried out on one of the training data sets. Variables with a p-value of less than 0.01 were entered into the final model. Since this is a predictive model, it was decided that all variables would be kept in the model, to maintain predictive power. After building the model on one of the training data sets, assumptions and residuals were checked. 

Initally, ampm was put in the model without any stratification. Checking the proportional hazards assumption, by both examinging the schoenfeld residuals as well as the cox.zph function in r, indicated that ampm violated the proportional hazard assumption. It was decided that this variable would be stratified on. 

Checking the plot of martingale residuals shows that there doesn’t appear to be any obvious pattern, indicating that the functional form seems adequate (how to check residuals for splines??)

Checking the deviance residuals shows that a considerable amount of questions have residuals with an absolute value of well over 2.5. Taking those questions out of the data set and refitting the model and performing cross validation, showed that fitting the model without these questions resulted in worse predictive performance. 

Looking at the score resiudals for each predictor indicated that there may be a couple points that can be considered influential. Experimentation with those questions, and refitting the model without those certain points, along with performing cross validation, showed that refitting the model without those question resulted in worse predictive performance, similar to how it did for the deivcance resiudals. 
After determining that the model did not grossly violate any assumptions or have any weird residuals, a cross-validation scheme was carried out. 

A cross-validation scheme was used for this analysis. The data set was split into 5 training and test data sets. Training data sets had 6,208 questions and test data sets had 1,552 questions. The model was built on the training data sets, and tested on the corresponding test data sets. To assess the prediction performance of this model, metrics similar to /cite{Chen} were computed. The model and it’s coefficients, built on the training data sets, were then use to calculate the predicted hazard ratios for questions in the test data sets. Those predicted hazard ratios were then entered into another cox regression model with the time until answer as the survival time variable. Metrics calculated were the hazard ratio for this model, along with the partial loglikelihood score and respective p-value. The AIC and R-square was also evaluated (explain what this is in survival analysis). Somers Dxy along with a concordance statistic were also calculated as performance metrics. This process was repeated each time a model built on a training data set was fit and predicted on the test data set. The metrics taken into consideration was the average of all of the metrics computed on the test data sets. The same metrics were also computed for predictions on the training data set itself. Although the metrics were not considerably high or impressive, they did not change from training data set to test data set. 

Lastly, the model was fit to the full data set. The same process and metrics were calculated. Metrics for the full data set are not different from the results of our cross validation plan. 

Results and Discussion (or separate the two). 

The final model gave an AIC value of 69945.32. The partial loglikelihood ratio test statistic was 1307.15 with a p-value of 0. The R-squared value for this model is 0.1555. Variables that proved to be significant predictors with a p-value of less than 0.01 are: new_category (apple product, camera, game console, home, pc, and vehical), contain_unanswered, title_quesitonmark, update, prior_effort, avg_tag_score and the spline on avg_tag_length. 

While the model itself overall is significant, the metrics for the models predictive accuracy are not impressive. This indicates that perhaps a different model might perform better as a model to predict the survival time for questions. Or, a model with more quantitative variables might preform better as well.  



\bibliographystyle{ECA_jasa}
\bibliography{questions}


\end{document}
