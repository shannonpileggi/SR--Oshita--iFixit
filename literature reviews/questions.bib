@article{Chen,
abstract = {Background: Cancer survival studies are commonly analyzed using survival-time prediction models for cancer prognosis. A number of different performance metrics are used to ascertain the concordance between the predicted risk score of each patient and the actual survival time, but these metrics can sometimes conflict. Alternatively, patients are sometimes divided into two classes according to a survival-time threshold, and binary classifiers are applied to predict each patient's class. Although this approach has several drawbacks, it does provide natural performance metrics such as positive and negative predictive values to enable unambiguous assessments. Methods: We compare the survival-time prediction and survival-time threshold approaches to analyzing cancer survival studies. We review and compare common performance metrics for the two approaches. We present new randomization tests and cross-validation methods to enable unambiguous statistical inferences for several performance metrics used with the survival-time prediction approach. We consider five survival prediction models consisting of one clinical model, two gene expression models, and two models from combinations of clinical and gene expression models. Results: A public breast cancer dataset was used to compare several performance metrics using five prediction models. 1) For some prediction models, the hazard ratio from fitting a Cox proportional hazards model was significant, but the two-group comparison was insignificant, and vice versa. 2) The randomization test and cross-validation were generally consistent with the p-values obtained from the standard performance metrics. 3) Binary classifiers highly depended on how the risk groups were defined; a slight change of the survival threshold for assignment of classes led to very different prediction results. Conclusions: 1) Different performance metrics for evaluation of a survival prediction model may give different conclusions in its discriminatory ability. 2) Evaluation using a high-risk versus low-risk group comparison depends on the selected risk-score threshold; a plot of p-values from all possible thresholds can show the sensitivity of the threshold selection. 3) A randomization test of the significance of Somers' rank correlation can be used for further evaluation of performance of a prediction model. 4) The cross-validated power of survival prediction models decreases as the training and test sets become less balanced. Background},
author = {Chen, Hung-Chia and Kodell, Ralph L and Cheng, Kuang Fu and Chen, James J},
file = {:Users/lisaoshita/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Assessment of performance of survival prediction models for cancer prognosis(3).pdf:pdf},
journal = {BMC Medical Research Methodology},
title = {{Assessment of performance of survival prediction models for cancer prognosis}},
year = {2012}
}
@article{Asaduzzaman2013,
abstract = {Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overflow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overflow are answered in a very short time. However, we found that the number of unanswered questions has increased significantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overflow services. In this paper, we mine data on unanswered questions from Stack Overflow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difficult to find otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overflow.},
archivePrefix = {arXiv},
arxivId = {1306.6078},
author = {Asaduzzaman, Muhammad and Mashiyat, Ahmed Shah and Roy, Chanchal K. and Schneider, Kevin A.},
doi = {10.1109/MSR.2013.6624015},
eprint = {1306.6078},
file = {:Users/lisaoshita/Desktop/literaturereviews/Asaduzzaman{\_}MSR2013{\_}SOChallenge.pdf:pdf},
isbn = {978-1-4673-2936-1},
issn = {2160-1852},
journal = {2013 10th Working Conference on Mining Software Repositories (MSR)},
keywords = {Communities,Data mining,Knowledge discovery,Predictive models,Software,Stack Overflow,Stack Overflow services,Taxonomy,Time factors,community-based question answering services,data mining,information seekers,prediction,question answering (information retrieval),question-answer,software developers,software engineering,software engineers,unanswered questions,voluntary services},
pages = {97--100},
title = {{Answering questions about unanswered questions of Stack Overflow}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6624015},
year = {2013}
}
@article{Mamykina2011,
abstract = {This paper analyzes a Question {\&} Answer site for programmers, Stack Overflow, that dramatically improves on the utility and performance of Q{\&}A systems for technical domains. Over 92{\%} of Stack Overflow questions about expert topics are answered - in a median time of 11 minutes. Using a mixed methods approach that combines statistical data analysis with user interviews, we seek to understand this success. We argue that it is not primarily due to an a priori superior technical design, but also to the high visibility and daily involvement of the design team within the community they serve. This model of continued community leadership presents challenges to both CSCW systems research as well as to attempts to apply the Stack Overflow model to other specialized knowledge domains.},
archivePrefix = {arXiv},
arxivId = {1306.6078},
author = {Mamykina, Lena and Manoim, Bella and Mittal, Manas and Hripcsak, George and Hartmann, Bj{\"{o}}rn},
doi = {10.1145/1978942.1979366},
eprint = {1306.6078},
file = {:Users/lisaoshita/Desktop/literaturereviews/mamykina-stackoverflow-chi2011.pdf:pdf},
isbn = {9781450302289},
issn = {2160-1852},
journal = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
keywords = {Q{\&}A,mixed methods analysis},
pages = {2857},
title = {{Design Lessons from the Fastest Q{\&}A Site in the West}},
url = {http://dl.acm.org/citation.cfm?doid=1978942.1979366},
year = {2011}
}
@article{Chua2013,
abstract = {The authors investigate the interplay between answer quality and answer speed across question types in community question-answering sites (CQAs). The research questions addressed are the following: (a) How do answer quality and answer speed vary across question types? (b) How do the relationships between answer quality and answer speed vary across question types? (c) How do the best quality answers and the fastest answers differ in terms of answer quality and answer speed across question types? (d) How do trends in answer quality vary over time across ques- tion types? From the posting of 3,000 questions in six CQAs, 5,356 answers were harvested and analyzed. There was a significant difference in answer quality and answer speed across question types, and there were generally no significant relationships between answer quality and answer speed. The best quality answers had better overall answer quality than the fastest answers but generally took longer to arrive. In addition, although the trend in answer quality had been mostly random across all question types, the quality of answers appeared to improve gradually when given time. By highlighting the subtle nuances in answer quality and answer speed across question types, this study is an attempt to explore a territory of CQA research that has hitherto been relatively uncharted. Introduction},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Chua, Alton Y. K. and Banerjee, Snehasish},
doi = {10.1002/asi},
eprint = {0803.1716},
isbn = {9783848215430},
issn = {14923831},
journal = {Journal of the American Society for Information Science and Technology},
number = {10},
pages = {2058--2068},
pmid = {502955140},
title = {{So Fast So Good: An Analysis of Answer Quality and Answer Speed in Community Question-Answering Sites}},
volume = {64},
year = {2013}
}
@article{Anderson2012,
author = {Anderson, Ashton and Huttenlocher, Daniel and Kleinberg, Jon and Leskovec, Jure},
doi = {10.1145/2339530.2339665},
isbn = {9781450314626},
journal = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
keywords = {question-answering,reputation,value prediction},
pages = {850},
title = {{Discovering value from community activity on focused question answering sites}},
url = {http://dl.acm.org/citation.cfm?doid=2339530.2339665},
year = {2012}
}
@article{Harper2008,
abstract = {Question and answer (Q{\&}A) sites such as Yahoo! Answers are places where users ask questions and others answer them. In this paper, we investigate predictors of answer quality through a comparative, controlled field study of responses provided across several online Q{\&}A sites. Along with several quantitative results concerning the effects of factors such as question topic and rhetorical strategy, we present two high-level messages. First, you get what you pay for in Q{\&}A sites. Answer quality was typically higher in Google Answers (a fee-based site) than in the free sites we studied, and paying more money for an answer led to better outcomes. Second, we find that a Q{\&}A site's community of users contributes to its success. Yahoo! Answers, a Q{\&}A site where anybody can answer questions, outperformed sites that depend on specific individuals to answer questions, such as library reference services.},
author = {Harper, F Maxwell and Raban, Daphne and Rafaeli, Sheizaf and Konstan, Joseph A},
doi = {10.1145/1357054.1357191},
isbn = {9781605580111},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
keywords = {Q{\&}A,digital reference,expert services,information exchanges,information quality,knowledge networks,online community},
pages = {865--874},
title = {{Predictors of Answer Quality in Online Q{\&}A Sites}},
url = {http://dl.acm.org/citation.cfm?id=1357054.1357191},
year = {2008}
}
@article{Ponzanelli2014a,
abstract = {Technical questions and answers (Q{\&}A) services have become a valuable resource for developers. A prominent example of technical Q{\&}A website is StackOverflow (SO), which relies on a growing community of more than two millions of users who actively contribute by asking questions and providing answers. To maintain the value of this resource, poor quality questions - among the more than 6,000 asked daily - have to be filtered out. Currently, poor quality questions are manually identified and reviewed by selected users in SO, this costs considerable time and effort. Automating the process would save time and unload the review queue, improving the efficiency of SO as a resource for developers. We present an approach to automate the classification of questions according to their quality. We present an empirical study that investigates how to model and predict the quality of a question by considering as features both the contents of a post (e.g., from simple textual features to more complex readability metrics) and community-related aspects (e.g., popularity of a user in the community). Our findings show that there is indeed the possibility of at least a partial automation of the costly SO review process.},
author = {Ponzanelli, Luca and Mocci, Andrea and Bacchelli, Alberto and Lanza, Michele},
doi = {10.1109/QSIC.2014.27},
isbn = {9781479971978},
issn = {15506002},
journal = {Proceedings - International Conference on Quality Software},
keywords = {Q{\&}A,Quality,StackOverflow,Technical Forum},
pages = {343--352},
title = {{Understanding and classifying the quality of technical forum questions}},
year = {2014}
}
@article{Toba2014,
author = {Toba, Hapnes and Ming, Zhao-yan and Adriani, Mirna and Chua, Tat-seng},
doi = {10.1016/j.ins.2013.10.030},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {question answering system,user generated content},
pages = {101--115},
publisher = {Elsevier Inc.},
title = {{Discovering high quality answers in community question answering archives using a hierarchy of classifiers}},
url = {http://dx.doi.org/10.1016/j.ins.2013.10.030},
volume = {261},
year = {2014}
}
@article{Weimer2007,
abstract = {Assessing the quality of user generated con- tent is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to as- sess the quality of forum posts automati- cally and test it on data provided by Nab- ble.com. We use state-of-the-art classifi- cation techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features. We achieve an accuracy of 89{\%} on the task of automatically assessing post quality in the software domain using forum specific fea- tures. Without forum specific features, we achieve an accuracy of 82{\%}.},
author = {Weimer, Markus and Gurevych, Iryna and M{\"{u}}hlh{\"{a}}user, Max},
doi = {10.3115/1557769.1557806},
journal = {Proceedings of the ACL},
number = {June},
pages = {125--128},
title = {{Automatically assessing the post quality in online discussions on software}},
url = {http://atlas.tk.informatik.tu-darmstadt.de/Publications/2007/acl2007.pdf},
year = {2007}
}
@article{Bedrick*2013,
abstract = {Abstract The hazard ratio is a standard summary for comparing survival curves yet hazard ratios are often difficult for scientists and clinicians to interpret. Insight into the interpretation of hazard ratios is obtained by relating hazard ratios to the maximum difference and an average difference between survival probabilities. These reformulations of the hazard ratio are useful in classroom discussions of survival analysis and when discussing analyses with scientists and clinicians. Large sample distribution theory is provided for these reformulations of the hazard ratio. Two examples are used to illustrate the ideas. Abstract The hazard ratio is a standard summary for comparing survival curves yet hazard ratios are often difficult for scientists and clinicians to interpret. Insight into the interpretation of hazard ratios is obtained by relating hazard ratios to the maximum difference and an average difference between survival probabilities. These reformulations of the hazard ratio are useful in classroom discussions of survival analysis and when discussing analyses with scientists and clinicians. Large sample distribution theory is provided for these reformulations of the hazard ratio. Two examples are used to illustrate the ideas.},
author = {Bedrick*, Edward J.},
doi = {10.1080/00031305.2013.868827},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {cox model,proportional hazards model,sur-},
number = {March},
pages = {131207055018006},
title = {{Two Useful Reformulations of the Hazard Ratio}},
url = {http://dx.doi.org/10.1080/00031305.2013.868827},
volume = {1305},
year = {2013}
}
@article{Rechavi2011,
abstract = {This study investigates two questions concerning question-and-answer sites. We analyzed data from "Yahoo!Answers", including 19 months and over 20 million interactions per month. The first question investigates the differences in response time and in the average number of answers between anasker's ranking of "Best Answer" (BA) and the community's BA. The second question concerns the impact of an explicit network on several implicit network activities. The results imply that askers use response time as a parameter to choose the BA, whereas the community chooses the BA with no regard to Answer Response Time (ART). Another finding implies that if the answerer is not listed in the asker's "explicit network, " it might result in longer ranking (award) time and in a slightly decreased number of answer stars (satisfaction-rate indicator).And yet, one result might be surprising. Being a "fan" of the asker implies a long response time to the question. This finding might contradict the intuition that our friends are the first to provide answers to our questions. Several explanations of this result from different research fields are suggested in the discussion.},
author = {Rechavi, Amit and Rafaeli, Sheizaf},
doi = {10.1109/PASSAT/SocialCom.2011.67},
isbn = {9780769545783},
journal = {Proceedings - 2011 IEEE International Conference on Privacy, Security, Risk and Trust and IEEE International Conference on Social Computing, PASSAT/SocialCom 2011},
keywords = {Response time,Satisfaction rates,Wisdom of the crowds,Yahoo! answers},
pages = {904--909},
title = {{Not all is gold that glitters response time {\&} satisfaction rates in Yahoo! answers}},
year = {2011}
}
@article{Yao2015,
author = {Yao, Yuan and Tong, Hanghang and Xie, Tao and Akoglu, Leman and Xu, Feng and Lu, Jian},
doi = {10.1016/j.ins.2014.12.038},
issn = {0020-0255},
journal = {INFORMATION SCIENCES},
pages = {70--82},
publisher = {Elsevier Inc.},
title = {{Detecting high-quality posts in community question answering sites}},
url = {http://dx.doi.org/10.1016/j.ins.2014.12.038},
volume = {302},
year = {2015}
}
@article{Zhang2011,
author = {Zhang, Zhongfeng and Li, Qiudan},
doi = {10.1016/j.eswa.2010.12.052},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {community question answering,cqa},
number = {6},
pages = {6848--6855},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications QuestionHolic : Hot topic discovery and trend analysis in community question answering systems}},
url = {http://dx.doi.org/10.1016/j.eswa.2010.12.052},
volume = {38},
year = {2011}
}
@article{Zhou2012,
abstract = {Automatic Subjective Question Answering (ASQA), which aims at answering users' subjective questions using summaries of multiple opinions, becomes increasingly important. One challenge of ASQA is that expected answers for subjective questions may not readily exist in theWeb. The rising and popularity of Community Question Answering (CQA) sites, which provide platforms for people to post and answer questions, provides an alternative to ASQA. One important task of ASQA is question subjectivity identification, which identifies whether a user is asking a subjective question. Unfortunately, there has been little labeled training data available for this task. In this paper, we propose an approach to collect training data automatically by utilizing social signals in CQA sites without involving any manual labeling. Experimental results show that our data-driven approach achieves 9:37{\%} relative improvement over the supervised approach using manually labeled data, and achieves 5:15{\%} relative gain over a stateof- the-art semi-supervised approach. In addition, we propose several heuristic features for question subjectivity identification. By adding these features, we achieve 11:23{\%} relative improvement over word n-gram feature under the same experimental setting.},
author = {Zhou, Tc and Si, Xiance and Chang, Ey and King, Irwin and Lyu, Mr},
isbn = {9781577355687},
journal = {Aaai},
keywords = {Special Track on Artificial Intelligence and the W},
pages = {164--170},
title = {{A Data-Driven Approach to Question Subjectivity Identification in Community Question Answering.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/4976/5134},
year = {2012}
}
@article{Duijn2015,
abstract = {—Stack Overflow (SO) is a question and answers (Q{\&}A) web platform on software development that is gaining in popularity. With increasing popularity often comes a very unwelcome side effect: A decrease in the average quality of a post. To keep Q{\&}A websites like SO useful it is vital that this side effect is countered. Previous research proved to be reasonably successful in using properties of questions to help identify low quality questions to be later reviewed and improved. We present an approach to improve the classification of high and low quality questions based on a novel source of information: the analysis of the code fragments in SO questions. We show that we get similar performance to classification based on a wider set of metrics thus potentially reaching a better overall classification.},
author = {Duijn, Maarten and Ku??era, Adam and Bacchelli, Alberto},
doi = {10.1109/MSR.2015.51},
isbn = {9780769555942},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Accuracy,Algorithm design and analysis,Classification algorithms,Correlation,Decision trees,Java,Measurement},
pages = {410--413},
title = {{Quality questions need quality code: Classifying code fragments on stack overflow}},
volume = {2015-Augus},
year = {2015}
}
@article{Bhat2014,
author = {Bhat, Vasudev and Gokhale, Adheesh and Jadhav, Ravi and Pudipeddi, Ravi and Akoglu, Leman},
isbn = {9781479958771},
journal = {IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
keywords = {-online communities,behavior,collec-,evidential feature analysis,human,on,probability distribution of the,question answering sites,question response time,response time of questions,tive intelligence,user engagement},
number = {Asonam},
pages = {328--335},
title = {{Min ( e ) d Your Tags : Analysis of Question Response Time in StackOverflow}},
year = {2014}
}
@article{Ravi2014,
abstract = {Asking the right question in the right way is an art (and a science). In a community question-answering setting, a good question is not just one that is found to be useful by other people—a question is good if it is also pre-sented clearly and shows prior research. Using a com-munity question-answering site that allows voting over the questions, we show that there is a notion of question quality that goes beyond mere popularity. We present techniques using latent topical models to automatically predict the quality of questions based on their content. Our best system achieves a prediction accuracy of 72{\%}, beating out strong baselines by a significant amount. We also examine the effect of question quality on the dy-namics of user behavior and the longevity of questions.},
author = {Ravi, Sujith and Pang, Bo and Rastagori, VIbhor and Kumar, Ravi},
isbn = {978-1-57735-657-8},
journal = {International AAAI Conference on Weblogs and Social Media},
keywords = {Full Papers},
number = {1},
pages = {426--435},
title = {{Great Question ! Question Quality in Community Q {\&} A}},
year = {2014}
}
