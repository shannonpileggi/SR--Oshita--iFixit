@article{Correa2013,
abstract = {Stack Overflow is widely regarded as the most popular Community driven Question Answering (CQA) website for programmers. Questions posted on Stack Overflow which are not related to programming topics, are marked as ‘closed' by experienced users and community moderators. A question can be ‘closed' for five reasons – duplicate, off-topic, subjective, not a real question and too localized. In this work, we present the first study of ‘closed' questions on Stack Overflow. We download 4 years of publicly available data which contains 3.4 Million questions. We first analyze and characterize the complete set of 0.1 Million ‘closed' questions. Next, we use a machine learning framework and build a predictive model to identify a ‘closed' question at the time of question creation. One of our key findings is that despite being marked as ‘closed', subjective questions contain high information value and are very popular with the users. We observe an increasing trend in the percentage of closed questions over time and find that this increase is positively correlated to the number of newly registered users. In addition, we also see a decrease in community participation to mark a ‘closed' question which has led to an increase in moderation job time. We also find that questions closed with the Duplicate and Off Topic labels are relatively more prone to reputation gaming. Our analysis suggests broader implications for content quality maintenance on CQA websites. For the ‘closed' question prediction task, we make use of multiple genres of feature sets based on - user profile , community process, textual style and question content. We use a state-of-art machine learning classifier based on an ensemble framework and achieve an overall accuracy of 70.3{\%}. Analysis of the feature space reveals that ‘closed' questions are relatively less informative and descriptive than non-‘closed' questions. To the best of our knowledge, this is the first experimental study to analyze and predict ‘closed' questions on Stack Overflow.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.7291v1},
author = {Correa, Denzil and Sureka, Ashish},
doi = {10.1145/2512938.2512954},
eprint = {arXiv:1307.7291v1},
file = {:Users/lisaoshita/Desktop/manuscript/literaturereviews/p201-correa.pdf:pdf},
isbn = {9781450320849},
issn = {1450320848},
journal = {Proceedings of the first ACM Conference on Online Social Networks},
pages = {201--212},
title = {{Fit or unfit: Analysis and prediction of 'closed questions' on Stack Overflow}},
url = {http://dl.acm.org/citation.cfm?doid=2512938.2512954},
year = {2013}
}
@inproceedings{Mahmud2013,
abstract = {Proceedings of the Seventh International AAAI Conference on Weblogs and Social Media},
author = {Mahmud, Jalal and Chen, Jilin and Nichols, Jeffrey},
booktitle = {Seventh International AAAI Conference on Weblogs and Social Media},
file = {:Users/lisaoshita/Desktop/twitter{\_}responsetime.pdf:pdf},
keywords = {Short Paper},
pages = {697--700},
title = {{When Will You Answer This? Estimating Response Time in Twitter}},
url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/viewPDFInterstitial/6072/6328},
year = {2013}
}
@inproceedings{Blumenstock2008,
abstract = {Wikipedia, the free encyclopedia, now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric word count for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.},
author = {Blumenstock, Joshua E},
booktitle = {Proceedings of the 17th International Conference on World Wide Web},
doi = {10.1145/1367497.1367673},
file = {:Users/lisaoshita/Desktop/p1095-blumenstock.pdf:pdf},
isbn = {9781605580852},
issn = {08963207},
keywords = {information quality,wikipedia,word count},
pages = {1095--1096},
publisher = {ACM},
title = {{Size matters: word count as a measure of quality on wikipedia}},
url = {http://portal.acm.org/citation.cfm?id=1367673},
year = {2008}
}
@article{Fu2015,
abstract = {As an increasing important source of information and knowledge, social questioning and answering sites (social Q{\&}A) have attracted significant attention from industry and academia, as they address the challenge of evaluating and predicting the quality of answers on such sites. However, few previous studies examined the answer quality by considering knowledge domains or topics as a potential factor. To fill this gap, a model consisting of 24 textual and non-textual features of answers was developed in this study to evaluate and predict answer quality for social Q{\&}A, and the model was applied to identify and compare useful features for predicting high-quality answers across four knowledge domains, including science, technology, art, and recreation. The findings indicate that review and user features are the most powerful indicators of high-quality answers regardless of knowledge domains, while the usefulness of textual features (length, structure, and writing style) varies across different knowledge domains. In the future, the findings could be applied to automatically assessing answer quality and quality control in social Q{\&}AMP;A.},
author = {Fu, Hengyi and Wu, Shuheng and Oh, Sanghee},
doi = {10.1002/pra2.2015.145052010088},
file = {:Users/lisaoshita/Desktop/manuscript/literaturereviews/a88-fu.pdf:pdf},
issn = {23739231},
journal = {Proceedings of the Association for Information Science and Technology},
keywords = {Social question and answer sites,answer quality,quality assessment},
number = {1},
pages = {1--5},
title = {{Evaluating answer quality across knowledge domains: Using textual and non-textual features in social Q{\&}A}},
volume = {52},
year = {2015}
}
@article{Grambsch1994,
abstract = {Nonproportional hazards can often be expressed by extending the Cox model to include time varying coefficients; e.g., for a single covariate, the hazard function for subject i is modelled as exp {\{}$\beta$(t)Zi(t){\}}. A common example is a treatment effect that decreases with time. We show that the function $\beta$i(t) can be directly visualized by smoothing an appropriate residual plot. Also, many tests of proportional hazards, including those of Cox (1972), Gill {\&} Schumacher (1987), Harrell (1986), Lin (1991), Moreau, O'Quigley {\&} Mesbah (1985), Nagelkerke, Oosting {\&} Hart (1984), O'Quigley {\&} Pessione (1989), Schoenfeld (1980) and Wei (1984) are related to time-weighted score tests of the proportional hazards hypothesis, and can be visualized as a weighted least-squares line fitted to the residual plot.},
author = {Grambsch, Patricia and Therneau, Terry},
file = {:Users/lisaoshita/Desktop/cox{\_}zph.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {515--526},
title = {{Proportional hazards tests and diagnostics based on weighted residuals}},
url = {https://doi.org/10.1093/biomet/81.3.515},
volume = {81},
year = {1994}
}
@article{Austin2012,
abstract = {BACKGROUND When outcomes are binary, the c-statistic (equivalent to the area under the Receiver Operating Characteristic curve) is a standard measure of the predictive accuracy of a logistic regression model. METHODS An analytical expression was derived under the assumption that a continuous explanatory variable follows a normal distribution in those with and without the condition. We then conducted an extensive set of Monte Carlo simulations to examine whether the expressions derived under the assumption of binormality allowed for accurate prediction of the empirical c-statistic when the explanatory variable followed a normal distribution in the combined sample of those with and without the condition. We also examine the accuracy of the predicted c-statistic when the explanatory variable followed a gamma, log-normal or uniform distribution in combined sample of those with and without the condition. RESULTS Under the assumption of binormality with equality of variances, the c-statistic follows a standard normal cumulative distribution function with dependence on the product of the standard deviation of the normal components (reflecting more heterogeneity) and the log-odds ratio (reflecting larger effects). Under the assumption of binormality with unequal variances, the c-statistic follows a standard normal cumulative distribution function with dependence on the standardized difference of the explanatory variable in those with and without the condition. In our Monte Carlo simulations, we found that these expressions allowed for reasonably accurate prediction of the empirical c-statistic when the distribution of the explanatory variable was normal, gamma, log-normal, and uniform in the entire sample of those with and without the condition. CONCLUSIONS The discriminative ability of a continuous explanatory variable cannot be judged by its odds ratio alone, but always needs to be considered in relation to the heterogeneity of the population.},
author = {Austin, Peter C. and Steyerberg, Ewout W.},
doi = {10.1186/1471-2288-12-82},
file = {:Users/lisaoshita/Desktop/concordance.pdf:pdf},
isbn = {1471-2288 (Electronic)$\backslash$r1471-2288 (Linking)},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Area under the receiver operating characteristic c,Discrimination,Logistic regression,Prediction,Predictive accuracy,Predictive model,ROC curve,Regression model,c-statistic},
pages = {1--8},
pmid = {22716998},
title = {{Interpreting the concordance statistic of a logistic regression model: Relation to the variance and odds ratio of a continuous explanatory variable}},
volume = {12},
year = {2012}
}
@article{Bellera2010,
abstract = {BACKGROUND The Cox model relies on the proportional hazards (PH) assumption, implying that the factors investigated have a constant impact on the hazard - or risk - over time. We emphasize the importance of this assumption and the misleading conclusions that can be inferred if it is violated; this is particularly essential in the presence of long follow-ups. METHODS We illustrate our discussion by analyzing prognostic factors of metastases in 979 women treated for breast cancer with surgery. Age, tumour size and grade, lymph node involvement, peritumoral vascular invasion (PVI), status of hormone receptors (HRec), Her2, and Mib1 were considered. RESULTS Median follow-up was 14 years; 264 women developed metastases. The conventional Cox model suggested that all factors but HRec, Her2, and Mib1 status were strong prognostic factors of metastases. Additional tests indicated that the PH assumption was not satisfied for some variables of the model. Tumour grade had a significant time-varying effect, but although its effect diminished over time, it remained strong. Interestingly, while the conventional Cox model did not show any significant effect of the HRec status, tests provided strong evidence that this variable had a non-constant effect over time. Negative HRec status increased the risk of metastases early but became protective thereafter. This reversal of effect may explain non-significant hazard ratios provided by previous conventional Cox analyses in studies with long follow-ups. CONCLUSIONS Investigating time-varying effects should be an integral part of Cox survival analyses. Detecting and accounting for time-varying effects provide insights on some specific time patterns, and on valuable biological information that could be missed otherwise.},
author = {Bellera, Carine A. and MacGrogan, Ga{\"{e}}tan and Debled, Marc and {De Lara}, Christine Tunon and Brouste, V{\'{e}}ronique and Mathoulin-P{\'{e}}lissier, Simone},
doi = {10.1186/1471-2288-10-20},
file = {:Users/lisaoshita/Desktop/manuscript/CR Predictive Modeling /phassumption{\_}violation.pdf:pdf},
isbn = {1471-2288},
issn = {14712288},
journal = {BMC Medical Research Methodology},
pmid = {20233435},
title = {{Variables with time-varying effects and the Cox model: Some statistical concepts illustrated with a prognostic factor study in breast cancer}},
volume = {10},
year = {2010}
}
@article{Hammermeister1979,
abstract = {A progression of univariate followed by multivariate analyses was applied to 46 variables selected from the clinical examination, exercise test, coronary arteriography, and quantitative angiographic assessment of left ventricular function in patients with coronary disease to determine those variables most predictive of survival. For the 733 medically treated patients, the final Cox's regression analysis showed that the left ventricular ejection fraction was most predictive of survival, followed by age, number of vessels with stenosis(es) greater than or equal to 70{\%}, and ventricular arrhythmia on the resting electrocardiogram. For the 1870 surgically treated patients, ventricular arrhythmia on the resting electrocardiogram was most predictive of survival followed by ejection fraction, heart murmur, left main coronary artery stenosis greater than or equal to 50{\%}, and use of diuretic agents.},
author = {Hammermeister, K E and DeRouen, T A and Dodge, H T},
doi = {10.1161/01.CIR.59.3.421},
file = {:Users/lisaoshita/Desktop/univ{\_}analysis.pdf:pdf},
journal = {American Heart Association, Inc.},
number = {3},
pages = {421--430},
title = {{Variables predictive of survival in patients with coronary disease. Selection by univariate and multivariate analyses from the clinical, electrocardiographic, exercise, arteriographic, and quantitative angiographic evaluations.}},
url = {http://dx.doi.org/10.1161/01.CIR.59.3.421},
volume = {59},
year = {1979}
}
@article{Rodriguez2010,
abstract = {In the machine learning field, the performance of a classifier is usually measured in terms of prediction error. In most real-world problems, the error cannot be exactly calculated and it must be estimated. Therefore, it is important to choose an appropriate estimator of the error. This paper analyzes the statistical properties, bias and variance, of the kappa-fold cross-validation classification error estimator (kappa-cv). Our main contribution is a novel theoretical decomposition of the variance of the kappa-cv considering its sources of variance: sensitivity to changes in the training set and sensitivity to changes in the folds. The paper also compares the bias and variance of the estimator for different values of kappa. The experimental study has been performed in artificial domains because they allow the exact computation of the implied quantities and we can rigorously specify the conditions of experimentation. The experimentation has been performed for two classifiers (naive Bayes and nearest neighbor), different numbers of folds, sample sizes, and training sets coming from assorted probability distributions. We conclude by including some practical recommendation on the use of kappa-fold cross validation.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Rodr{\'{i}}guez, Juan Diego and P{\'{e}}rez, Aritz and Lozano, Jose Antonio},
doi = {10.1109/TPAMI.2009.187},
eprint = {9605103},
file = {:Users/lisaoshita/Desktop/kfold{\_}cv{\_}sensitivity.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {analysis and machine intelligence,e transactions on pattern},
number = {3},
pages = {569--575},
pmid = {20075479},
primaryClass = {cs},
title = {{Sensitivity analysis of kappa-fold cross validation in prediction error estimation}},
volume = {32},
year = {2010}
}
@incollection{Moore2010,
abstract = {Applied Survival Analysis Using R covers the main principles of survival analysis, gives examples of how it is applied, and teaches how to put those principles to use to analyze data using R as a vehicle. Survival data, where the primary outcome is time to a specific event, arise in many areas of biomedical research, including clinical trials, epidemiological studies, and studies of animals. Many survival methods are extensions of techniques used in linear regression and categorical data, while other aspects of this field are unique to survival data. This text employs numerous actual examples to illustrate survival curve estimation, comparison of survivals of different groups, proper accounting for censoring and truncation, model variable selection, and residual analysis. Because explaining survival analysis requires more advanced mathematics than many other statistical topics, this book is organized with basic concepts and most frequently used procedures covered in earlier chapters, with more advanced topics near the end and in the appendices. A background in basic linear regression and categorical data analysis, as well as a basic knowledge of calculus and the R system, will help the reader to fully appreciate the information presented. Examples are simple and straightforward while still illustrating key points, shedding light on the application of survival analysis in a way that is useful for graduate students, researchers, and practitioners in biostatistics.},
author = {Moore, Dirk},
chapter = {5},
doi = {10.1007/978-3-319-31245-3},
edition = {1},
editor = {Gentleman, Robert and Hornik, Kurt and Parmigiani, Giovanni},
file = {:Users/lisaoshita/Desktop/manuscript/CR Predictive Modeling /Applied Survival Analysis Using R.pdf:pdf},
isbn = {9783319312439},
number = {March},
pages = {226},
publisher = {Springer International Publishing},
title = {{Applied Survival Analysis Using R}},
year = {2010}
}
@incollection{Kleinbaum2011,
abstract = {This greatly expanded second edition of Survival Analysis- A Self-learning$\backslash$nText provides a highly readable description of state-of-the-art methods of$\backslash$nanalysis of survival/event-history data. This text is suitable for researchers$\backslash$nand statisticians working in the medical and other life sciences as well as$\backslash$nstatisticians in academia who teach introductory and second-level courses on$\backslash$nsurvival analysis. The second edition continues to use the unique "lecture-$\backslash$nbook" format of the first (1996) edition with the addition of three new$\backslash$nchapters on advanced topics:$\backslash$n$\backslash$nChapter 7: Parametric Models$\backslash$n$\backslash$nChapter 8: Recurrent events$\backslash$n$\backslash$nChapter 9: Competing Risks.$\backslash$n$\backslash$nAlso, the Computer Appendix has been revised to provide step-by-step$\backslash$ninstructions for using the computer packages {\{}STATA{\}} (Version 7.0), {\{}SAS{\}} (Version$\backslash$n8.2), and {\{}SPSS{\}} (version 11.5) to carry out the procedures presented in the$\backslash$nmain text.$\backslash$n$\backslash$nThe original six chapters have been modified slightly$\backslash$n$\backslash$n{\_}$\backslash$n$\backslash$nto expand and clarify aspects of survival analysis in response to suggestions$\backslash$nby students, colleagues and reviewers, and$\backslash$n$\backslash$nto add theoretical background, particularly regarding the formulation of the$\backslash$n(partial) likelihood functions for proportional hazards, stratified, and$\backslash$nextended Cox regression models$\backslash$n$\backslash$n{\_}$\backslash$n$\backslash$nDavid Kleinbaum is Professor of Epidemiology at the Rollins School of Public$\backslash$nHealth at Emory University, Atlanta, Georgia. Dr. Kleinbaum is internationally$\backslash$nknown for innovative textbooks and teaching on epidemiological methods,$\backslash$nmultiple linear regression, logistic regression, and survival analysis. He has$\backslash$nprovided extensive worldwide short-course training in over 150 short courses$\backslash$non statistical and epidemiological methods. He is also the author of {\{}ActivEpi{\}}$\backslash$n(2002), an interactive computer-based instructional text on fundamentals of$\backslash$nepidemiology, which has been used in a variety of educational environments$\backslash$nincluding distance learning.$\backslash$n$\backslash$nMitchel Klein is Research Assistant Professor with a joint appointment in the$\backslash$nDepartment of Environmental and Occupational Health ({\{}EOH{\}}) and the Department$\backslash$nof Epidemiology, also at the Rollins School of Public Health at Emory$\backslash$nUniversity. Dr. Klein is also co-author with Dr. Kleinbaum of the second$\backslash$nedition of Logistic Regression- A {\{}Self-Learning{\}} Text (2002). He has regularly$\backslash$ntaught epidemiologic methods courses at Emory to graduate students in public$\backslash$nhealth and in clinical medicine. He is responsible for the epidemiologic$\backslash$nmethods training of physicians enrolled in Emory's Master of Science in$\backslash$nClinical Research Program, and has collaborated with Dr. Kleinbaum both$\backslash$nnationally and internationally in teaching several short courses on various$\backslash$ntopics in epidemiologic methods.},
archivePrefix = {arXiv},
arxivId = {1002.2080},
author = {Kleinbaum, David and Klein, Mitchell},
booktitle = {Biometrical Journal},
chapter = {1},
doi = {10.1016/B978-0-12-387667-6.00013-0},
edition = {Third},
editor = {Gail, M and Krickeberg, K and Samet, J.M. and Tsiatis, A and Wong, W},
eprint = {1002.2080},
file = {:Users/lisaoshita/Desktop/survival{\_}analysis{\_}text.pdf:pdf},
isbn = {1441966455},
issn = {18715125},
keywords = {statistics for biology and health},
pages = {715},
pmid = {22469268},
publisher = {Springer-Verlag New York},
title = {{Survival Analysis: A Self-Learning Text, Third Edition (Statistics for Biology and Health)}},
year = {2011}
}
@article{Bland1998,
abstract = {As we have observed, 1 analysis of survival data requires special techniques because some observations are censored as the event of interest has not occurred for all patients. For example, when patients are recruited over two years one recruited at the end of the study may be alive at one year follow up, whereas one recruited at the start may have died after two years. The patient who died has a longer observed survival than the one who still survives and whose ultimate survival time is unknown. The table shows data from a study of conception in subfertile women. 2 The event is conception, and women " survived " until they conceived. One woman conceived after 16 months (menstrual cycles), whereas several were followed for shorter time periods during which they did not conceive; their time to conceive was thus censored. We wish to estimate the proportion surviving (not having conceived) by any given time, which is also the estimated probability of survival to that time for a member of the population from which the sample is drawn. Because of the censoring we use the Kaplan-Meier method. For each time interval we estimate the probability that those who have survived to the beginning will survive to the end. This is a condi-tional probability (the probability of being a survivor at the end of the interval on condition that the subject was a survivor at the beginning of the interval). Survival to any time point is calculated as the product of the conditional probabilities of surviving each time interval. These data are unusual in representing months (menstrual cycles); usually the conditional probabilities relate to days. The calculations are simpli-fied by ignoring times at which there were no recorded survival times (whether events or censored times). In the example, the probability of surviving for two months is the probability of surviving the first month times the probability of surviving the second month given that the first month was survived. Of 38 women, 32 survived the first month, or 0.842. Of the 32 women at the start of the second month (" at risk " of conception), 27 had not conceived by the end of the month. The conditional probability of surviving the second month is thus 27/32 = 0.844, and the overall probability of surviving (not conceiving) after two months is 0.842 × 0.844 = 0.711. We continue in this way to the end of the table, or until we reach the last event. Observations censored at a given time affect the number still at risk at the start of the next month. The estimated probability changes only in months when there is a conception. In practice, a computer is used to do these calculations. Standard errors and confidence intervals for the estimated survival probabilities can be found by Greenwood's method. 3 Survival probabilities are usually presented as a survival curve (figure). The " curve " is a step function, with sudden changes in the estimated probability corresponding to times at which an event was observed. The times of the censored data are indicated by short vertical lines. There are three assumptions in the above. Firstly, we assume that at any time patients who are censored have the same survival prospects as those who continue to be followed. This assumption is not easily testable. Censoring may be for various reasons. In the conception study some women had received hormone treatment to promote ovulation, and others had stopped trying to conceive. Thus they were no longer part of the population we wanted to study, and their survival times were censored. In most studies some subjects drop out for reasons unrelated to the condition under study (for example, emigration) If, however, for some patients in this study censoring was related to failure to conceive this would have biased the estimated survival probabilities downwards. Secondly, we assume that the survival probabilities are the same for subjects recruited early and late in the study. In a long term observational study of patients with cancer, for example, the case mix may change over the period of recruitment, or there may be an innova-tion in ancillary treatment. This assumption may be tested, provided we have enough data to estimate survival curves for different subsets of the data. Thirdly, we assume that the event happens at the time specified. This is not a problem for the conception data, but could be, for example, if the event were recur-rence of a tumour which would be detected at a regu-lar examination. All we would know is that the event happened between two examinations. This imprecision would bias the survival probabilities upwards. When the observations are at regular intervals this can be allowed for quite easily, using the actuarial method. 3},
author = {Bland, J M. and Altman, D. G},
doi = {10.1136/bmj.317.7172.1572},
file = {:Users/lisaoshita/Desktop/kaplan{\_}meier{\_}est.pdf:pdf},
isbn = {0959-8138 (Print)$\backslash$r0959-535X (Linking)},
issn = {0959-8138},
journal = {Bmj},
number = {7172},
pages = {1572--1580},
pmid = {9836663},
title = {{Statistics Notes: Survival probabilities (the Kaplan-Meier method)}},
url = {http://www.bmj.com/cgi/doi/10.1136/bmj.317.7172.1572},
volume = {317},
year = {1998}
}
@book{Harrell2015,
abstract = {Multivariable regression models are widely used in health science research, mainly for two purposes: prediction and effect estimation. Various strategies have been recommended when building a regression model: a) use the right statistical method that matches the structure of the data; b) ensure an appropriate sample size by limiting the number of variables according to the number of events; c) prevent or correct for model overfitting; d) be aware of the problems associated with automatic variable selection procedures (such as stepwise), and e) always assess the performance of the final model in regard to calibration and discrimination measures. If resources allow, validate the prediction model on external data.},
author = {Harrell, Frank E.},
booktitle = {Springer Series in Statistics},
doi = {10.1007/978-1-4757-3462-1},
file = {:Users/lisaoshita/Desktop/CR Predictive Modeling /RegressionModelingStrategies.pdf:pdf},
isbn = {978-1-4419-2918-1},
issn = {0172-7397},
keywords = {Department of Biostatistics,Nashville,School of Medicine,Tennessee,USA,Vanderbilt University},
number = {6},
pages = {501--507},
pmid = {21531065},
title = {{Regression Modeling Strategies}},
url = {papers://55069ee6-504c-4f60-bfa9-053c4dcabb39/Paper/p398{\%}5Cnhttp://link.springer.com/10.1007/978-3-319-19425-7{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/21531065{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/S0300893211003502{\%}5Cnhttp://link.springer.com/10.1},
volume = {64},
year = {2015}
}
@article{Hopfer2013,
abstract = {To examine the influence of conduct disorder (CD) on substance use initiation.},
author = {Hopfer, Christian and Salomonsen-Sautel, Stacy and Mikulich-Gilbertson, Susan and Min, Sung-Joon and McQueen, Matt and Crowley, Thomas and Young, Susan and Corley, Robin and Sakai, Joseph and Thurstone, Christian and Hoffenberg, Analice and Hartman, Christie and Hewitt, John},
doi = {10.1016/j.jaac.2013.02.014},
isbn = {0890-8567},
issn = {1527-5418},
journal = {Journal of the American Academy of Child and Adolescent Psychiatry},
keywords = {applied example},
mendeley-tags = {applied example},
number = {5},
pages = {511--518.e4},
pmid = {23622852},
publisher = {Elsevier Inc},
title = {{Conduct disorder and initiation of substance use: a prospective longitudinal study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23622852},
volume = {52},
year = {2013}
}
@article{Krivtsov2002,
abstract = {The paper considers an empirical approach to the root-cause analysis of a certain kind of automobile tire failure. Tire life data are obtained from a laboratory test, which is developed to duplicate field failures. A number of parameters related to tire geometry and physical properties are selected as explanatory variables that potentially affect a tire???s life on test. Analysis of the life test data is performed via the Cox survival regression model. The paper also elaborates on the application of an ordinary (non-survival) linear regression to modeling the failure initiation and propagation. The developed statistical models help to identify the elements of tire design affecting the probability of tire failure due to the failure mode in question. ?? 2002 Published by Elsevier Science Ltd.},
author = {Krivtsov, V. V. and Tananko, D. E. and Davis, T. P.},
doi = {10.1016/S0951-8320(02)00169-2},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Hazard,Multiple regression,Reliability,Survival regression,applied example},
mendeley-tags = {applied example},
number = {3},
pages = {267--273},
title = {{Regression approach to tire reliability analysis}},
volume = {78},
year = {2002}
}
@article{Do2003,
author = {Do, Author Kim-anh and Johnson, Marcella M and Doherty, Dorota A and Lee, J Jack and Feng, Xi and Dong, Qiong and Hong, Waun K and Khuri, Fadlo R and Fu, Karen K and Spitz, Margaret R and Causes, Source Cancer and Mar, No and Do, Kim-anh and Johnson, Marcella M and Doherty, Dorota A and Lee, J Jack and Wu, Xi Feng and Dong, Qiong and Waun, K},
keywords = {alcohol drinking,head and neck neoplasms,risk factors,second primary,smoking},
number = {2},
pages = {131--138},
title = {{Second Primary Tumors in Patients with Upper Aerodigestive Tract Cancers : Joint Effects of Smoking and Alcohol ( United States ) Published by : Springer Stable URL : http://www.jstor.org/stable/3553625 Accessed : 26-05-2016 09 : 36 UTC Second primary tum}},
volume = {14},
year = {2003}
}
@article{Sulkowski2002,
abstract = {Context Conflicting reports exist regarding the effect of hepatitis C virus (HCV) on the progression of human immunodeficiency virus (HIV) disease. Objective To assess the effect of HCV infection on clinical and immunologic pro- gression of HIV disease and immunologic response to highly active antiretroviral therapy (HAART). Design Prospective cohort study. Setting University-based, urban HIV clinic in the United States. Patients There were 1955 patients enrolled between January 1995 and January 2001 whowere eligible for analysis because of having at least 1 return visit to the clinic and being free of acquired immunodeficiency syndrome (AIDS) at enrollment. Median (in- terquartile range) length of follow-up was 2.19 (1.00-3.50) years for HCV-infected and 2.00 (1.00-3.00) years for HCV-uninfected patients. Main Outcome Measures Progression to an AIDS-defining illness, survival, and progression to a CD4 cell count below 200/µL; CD4 cell count change following ini- tiation of effective HAART (resulting in a viral load of ?400 copies/mL recorded at ?75{\%} of measurements). Results No difference was detected in the risk of acquiring an AIDS-defining illness (HCV-infected patients, 231 events [26.4{\%}] and HCV-uninfected patients, 264 events [24.4{\%}]; relative hazard [RH], 1.03; 95{\%} confidence interval [CI], 0.86-1.23) or in the risk of death (HCV-infected patients, 153 deaths [17.5{\%}] and HCV- uninfected patients, 168 deaths [15.5{\%}]; RH, 1.05; 95{\%} CI, 0.85 -1.30). Although an increased risk of death was detected in the subgroup of 429 HCV-infected patients with a baseline CD4 cell count of 50/µL through 200/µL (RH, 1.51; 95{\%} CI, 1.01-2.27), after adjustment for exposure to HAART and its effectiveness in a multi- variate Cox regression analysis, death was not independently associated with HCV infection in this subgroup (RH, 1.01; 95{\%} CI, 0.65-1.56). Similarly, in those receiving effective HAART (n=208), there was no difference in the increase in CD4 cell count or CD4 percentage during HAART in HCV-infected compared with HCV-uninfected patients.Conclusions Among patients in this urban US cohort, we did not detect evidence that HCV infection substantially alters the risk of dying, developing AIDS, or respond- ing immunologically to HAART, especially after accounting for differences in its ad- ministration and effectiveness.},
author = {Sulkowski, Mark S and Moore, Richard D and Chaisson, Richard E and Thomas, David L},
doi = {10.1097/00019048-200205000-00059},
isbn = {0098-7484},
issn = {0098-7484},
journal = {Jama},
keywords = {applied example},
mendeley-tags = {applied example},
number = {2},
pages = {199--206},
pmid = {12095384},
title = {{Hepatitis C and Progression of HIV Disease}},
volume = {288},
year = {2002}
}
@article{Carey2006,
abstract = {CONTEXT: Gene expression analysis has identified several breast cancer subtypes, including basal-like, human epidermal growth factor receptor-2 positive/estrogen receptor negative (HER2+/ER-), luminal A, and luminal B. OBJECTIVES: To determine population-based distributions and clinical associations for breast cancer subtypes. DESIGN, SETTING, AND PARTICIPANTS: Immunohistochemical surrogates for each subtype were applied to 496 incident cases of invasive breast cancer from the Carolina Breast Cancer Study (ascertained between May 1993 and December 1996), a population-based, case-control study that oversampled premenopausal and African American women. Subtype definitions were as follows: luminal A (ER+ and/or progesterone receptor positive [PR+], HER2-), luminal B (ER+ and/or PR+, HER2+), basal-like (ER-, PR-, HER2-, cytokeratin 5/6 positive, and/or HER1+), HER2+/ER- (ER-, PR-, and HER2+), and unclassified (negative for all 5 markers). MAIN OUTCOME MEASURES: We examined the prevalence of breast cancer subtypes within racial and menopausal subsets and determined their associations with tumor size, axillary nodal status, mitotic index, nuclear pleomorphism, combined grade, p53 mutation status, and breast cancer-specific survival. RESULTS: The basal-like breast cancer subtype was more prevalent among premenopausal African American women (39{\%}) compared with postmenopausal African American women (14{\%}) and non-African American women (16{\%}) of any age (P{\textless}.001), whereas the luminal A subtype was less prevalent (36{\%} vs 59{\%} and 54{\%}, respectively). The HER2+/ER- subtype did not vary with race or menopausal status (6{\%}-9{\%}). Compared with luminal A, basal-like tumors had more TP53 mutations (44{\%} vs 15{\%}, P{\textless}.001), higher mitotic index (odds ratio [OR], 11.0; 95{\%} confidence interval [CI], 5.6-21.7), more marked nuclear pleomorphism (OR, 9.7; 95{\%} CI, 5.3-18.0), and higher combined grade (OR, 8.3; 95{\%} CI, 4.4-15.6). Breast cancer-specific survival differed by subtype (P{\textless}.001), with shortest survival among HER2+/ER- and basal-like subtypes. CONCLUSIONS: Basal-like breast tumors occurred at a higher prevalence among premenopausal African American patients compared with postmenopausal African American and non-African American patients in this population-based study. A higher prevalence of basal-like breast tumors and a lower prevalence of luminal A tumors could contribute to the poor prognosis of young African American women with breast cancer.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Carey, Lisa A and Perou, Charles M and Livasy, Chad A and Dressler, Lynn G and Cowan, David and Conway, Kathleen and Karaca, Gamze and Troester, Melissa A and Tse, Chiu Kit and Edmiston, Sharon and Deming, Sandra L and Geradts, Joseph and Cheang, Maggie C U and Nielsen, Torsten O and Moorman, Patricia G and Earp, H Shelton and Millikan, Robert C},
doi = {10.1001/jama.295.21.2492},
eprint = {arXiv:1011.1669v3},
isbn = {1538-3598 (Electronic)},
issn = {1538-3598},
journal = {JAMA : the journal of the American Medical Association},
keywords = {Adult,African Americans,Aged,Breast Neoplasms,Breast Neoplasms: classification,Breast Neoplasms: ethnology,Breast Neoplasms: metabolism,Breast Neoplasms: mortality,Breast Neoplasms: pathology,Epidermal Growth Factor,Epidermal Growth Factor: metabolism,ErbB-2,ErbB-2: metabolism,Estrogen,Estrogen: metabolism,Female,Humans,Keratin-5,Keratin-6,Keratins,Keratins: metabolism,Menopause,Middle Aged,Progesterone,Progesterone: metabolism,Receptor,Receptors,applied example,erbB-2,erbB-2: metabolism},
mendeley-tags = {applied example},
number = {21},
pages = {2492--502},
pmid = {16757721},
title = {{Race, breast cancer subtypes, and survival in the Carolina Breast Cancer Study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16757721{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3029098{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {295},
year = {2006}
}
@article{Meloy2012,
abstract = {Children who enter the child welfare system at a young age are at risk for a myriad of developmental, physical, and mental health problems. The risks faced by these vulnerable young children may be exacerbated by placement disruptions during foster care. This study utilizes administrative data from Illinois to explore the potential of child care assistance programs to reduce placement disruptions among foster children under the age of five. Survival analysis results suggest that receipt of child care assistance is associated with a reduced risk of placement disruption over time, especially for children who enter foster care as preschoolers. These findings are discussed in the context of the literature on the compensatory role that early care and education can play in short circuiting the detrimental impacts of toxic stress. With regard to public policy, they suggest an important, largely unexamined, role for child care support within the child welfare system. ?? 2012 Elsevier Inc.},
author = {Meloy, Mary Elizabeth and Phillips, Deborah A.},
doi = {10.1016/j.appdev.2012.06.001},
isbn = {0193-3973},
issn = {01933973},
journal = {Journal of Applied Developmental Psychology},
keywords = {Child care,Foster care,Stability,applied example},
mendeley-tags = {applied example},
number = {5},
pages = {252--259},
publisher = {Elsevier Inc.},
title = {{Foster children and placement stability: The role of child care assistance}},
url = {http://dx.doi.org/10.1016/j.appdev.2012.06.001},
volume = {33},
year = {2012}
}
@article{Ardeshna2011,
abstract = {Introduction: This clinical study was designed to evaluate the efficacy of innovative fiber-reinforced-thermoplastic (FRP) bonded orthodontic retainers. Methods: Anterior lingual retainers were formed by using a 2-step process from preimpregnated unidirectional long glass fibers (volume fraction, 0.25) in a thermoplastic resin matrix of either poly(ethylene terephthalate glycol) or polycarbonate. Seventy-six canine-to-canine retainers were placed in 56 patients by using the acid-etch technique over a 34-month period. They were evaluated for clinical acceptability to function as a retainer, structural integrity of the FRP, and integrity of the bonding. Variables examined included material composition, design factors, and mechanism of failure of the retainers. The results were analyzed by using the univariate Kaplan-Meier survival method and the multivariate Cox regression model. Results: The overall median survival time was 7.6 months, with 33{\%} surviving after 12 months. The retainer with the longest service was in use for over 24 months. The variables with the most significant effects and improved survival rates were FRP formulation (polycarbonate), retainer thickness (1.02 mm), and number of teeth overlapped (all 6 teeth). Failure was primarily due to bond failure at the enamel-adhesive or adhesive-FRP interface. Conclusions: With improved survival times, FRP retainers could be a viable alternative to metal retainers. {\textcopyright} 2011 by the American Association of Orthodontists.},
author = {Ardeshna, Anil P.},
doi = {10.1016/j.ajodo.2009.07.028},
isbn = {1097-6752 (Electronic)$\backslash$r0889-5406 (Linking)},
issn = {08895406},
journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
keywords = {applied example},
mendeley-tags = {applied example},
number = {6},
pages = {761--767},
pmid = {21640882},
publisher = {American Association of Orthodontists},
title = {{Clinical evaluation of fiber-reinforced-plastic bonded orthodontic retainers}},
url = {http://dx.doi.org/10.1016/j.ajodo.2009.07.028},
volume = {139},
year = {2011}
}
@article{Mamykina2011,
abstract = {This paper analyzes a Question {\&} Answer site for programmers, Stack Overflow, that dramatically improves on the utility and performance of Q{\&}A systems for technical domains. Over 92{\%} of Stack Overflow questions about expert topics are answered - in a median time of 11 minutes. Using a mixed methods approach that combines statistical data analysis with user interviews, we seek to understand this success. We argue that it is not primarily due to an a priori superior technical design, but also to the high visibility and daily involvement of the design team within the community they serve. This model of continued community leadership presents challenges to both CSCW systems research as well as to attempts to apply the Stack Overflow model to other specialized knowledge domains.},
archivePrefix = {arXiv},
arxivId = {1306.6078},
author = {Mamykina, Lena and Manoim, Bella and Mittal, Manas and Hripcsak, George and Hartmann, Bj{\"{o}}rn},
doi = {10.1145/1978942.1979366},
eprint = {1306.6078},
file = {:Users/lisaoshita/Desktop/literaturereviews/mamykina-stackoverflow-chi2011.pdf:pdf},
isbn = {9781450302289},
issn = {2160-1852},
journal = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
keywords = {Q{\&}A,mixed methods analysis},
pages = {2857},
title = {{Design Lessons from the Fastest Q{\&}A Site in the West}},
url = {http://dl.acm.org/citation.cfm?doid=1978942.1979366},
year = {2011}
}
@article{Asaduzzaman2013,
abstract = {Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overflow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overflow are answered in a very short time. However, we found that the number of unanswered questions has increased significantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overflow services. In this paper, we mine data on unanswered questions from Stack Overflow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difficult to find otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overflow.},
archivePrefix = {arXiv},
arxivId = {1306.6078},
author = {Asaduzzaman, Muhammad and Mashiyat, Ahmed Shah and Roy, Chanchal K. and Schneider, Kevin A.},
doi = {10.1109/MSR.2013.6624015},
eprint = {1306.6078},
file = {:Users/lisaoshita/Library/Application Support/Mendeley Desktop/Downloaded/Asaduzzaman et al. - 2013 - Answering questions about unanswered questions of Stack Overflow.pdf:pdf},
isbn = {978-1-4673-2936-1},
issn = {2160-1852},
journal = {2013 10th Working Conference on Mining Software Repositories (MSR)},
keywords = {Communities,Data mining,Knowledge discovery,Predictive models,Software,Stack Overflow,Stack Overflow services,Taxonomy,Time factors,community-based question answering services,data mining,information seekers,prediction,question answering (information retrieval),question-answer,software developers,software engineering,software engineers,unanswered questions,voluntary services},
pages = {97--100},
title = {{Answering questions about unanswered questions of Stack Overflow}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6624015},
year = {2013}
}
@article{Chen,
abstract = {Background: Cancer survival studies are commonly analyzed using survival-time prediction models for cancer prognosis. A number of different performance metrics are used to ascertain the concordance between the predicted risk score of each patient and the actual survival time, but these metrics can sometimes conflict. Alternatively, patients are sometimes divided into two classes according to a survival-time threshold, and binary classifiers are applied to predict each patient's class. Although this approach has several drawbacks, it does provide natural performance metrics such as positive and negative predictive values to enable unambiguous assessments. Methods: We compare the survival-time prediction and survival-time threshold approaches to analyzing cancer survival studies. We review and compare common performance metrics for the two approaches. We present new randomization tests and cross-validation methods to enable unambiguous statistical inferences for several performance metrics used with the survival-time prediction approach. We consider five survival prediction models consisting of one clinical model, two gene expression models, and two models from combinations of clinical and gene expression models. Results: A public breast cancer dataset was used to compare several performance metrics using five prediction models. 1) For some prediction models, the hazard ratio from fitting a Cox proportional hazards model was significant, but the two-group comparison was insignificant, and vice versa. 2) The randomization test and cross-validation were generally consistent with the p-values obtained from the standard performance metrics. 3) Binary classifiers highly depended on how the risk groups were defined; a slight change of the survival threshold for assignment of classes led to very different prediction results. Conclusions: 1) Different performance metrics for evaluation of a survival prediction model may give different conclusions in its discriminatory ability. 2) Evaluation using a high-risk versus low-risk group comparison depends on the selected risk-score threshold; a plot of p-values from all possible thresholds can show the sensitivity of the threshold selection. 3) A randomization test of the significance of Somers' rank correlation can be used for further evaluation of performance of a prediction model. 4) The cross-validated power of survival prediction models decreases as the training and test sets become less balanced. Background},
author = {Chen, Hung-Chia and Kodell, Ralph L and Cheng, Kuang Fu and Chen, James J},
doi = {10.1186/1471-2288-12-102},
file = {:Users/lisaoshita/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Assessment of performance of survival prediction models for cancer prognosis(3).pdf:pdf},
journal = {BMC Medical Research Methodology},
number = {1},
pages = {102},
title = {{Assessment of performance of survival prediction models for cancer prognosis}},
url = {https://doi.org/10.1186/1471-2288-12-102},
volume = {12},
year = {2012}
}
@article{Yao2015,
author = {Yao, Yuan and Tong, Hanghang and Xie, Tao and Akoglu, Leman and Xu, Feng and Lu, Jian},
doi = {10.1016/j.ins.2014.12.038},
issn = {0020-0255},
journal = {INFORMATION SCIENCES},
pages = {70--82},
publisher = {Elsevier Inc.},
title = {{Detecting high-quality posts in community question answering sites}},
url = {http://dx.doi.org/10.1016/j.ins.2014.12.038},
volume = {302},
year = {2015}
}
@article{Chua2013,
abstract = {The authors investigate the interplay between answer quality and answer speed across question types in community question-answering sites (CQAs). The research questions addressed are the following: (a) How do answer quality and answer speed vary across question types? (b) How do the relationships between answer quality and answer speed vary across question types? (c) How do the best quality answers and the fastest answers differ in terms of answer quality and answer speed across question types? (d) How do trends in answer quality vary over time across ques- tion types? From the posting of 3,000 questions in six CQAs, 5,356 answers were harvested and analyzed. There was a significant difference in answer quality and answer speed across question types, and there were generally no significant relationships between answer quality and answer speed. The best quality answers had better overall answer quality than the fastest answers but generally took longer to arrive. In addition, although the trend in answer quality had been mostly random across all question types, the quality of answers appeared to improve gradually when given time. By highlighting the subtle nuances in answer quality and answer speed across question types, this study is an attempt to explore a territory of CQA research that has hitherto been relatively uncharted. Introduction},
archivePrefix = {arXiv},
arxivId = {0803.1716},
author = {Chua, Alton Y. K. and Banerjee, Snehasish},
doi = {10.1002/asi},
eprint = {0803.1716},
isbn = {9783848215430},
issn = {14923831},
journal = {Journal of the American Society for Information Science and Technology},
number = {10},
pages = {2058--2068},
pmid = {502955140},
title = {{So Fast So Good: An Analysis of Answer Quality and Answer Speed in Community Question-Answering Sites}},
volume = {64},
year = {2013}
}
@article{Zhou2012,
abstract = {Automatic Subjective Question Answering (ASQA), which aims at answering users' subjective questions using summaries of multiple opinions, becomes increasingly important. One challenge of ASQA is that expected answers for subjective questions may not readily exist in theWeb. The rising and popularity of Community Question Answering (CQA) sites, which provide platforms for people to post and answer questions, provides an alternative to ASQA. One important task of ASQA is question subjectivity identification, which identifies whether a user is asking a subjective question. Unfortunately, there has been little labeled training data available for this task. In this paper, we propose an approach to collect training data automatically by utilizing social signals in CQA sites without involving any manual labeling. Experimental results show that our data-driven approach achieves 9:37{\%} relative improvement over the supervised approach using manually labeled data, and achieves 5:15{\%} relative gain over a stateof- the-art semi-supervised approach. In addition, we propose several heuristic features for question subjectivity identification. By adding these features, we achieve 11:23{\%} relative improvement over word n-gram feature under the same experimental setting.},
author = {Zhou, Tc and Si, Xiance and Chang, Ey and King, Irwin and Lyu, Mr},
isbn = {9781577355687},
journal = {Aaai},
keywords = {Special Track on Artificial Intelligence and the W},
pages = {164--170},
title = {{A Data-Driven Approach to Question Subjectivity Identification in Community Question Answering.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/4976/5134},
year = {2012}
}
@article{Toba2014,
author = {Toba, Hapnes and Ming, Zhao-yan and Adriani, Mirna and Chua, Tat-seng},
doi = {10.1016/j.ins.2013.10.030},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {question answering system,user generated content},
pages = {101--115},
publisher = {Elsevier Inc.},
title = {{Discovering high quality answers in community question answering archives using a hierarchy of classifiers}},
url = {http://dx.doi.org/10.1016/j.ins.2013.10.030},
volume = {261},
year = {2014}
}
@article{Zhang2011,
author = {Zhang, Zhongfeng and Li, Qiudan},
doi = {10.1016/j.eswa.2010.12.052},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {community question answering,cqa},
number = {6},
pages = {6848--6855},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications QuestionHolic : Hot topic discovery and trend analysis in community question answering systems}},
url = {http://dx.doi.org/10.1016/j.eswa.2010.12.052},
volume = {38},
year = {2011}
}
@article{Bedrick*2013,
abstract = {Abstract The hazard ratio is a standard summary for comparing survival curves yet hazard ratios are often difficult for scientists and clinicians to interpret. Insight into the interpretation of hazard ratios is obtained by relating hazard ratios to the maximum difference and an average difference between survival probabilities. These reformulations of the hazard ratio are useful in classroom discussions of survival analysis and when discussing analyses with scientists and clinicians. Large sample distribution theory is provided for these reformulations of the hazard ratio. Two examples are used to illustrate the ideas. Abstract The hazard ratio is a standard summary for comparing survival curves yet hazard ratios are often difficult for scientists and clinicians to interpret. Insight into the interpretation of hazard ratios is obtained by relating hazard ratios to the maximum difference and an average difference between survival probabilities. These reformulations of the hazard ratio are useful in classroom discussions of survival analysis and when discussing analyses with scientists and clinicians. Large sample distribution theory is provided for these reformulations of the hazard ratio. Two examples are used to illustrate the ideas.},
author = {Bedrick*, Edward J.},
doi = {10.1080/00031305.2013.868827},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {cox model,proportional hazards model,sur-},
number = {March},
pages = {131207055018006},
title = {{Two Useful Reformulations of the Hazard Ratio}},
url = {http://dx.doi.org/10.1080/00031305.2013.868827},
volume = {1305},
year = {2013}
}
@article{Bhat2014,
author = {Bhat, Vasudev and Gokhale, Adheesh and Jadhav, Ravi and Pudipeddi, Ravi and Akoglu, Leman},
isbn = {9781479958771},
journal = {IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
keywords = {-online communities,behavior,collec-,evidential feature analysis,human,on,probability distribution of the,question answering sites,question response time,response time of questions,tive intelligence,user engagement},
number = {Asonam},
pages = {328--335},
title = {{Min ( e ) d Your Tags : Analysis of Question Response Time in StackOverflow}},
year = {2014}
}
@article{Ponzanelli2014a,
abstract = {Technical questions and answers (Q{\&}A) services have become a valuable resource for developers. A prominent example of technical Q{\&}A website is StackOverflow (SO), which relies on a growing community of more than two millions of users who actively contribute by asking questions and providing answers. To maintain the value of this resource, poor quality questions - among the more than 6,000 asked daily - have to be filtered out. Currently, poor quality questions are manually identified and reviewed by selected users in SO, this costs considerable time and effort. Automating the process would save time and unload the review queue, improving the efficiency of SO as a resource for developers. We present an approach to automate the classification of questions according to their quality. We present an empirical study that investigates how to model and predict the quality of a question by considering as features both the contents of a post (e.g., from simple textual features to more complex readability metrics) and community-related aspects (e.g., popularity of a user in the community). Our findings show that there is indeed the possibility of at least a partial automation of the costly SO review process.},
author = {Ponzanelli, Luca and Mocci, Andrea and Bacchelli, Alberto and Lanza, Michele},
doi = {10.1109/QSIC.2014.27},
isbn = {9781479971978},
issn = {15506002},
journal = {Proceedings - International Conference on Quality Software},
keywords = {Q{\&}A,Quality,StackOverflow,Technical Forum},
pages = {343--352},
title = {{Understanding and classifying the quality of technical forum questions}},
year = {2014}
}
@article{Duijn2015,
abstract = {—Stack Overflow (SO) is a question and answers (Q{\&}A) web platform on software development that is gaining in popularity. With increasing popularity often comes a very unwelcome side effect: A decrease in the average quality of a post. To keep Q{\&}A websites like SO useful it is vital that this side effect is countered. Previous research proved to be reasonably successful in using properties of questions to help identify low quality questions to be later reviewed and improved. We present an approach to improve the classification of high and low quality questions based on a novel source of information: the analysis of the code fragments in SO questions. We show that we get similar performance to classification based on a wider set of metrics thus potentially reaching a better overall classification.},
author = {Duijn, Maarten and Ku??era, Adam and Bacchelli, Alberto},
doi = {10.1109/MSR.2015.51},
isbn = {9780769555942},
issn = {21601860},
journal = {IEEE International Working Conference on Mining Software Repositories},
keywords = {Accuracy,Algorithm design and analysis,Classification algorithms,Correlation,Decision trees,Java,Measurement},
pages = {410--413},
title = {{Quality questions need quality code: Classifying code fragments on stack overflow}},
volume = {2015-Augus},
year = {2015}
}
@article{Anderson2012,
author = {Anderson, Ashton and Huttenlocher, Daniel and Kleinberg, Jon and Leskovec, Jure},
doi = {10.1145/2339530.2339665},
isbn = {9781450314626},
journal = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
keywords = {question-answering,reputation,value prediction},
pages = {850},
title = {{Discovering value from community activity on focused question answering sites}},
url = {http://dl.acm.org/citation.cfm?doid=2339530.2339665},
year = {2012}
}
@article{Weimer2007,
abstract = {Assessing the quality of user generated con- tent is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to as- sess the quality of forum posts automati- cally and test it on data provided by Nab- ble.com. We use state-of-the-art classifi- cation techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features. We achieve an accuracy of 89{\%} on the task of automatically assessing post quality in the software domain using forum specific fea- tures. Without forum specific features, we achieve an accuracy of 82{\%}.},
author = {Weimer, Markus and Gurevych, Iryna and M{\"{u}}hlh{\"{a}}user, Max},
doi = {10.3115/1557769.1557806},
journal = {Proceedings of the ACL},
number = {June},
pages = {125--128},
title = {{Automatically assessing the post quality in online discussions on software}},
url = {http://atlas.tk.informatik.tu-darmstadt.de/Publications/2007/acl2007.pdf},
year = {2007}
}
@article{Rechavi2011,
abstract = {This study investigates two questions concerning question-and-answer sites. We analyzed data from "Yahoo!Answers", including 19 months and over 20 million interactions per month. The first question investigates the differences in response time and in the average number of answers between anasker's ranking of "Best Answer" (BA) and the community's BA. The second question concerns the impact of an explicit network on several implicit network activities. The results imply that askers use response time as a parameter to choose the BA, whereas the community chooses the BA with no regard to Answer Response Time (ART). Another finding implies that if the answerer is not listed in the asker's "explicit network, " it might result in longer ranking (award) time and in a slightly decreased number of answer stars (satisfaction-rate indicator).And yet, one result might be surprising. Being a "fan" of the asker implies a long response time to the question. This finding might contradict the intuition that our friends are the first to provide answers to our questions. Several explanations of this result from different research fields are suggested in the discussion.},
author = {Rechavi, Amit and Rafaeli, Sheizaf},
doi = {10.1109/PASSAT/SocialCom.2011.67},
isbn = {9780769545783},
journal = {Proceedings - 2011 IEEE International Conference on Privacy, Security, Risk and Trust and IEEE International Conference on Social Computing, PASSAT/SocialCom 2011},
keywords = {Response time,Satisfaction rates,Wisdom of the crowds,Yahoo! answers},
pages = {904--909},
title = {{Not all is gold that glitters response time {\&} satisfaction rates in Yahoo! answers}},
year = {2011}
}
@article{Harper2008,
abstract = {Question and answer (Q{\&}A) sites such as Yahoo! Answers are places where users ask questions and others answer them. In this paper, we investigate predictors of answer quality through a comparative, controlled field study of responses provided across several online Q{\&}A sites. Along with several quantitative results concerning the effects of factors such as question topic and rhetorical strategy, we present two high-level messages. First, you get what you pay for in Q{\&}A sites. Answer quality was typically higher in Google Answers (a fee-based site) than in the free sites we studied, and paying more money for an answer led to better outcomes. Second, we find that a Q{\&}A site's community of users contributes to its success. Yahoo! Answers, a Q{\&}A site where anybody can answer questions, outperformed sites that depend on specific individuals to answer questions, such as library reference services.},
author = {Harper, F Maxwell and Raban, Daphne and Rafaeli, Sheizaf and Konstan, Joseph A},
doi = {10.1145/1357054.1357191},
isbn = {9781605580111},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
keywords = {Q{\&}A,digital reference,expert services,information exchanges,information quality,knowledge networks,online community},
pages = {865--874},
title = {{Predictors of Answer Quality in Online Q{\&}A Sites}},
url = {http://dl.acm.org/citation.cfm?id=1357054.1357191},
year = {2008}
}
@article{Ravi2014,
abstract = {Asking the right question in the right way is an art (and a science). In a community question-answering setting, a good question is not just one that is found to be useful by other people—a question is good if it is also pre-sented clearly and shows prior research. Using a com-munity question-answering site that allows voting over the questions, we show that there is a notion of question quality that goes beyond mere popularity. We present techniques using latent topical models to automatically predict the quality of questions based on their content. Our best system achieves a prediction accuracy of 72{\%}, beating out strong baselines by a significant amount. We also examine the effect of question quality on the dy-namics of user behavior and the longevity of questions.},
author = {Ravi, Sujith and Pang, Bo and Rastagori, VIbhor and Kumar, Ravi},
isbn = {978-1-57735-657-8},
journal = {International AAAI Conference on Weblogs and Social Media},
keywords = {Full Papers},
number = {1},
pages = {426--435},
title = {{Great Question ! Question Quality in Community Q {\&} A}},
year = {2014}
}
