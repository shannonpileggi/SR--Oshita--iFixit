@article{Oakes1981,
abstract = {Some recent developments in the analysis of censored survival times with explanatory variables are considered. The discussion centres on the log linear model introduced by Cox for the hazard function, and the corresponding partial likelihood. Special topics covered include efficiency comparisons of semiparametric and nonparametric estimation, stochastic covariates, approximations to the partial likelihood from grouped data, and the connection with log linear models for contingency tables. A number of areas for further research are indicated. /// On examine des d{\'{e}}veloppements r{\'{e}}cents de l'analyse statistique d'observations de dur{\'{e}}e de vie censur{\'{e}}es et aux covariables. On discute le mod{\`{e}}le log-lin{\'{e}}aire de Cox, et la vraisemblance partielle correspondante. Les sujets sp{\'{e}}cifiques qu'on pr{\'{e}}sente comprennent des comparaisons d'efficacit{\'{e}} relatives aux estimateurs semi-param{\'{e}}triques et nonparam{\'{e}}triques, des covariables al{\'{e}}atoires, des approximations de donn{\'{e}}es group{\'{e}}es, et les relations aux mod{\`{e}}les log-lin{\'{e}}aires des tables de fr{\'{e}}quences. On indique quelques mati{\`{e}}res de recherches ult{\'{e}}rieures.},
author = {Oakes, David},
doi = {10.2307/1402606},
file = {:Users/lisaoshita/Desktop/log{\_}partial{\_}likelihood.pdf:pdf},
isbn = {03067734},
issn = {0306-7734},
journal = {International Statistical Review / Revue Internationale de Statistique},
keywords = {analysis,asymptotic theory,conditional likelihood,hazard functions,life tables,partial likelihood,point,processes,proportional hazards,survival times,time-dependent covariates in survival},
number = {3},
pages = {235--252},
title = {{Survival Times: Aspects of Partial Likelihood}},
url = {http://www.jstor.org/stable/1402606},
volume = {49},
year = {1981}
}
@article{Correa2013,
abstract = {Stack Overflow is widely regarded as the most popular Community driven Question Answering (CQA) website for programmers. Questions posted on Stack Overflow which are not related to programming topics, are marked as ‘closed' by experienced users and community moderators. A question can be ‘closed' for five reasons – duplicate, off-topic, subjective, not a real question and too localized. In this work, we present the first study of ‘closed' questions on Stack Overflow. We download 4 years of publicly available data which contains 3.4 Million questions. We first analyze and characterize the complete set of 0.1 Million ‘closed' questions. Next, we use a machine learning framework and build a predictive model to identify a ‘closed' question at the time of question creation. One of our key findings is that despite being marked as ‘closed', subjective questions contain high information value and are very popular with the users. We observe an increasing trend in the percentage of closed questions over time and find that this increase is positively correlated to the number of newly registered users. In addition, we also see a decrease in community participation to mark a ‘closed' question which has led to an increase in moderation job time. We also find that questions closed with the Duplicate and Off Topic labels are relatively more prone to reputation gaming. Our analysis suggests broader implications for content quality maintenance on CQA websites. For the ‘closed' question prediction task, we make use of multiple genres of feature sets based on - user profile , community process, textual style and question content. We use a state-of-art machine learning classifier based on an ensemble framework and achieve an overall accuracy of 70.3{\%}. Analysis of the feature space reveals that ‘closed' questions are relatively less informative and descriptive than non-‘closed' questions. To the best of our knowledge, this is the first experimental study to analyze and predict ‘closed' questions on Stack Overflow.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.7291v1},
author = {Correa, Denzil and Sureka, Ashish},
doi = {10.1145/2512938.2512954},
eprint = {arXiv:1307.7291v1},
file = {:Users/lisaoshita/Desktop/manuscript/literaturereviews/p201-correa.pdf:pdf},
isbn = {9781450320849},
issn = {1450320848},
journal = {Proceedings of the first ACM Conference on Online Social Networks},
pages = {201--212},
title = {{Fit or unfit: Analysis and prediction of 'closed questions' on Stack Overflow}},
url = {http://dl.acm.org/citation.cfm?doid=2512938.2512954},
year = {2013}
}
@inproceedings{Mahmud2013,
abstract = {Proceedings of the Seventh International AAAI Conference on Weblogs and Social Media},
author = {Mahmud, Jalal and Chen, Jilin and Nichols, Jeffrey},
booktitle = {Seventh International AAAI Conference on Weblogs and Social Media},
file = {:Users/lisaoshita/Desktop/twitter{\_}responsetime.pdf:pdf},
keywords = {Short Paper},
pages = {697--700},
title = {{When Will You Answer This? Estimating Response Time in Twitter}},
url = {http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/viewPDFInterstitial/6072/6328},
year = {2013}
}
@inproceedings{Blumenstock2008,
abstract = {Wikipedia, the free encyclopedia, now contains over two million English articles, and is widely regarded as a high-quality, authoritative encyclopedia. Some Wikipedia articles, however, are of questionable quality, and it is not always apparent to the visitor which articles are good and which are bad. We propose a simple metric word count for measuring article quality. In spite of its striking simplicity, we show that this metric significantly outperforms the more complex methods described in related work.},
author = {Blumenstock, Joshua E},
booktitle = {Proceedings of the 17th International Conference on World Wide Web},
doi = {10.1145/1367497.1367673},
file = {:Users/lisaoshita/Desktop/p1095-blumenstock.pdf:pdf},
isbn = {9781605580852},
issn = {08963207},
keywords = {information quality,wikipedia,word count},
pages = {1095--1096},
publisher = {ACM},
title = {{Size matters: word count as a measure of quality on wikipedia}},
url = {http://portal.acm.org/citation.cfm?id=1367673},
year = {2008}
}
@article{Fu2015,
abstract = {As an increasing important source of information and knowledge, social questioning and answering sites (social Q{\&}A) have attracted significant attention from industry and academia, as they address the challenge of evaluating and predicting the quality of answers on such sites. However, few previous studies examined the answer quality by considering knowledge domains or topics as a potential factor. To fill this gap, a model consisting of 24 textual and non-textual features of answers was developed in this study to evaluate and predict answer quality for social Q{\&}A, and the model was applied to identify and compare useful features for predicting high-quality answers across four knowledge domains, including science, technology, art, and recreation. The findings indicate that review and user features are the most powerful indicators of high-quality answers regardless of knowledge domains, while the usefulness of textual features (length, structure, and writing style) varies across different knowledge domains. In the future, the findings could be applied to automatically assessing answer quality and quality control in social Q{\&}AMP;A.},
author = {Fu, Hengyi and Wu, Shuheng and Oh, Sanghee},
doi = {10.1002/pra2.2015.145052010088},
file = {:Users/lisaoshita/Desktop/manuscript/literaturereviews/a88-fu.pdf:pdf},
issn = {23739231},
journal = {Proceedings of the Association for Information Science and Technology},
keywords = {Social question and answer sites,answer quality,quality assessment},
number = {1},
pages = {1--5},
title = {{Evaluating answer quality across knowledge domains: Using textual and non-textual features in social Q{\&}A}},
volume = {52},
year = {2015}
}
@article{Grambsch1994,
abstract = {Nonproportional hazards can often be expressed by extending the Cox model to include time varying coefficients; e.g., for a single covariate, the hazard function for subject i is modelled as exp {\{}$\beta$(t)Zi(t){\}}. A common example is a treatment effect that decreases with time. We show that the function $\beta$i(t) can be directly visualized by smoothing an appropriate residual plot. Also, many tests of proportional hazards, including those of Cox (1972), Gill {\&} Schumacher (1987), Harrell (1986), Lin (1991), Moreau, O'Quigley {\&} Mesbah (1985), Nagelkerke, Oosting {\&} Hart (1984), O'Quigley {\&} Pessione (1989), Schoenfeld (1980) and Wei (1984) are related to time-weighted score tests of the proportional hazards hypothesis, and can be visualized as a weighted least-squares line fitted to the residual plot.},
author = {Grambsch, Patricia and Therneau, Terry},
file = {:Users/lisaoshita/Desktop/cox{\_}zph.pdf:pdf},
journal = {Biometrika},
number = {3},
pages = {515--526},
title = {{Proportional hazards tests and diagnostics based on weighted residuals}},
url = {https://doi.org/10.1093/biomet/81.3.515},
volume = {81},
year = {1994}
}
@article{Hammermeister1979,
abstract = {A progression of univariate followed by multivariate analyses was applied to 46 variables selected from the clinical examination, exercise test, coronary arteriography, and quantitative angiographic assessment of left ventricular function in patients with coronary disease to determine those variables most predictive of survival. For the 733 medically treated patients, the final Cox's regression analysis showed that the left ventricular ejection fraction was most predictive of survival, followed by age, number of vessels with stenosis(es) greater than or equal to 70{\%}, and ventricular arrhythmia on the resting electrocardiogram. For the 1870 surgically treated patients, ventricular arrhythmia on the resting electrocardiogram was most predictive of survival followed by ejection fraction, heart murmur, left main coronary artery stenosis greater than or equal to 50{\%}, and use of diuretic agents.},
author = {Hammermeister, K E and DeRouen, T A and Dodge, H T},
doi = {10.1161/01.CIR.59.3.421},
file = {:Users/lisaoshita/Desktop/univ{\_}analysis.pdf:pdf},
journal = {American Heart Association, Inc.},
number = {3},
pages = {421--430},
title = {{Variables predictive of survival in patients with coronary disease. Selection by univariate and multivariate analyses from the clinical, electrocardiographic, exercise, arteriographic, and quantitative angiographic evaluations.}},
url = {http://dx.doi.org/10.1161/01.CIR.59.3.421},
volume = {59},
year = {1979}
}
@article{Rodriguez2010,
abstract = {In the machine learning field, the performance of a classifier is usually measured in terms of prediction error. In most real-world problems, the error cannot be exactly calculated and it must be estimated. Therefore, it is important to choose an appropriate estimator of the error. This paper analyzes the statistical properties, bias and variance, of the kappa-fold cross-validation classification error estimator (kappa-cv). Our main contribution is a novel theoretical decomposition of the variance of the kappa-cv considering its sources of variance: sensitivity to changes in the training set and sensitivity to changes in the folds. The paper also compares the bias and variance of the estimator for different values of kappa. The experimental study has been performed in artificial domains because they allow the exact computation of the implied quantities and we can rigorously specify the conditions of experimentation. The experimentation has been performed for two classifiers (naive Bayes and nearest neighbor), different numbers of folds, sample sizes, and training sets coming from assorted probability distributions. We conclude by including some practical recommendation on the use of kappa-fold cross validation.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Rodr{\'{i}}guez, Juan Diego and P{\'{e}}rez, Aritz and Lozano, Jose Antonio},
doi = {10.1109/TPAMI.2009.187},
eprint = {9605103},
file = {:Users/lisaoshita/Desktop/kfold{\_}cv{\_}sensitivity.pdf:pdf},
isbn = {0-7803-3213-X},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {analysis and machine intelligence,e transactions on pattern},
number = {3},
pages = {569--575},
pmid = {20075479},
primaryClass = {cs},
title = {{Sensitivity analysis of kappa-fold cross validation in prediction error estimation}},
volume = {32},
year = {2010}
}
@incollection{Moore2010,
abstract = {Applied Survival Analysis Using R covers the main principles of survival analysis, gives examples of how it is applied, and teaches how to put those principles to use to analyze data using R as a vehicle. Survival data, where the primary outcome is time to a specific event, arise in many areas of biomedical research, including clinical trials, epidemiological studies, and studies of animals. Many survival methods are extensions of techniques used in linear regression and categorical data, while other aspects of this field are unique to survival data. This text employs numerous actual examples to illustrate survival curve estimation, comparison of survivals of different groups, proper accounting for censoring and truncation, model variable selection, and residual analysis. Because explaining survival analysis requires more advanced mathematics than many other statistical topics, this book is organized with basic concepts and most frequently used procedures covered in earlier chapters, with more advanced topics near the end and in the appendices. A background in basic linear regression and categorical data analysis, as well as a basic knowledge of calculus and the R system, will help the reader to fully appreciate the information presented. Examples are simple and straightforward while still illustrating key points, shedding light on the application of survival analysis in a way that is useful for graduate students, researchers, and practitioners in biostatistics.},
author = {Moore, Dirk},
chapter = {5},
doi = {10.1007/978-3-319-31245-3},
edition = {1},
editor = {Gentleman, Robert and Hornik, Kurt and Parmigiani, Giovanni},
file = {:Users/lisaoshita/Desktop/manuscript/CR Predictive Modeling /Applied Survival Analysis Using R.pdf:pdf},
isbn = {9783319312439},
number = {March},
pages = {226},
publisher = {Springer International Publishing},
title = {{Applied Survival Analysis Using R}},
year = {2010}
}
@incollection{Kleinbaum2011,
abstract = {This greatly expanded second edition of Survival Analysis- A Self-learning$\backslash$nText provides a highly readable description of state-of-the-art methods of$\backslash$nanalysis of survival/event-history data. This text is suitable for researchers$\backslash$nand statisticians working in the medical and other life sciences as well as$\backslash$nstatisticians in academia who teach introductory and second-level courses on$\backslash$nsurvival analysis. The second edition continues to use the unique "lecture-$\backslash$nbook" format of the first (1996) edition with the addition of three new$\backslash$nchapters on advanced topics:$\backslash$n$\backslash$nChapter 7: Parametric Models$\backslash$n$\backslash$nChapter 8: Recurrent events$\backslash$n$\backslash$nChapter 9: Competing Risks.$\backslash$n$\backslash$nAlso, the Computer Appendix has been revised to provide step-by-step$\backslash$ninstructions for using the computer packages {\{}STATA{\}} (Version 7.0), {\{}SAS{\}} (Version$\backslash$n8.2), and {\{}SPSS{\}} (version 11.5) to carry out the procedures presented in the$\backslash$nmain text.$\backslash$n$\backslash$nThe original six chapters have been modified slightly$\backslash$n$\backslash$n{\_}$\backslash$n$\backslash$nto expand and clarify aspects of survival analysis in response to suggestions$\backslash$nby students, colleagues and reviewers, and$\backslash$n$\backslash$nto add theoretical background, particularly regarding the formulation of the$\backslash$n(partial) likelihood functions for proportional hazards, stratified, and$\backslash$nextended Cox regression models$\backslash$n$\backslash$n{\_}$\backslash$n$\backslash$nDavid Kleinbaum is Professor of Epidemiology at the Rollins School of Public$\backslash$nHealth at Emory University, Atlanta, Georgia. Dr. Kleinbaum is internationally$\backslash$nknown for innovative textbooks and teaching on epidemiological methods,$\backslash$nmultiple linear regression, logistic regression, and survival analysis. He has$\backslash$nprovided extensive worldwide short-course training in over 150 short courses$\backslash$non statistical and epidemiological methods. He is also the author of {\{}ActivEpi{\}}$\backslash$n(2002), an interactive computer-based instructional text on fundamentals of$\backslash$nepidemiology, which has been used in a variety of educational environments$\backslash$nincluding distance learning.$\backslash$n$\backslash$nMitchel Klein is Research Assistant Professor with a joint appointment in the$\backslash$nDepartment of Environmental and Occupational Health ({\{}EOH{\}}) and the Department$\backslash$nof Epidemiology, also at the Rollins School of Public Health at Emory$\backslash$nUniversity. Dr. Klein is also co-author with Dr. Kleinbaum of the second$\backslash$nedition of Logistic Regression- A {\{}Self-Learning{\}} Text (2002). He has regularly$\backslash$ntaught epidemiologic methods courses at Emory to graduate students in public$\backslash$nhealth and in clinical medicine. He is responsible for the epidemiologic$\backslash$nmethods training of physicians enrolled in Emory's Master of Science in$\backslash$nClinical Research Program, and has collaborated with Dr. Kleinbaum both$\backslash$nnationally and internationally in teaching several short courses on various$\backslash$ntopics in epidemiologic methods.},
archivePrefix = {arXiv},
arxivId = {1002.2080},
author = {Kleinbaum, David and Klein, Mitchell},
booktitle = {Biometrical Journal},
chapter = {1},
doi = {10.1016/B978-0-12-387667-6.00013-0},
edition = {Third},
editor = {Gail, M and Krickeberg, K and Samet, J.M. and Tsiatis, A and Wong, W},
eprint = {1002.2080},
file = {:Users/lisaoshita/Desktop/survival{\_}analysis{\_}text.pdf:pdf},
isbn = {1441966455},
issn = {18715125},
keywords = {statistics for biology and health},
pages = {715},
pmid = {22469268},
publisher = {Springer-Verlag New York},
title = {{Survival Analysis: A Self-Learning Text, Third Edition (Statistics for Biology and Health)}},
year = {2011}
}
@article{Bland1998,
abstract = {As we have observed, 1 analysis of survival data requires special techniques because some observations are censored as the event of interest has not occurred for all patients. For example, when patients are recruited over two years one recruited at the end of the study may be alive at one year follow up, whereas one recruited at the start may have died after two years. The patient who died has a longer observed survival than the one who still survives and whose ultimate survival time is unknown. The table shows data from a study of conception in subfertile women. 2 The event is conception, and women " survived " until they conceived. One woman conceived after 16 months (menstrual cycles), whereas several were followed for shorter time periods during which they did not conceive; their time to conceive was thus censored. We wish to estimate the proportion surviving (not having conceived) by any given time, which is also the estimated probability of survival to that time for a member of the population from which the sample is drawn. Because of the censoring we use the Kaplan-Meier method. For each time interval we estimate the probability that those who have survived to the beginning will survive to the end. This is a condi-tional probability (the probability of being a survivor at the end of the interval on condition that the subject was a survivor at the beginning of the interval). Survival to any time point is calculated as the product of the conditional probabilities of surviving each time interval. These data are unusual in representing months (menstrual cycles); usually the conditional probabilities relate to days. The calculations are simpli-fied by ignoring times at which there were no recorded survival times (whether events or censored times). In the example, the probability of surviving for two months is the probability of surviving the first month times the probability of surviving the second month given that the first month was survived. Of 38 women, 32 survived the first month, or 0.842. Of the 32 women at the start of the second month (" at risk " of conception), 27 had not conceived by the end of the month. The conditional probability of surviving the second month is thus 27/32 = 0.844, and the overall probability of surviving (not conceiving) after two months is 0.842 × 0.844 = 0.711. We continue in this way to the end of the table, or until we reach the last event. Observations censored at a given time affect the number still at risk at the start of the next month. The estimated probability changes only in months when there is a conception. In practice, a computer is used to do these calculations. Standard errors and confidence intervals for the estimated survival probabilities can be found by Greenwood's method. 3 Survival probabilities are usually presented as a survival curve (figure). The " curve " is a step function, with sudden changes in the estimated probability corresponding to times at which an event was observed. The times of the censored data are indicated by short vertical lines. There are three assumptions in the above. Firstly, we assume that at any time patients who are censored have the same survival prospects as those who continue to be followed. This assumption is not easily testable. Censoring may be for various reasons. In the conception study some women had received hormone treatment to promote ovulation, and others had stopped trying to conceive. Thus they were no longer part of the population we wanted to study, and their survival times were censored. In most studies some subjects drop out for reasons unrelated to the condition under study (for example, emigration) If, however, for some patients in this study censoring was related to failure to conceive this would have biased the estimated survival probabilities downwards. Secondly, we assume that the survival probabilities are the same for subjects recruited early and late in the study. In a long term observational study of patients with cancer, for example, the case mix may change over the period of recruitment, or there may be an innova-tion in ancillary treatment. This assumption may be tested, provided we have enough data to estimate survival curves for different subsets of the data. Thirdly, we assume that the event happens at the time specified. This is not a problem for the conception data, but could be, for example, if the event were recur-rence of a tumour which would be detected at a regu-lar examination. All we would know is that the event happened between two examinations. This imprecision would bias the survival probabilities upwards. When the observations are at regular intervals this can be allowed for quite easily, using the actuarial method. 3},
author = {Bland, J M. and Altman, D. G},
doi = {10.1136/bmj.317.7172.1572},
file = {:Users/lisaoshita/Desktop/kaplan{\_}meier{\_}est.pdf:pdf},
isbn = {0959-8138 (Print)$\backslash$r0959-535X (Linking)},
issn = {0959-8138},
journal = {Bmj},
number = {7172},
pages = {1572--1580},
pmid = {9836663},
title = {{Statistics Notes: Survival probabilities (the Kaplan-Meier method)}},
url = {http://www.bmj.com/cgi/doi/10.1136/bmj.317.7172.1572},
volume = {317},
year = {1998}
}
@book{Harrell2015,
abstract = {Multivariable regression models are widely used in health science research, mainly for two purposes: prediction and effect estimation. Various strategies have been recommended when building a regression model: a) use the right statistical method that matches the structure of the data; b) ensure an appropriate sample size by limiting the number of variables according to the number of events; c) prevent or correct for model overfitting; d) be aware of the problems associated with automatic variable selection procedures (such as stepwise), and e) always assess the performance of the final model in regard to calibration and discrimination measures. If resources allow, validate the prediction model on external data.},
author = {Harrell, Frank E.},
booktitle = {Springer Series in Statistics},
doi = {10.1007/978-1-4757-3462-1},
file = {:Users/lisaoshita/Desktop/CR Predictive Modeling /RegressionModelingStrategies.pdf:pdf},
isbn = {978-1-4419-2918-1},
issn = {0172-7397},
keywords = {Department of Biostatistics,Nashville,School of Medicine,Tennessee,USA,Vanderbilt University},
number = {6},
pages = {501--507},
pmid = {21531065},
title = {{Regression Modeling Strategies}},
url = {papers://55069ee6-504c-4f60-bfa9-053c4dcabb39/Paper/p398{\%}5Cnhttp://link.springer.com/10.1007/978-3-319-19425-7{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/21531065{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/S0300893211003502{\%}5Cnhttp://link.springer.com/10.1},
volume = {64},
year = {2015}
}

@article{Asaduzzaman2013,
abstract = {Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overflow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overflow are answered in a very short time. However, we found that the number of unanswered questions has increased significantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overflow services. In this paper, we mine data on unanswered questions from Stack Overflow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difficult to find otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overflow.},
archivePrefix = {arXiv},
arxivId = {1306.6078},
author = {Asaduzzaman, Muhammad and Mashiyat, Ahmed Shah and Roy, Chanchal K. and Schneider, Kevin A.},
doi = {10.1109/MSR.2013.6624015},
eprint = {1306.6078},
file = {:Users/lisaoshita/Library/Application Support/Mendeley Desktop/Downloaded/Asaduzzaman et al. - 2013 - Answering questions about unanswered questions of Stack Overflow.pdf:pdf},
isbn = {978-1-4673-2936-1},
issn = {2160-1852},
journal = {2013 10th Working Conference on Mining Software Repositories (MSR)},
keywords = {Communities,Data mining,Knowledge discovery,Predictive models,Software,Stack Overflow,Stack Overflow services,Taxonomy,Time factors,community-based question answering services,data mining,information seekers,prediction,question answering (information retrieval),question-answer,software developers,software engineering,software engineers,unanswered questions,voluntary services},
pages = {97--100},
title = {{Answering questions about unanswered questions of Stack Overflow}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6624015},
year = {2013}
}
@article{Chen,
abstract = {Background: Cancer survival studies are commonly analyzed using survival-time prediction models for cancer prognosis. A number of different performance metrics are used to ascertain the concordance between the predicted risk score of each patient and the actual survival time, but these metrics can sometimes conflict. Alternatively, patients are sometimes divided into two classes according to a survival-time threshold, and binary classifiers are applied to predict each patient's class. Although this approach has several drawbacks, it does provide natural performance metrics such as positive and negative predictive values to enable unambiguous assessments. Methods: We compare the survival-time prediction and survival-time threshold approaches to analyzing cancer survival studies. We review and compare common performance metrics for the two approaches. We present new randomization tests and cross-validation methods to enable unambiguous statistical inferences for several performance metrics used with the survival-time prediction approach. We consider five survival prediction models consisting of one clinical model, two gene expression models, and two models from combinations of clinical and gene expression models. Results: A public breast cancer dataset was used to compare several performance metrics using five prediction models. 1) For some prediction models, the hazard ratio from fitting a Cox proportional hazards model was significant, but the two-group comparison was insignificant, and vice versa. 2) The randomization test and cross-validation were generally consistent with the p-values obtained from the standard performance metrics. 3) Binary classifiers highly depended on how the risk groups were defined; a slight change of the survival threshold for assignment of classes led to very different prediction results. Conclusions: 1) Different performance metrics for evaluation of a survival prediction model may give different conclusions in its discriminatory ability. 2) Evaluation using a high-risk versus low-risk group comparison depends on the selected risk-score threshold; a plot of p-values from all possible thresholds can show the sensitivity of the threshold selection. 3) A randomization test of the significance of Somers' rank correlation can be used for further evaluation of performance of a prediction model. 4) The cross-validated power of survival prediction models decreases as the training and test sets become less balanced. Background},
author = {Chen, Hung-Chia and Kodell, Ralph L and Cheng, Kuang Fu and Chen, James J},
doi = {10.1186/1471-2288-12-102},
file = {:Users/lisaoshita/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - Assessment of performance of survival prediction models for cancer prognosis(3).pdf:pdf},
journal = {BMC Medical Research Methodology},
number = {1},
pages = {102},
title = {{Assessment of performance of survival prediction models for cancer prognosis}},
url = {https://doi.org/10.1186/1471-2288-12-102},
volume = {12},
year = {2012}
}
@article{Yao2015,
author = {Yao, Yuan and Tong, Hanghang and Xie, Tao and Akoglu, Leman and Xu, Feng and Lu, Jian},
doi = {10.1016/j.ins.2014.12.038},
issn = {0020-0255},
journal = {INFORMATION SCIENCES},
pages = {70--82},
publisher = {Elsevier Inc.},
title = {{Detecting high-quality posts in community question answering sites}},
url = {http://dx.doi.org/10.1016/j.ins.2014.12.038},
volume = {302},
year = {2015}
}
@article{Toba2014,
author = {Toba, Hapnes and Ming, Zhao-yan and Adriani, Mirna and Chua, Tat-seng},
doi = {10.1016/j.ins.2013.10.030},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {question answering system,user generated content},
pages = {101--115},
publisher = {Elsevier Inc.},
title = {{Discovering high quality answers in community question answering archives using a hierarchy of classifiers}},
url = {http://dx.doi.org/10.1016/j.ins.2013.10.030},
volume = {261},
year = {2014}
}

@article{Bhat2014,
author = {Bhat, Vasudev and Gokhale, Adheesh and Jadhav, Ravi and Pudipeddi, Ravi and Akoglu, Leman},
isbn = {9781479958771},
journal = {IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
keywords = {-online communities,behavior,collec-,evidential feature analysis,human,on,probability distribution of the,question answering sites,question response time,response time of questions,tive intelligence,user engagement},
number = {Asonam},
pages = {328--335},
title = {{Min ( e ) d Your Tags : Analysis of Question Response Time in StackOverflow}},
year = {2014}
}
@article{Ponzanelli2014a,
abstract = {Technical questions and answers (Q{\&}A) services have become a valuable resource for developers. A prominent example of technical Q{\&}A website is StackOverflow (SO), which relies on a growing community of more than two millions of users who actively contribute by asking questions and providing answers. To maintain the value of this resource, poor quality questions - among the more than 6,000 asked daily - have to be filtered out. Currently, poor quality questions are manually identified and reviewed by selected users in SO, this costs considerable time and effort. Automating the process would save time and unload the review queue, improving the efficiency of SO as a resource for developers. We present an approach to automate the classification of questions according to their quality. We present an empirical study that investigates how to model and predict the quality of a question by considering as features both the contents of a post (e.g., from simple textual features to more complex readability metrics) and community-related aspects (e.g., popularity of a user in the community). Our findings show that there is indeed the possibility of at least a partial automation of the costly SO review process.},
author = {Ponzanelli, Luca and Mocci, Andrea and Bacchelli, Alberto and Lanza, Michele},
doi = {10.1109/QSIC.2014.27},
isbn = {9781479971978},
issn = {15506002},
journal = {Proceedings - International Conference on Quality Software},
keywords = {Q{\&}A,Quality,StackOverflow,Technical Forum},
pages = {343--352},
title = {{Understanding and classifying the quality of technical forum questions}},
year = {2014}
}

@article{Weimer2007,
abstract = {Assessing the quality of user generated con- tent is an important problem for many web forums. While quality is currently assessed manually, we propose an algorithm to as- sess the quality of forum posts automati- cally and test it on data provided by Nab- ble.com. We use state-of-the-art classifi- cation techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features. We achieve an accuracy of 89{\%} on the task of automatically assessing post quality in the software domain using forum specific fea- tures. Without forum specific features, we achieve an accuracy of 82{\%}.},
author = {Weimer, Markus and Gurevych, Iryna and M{\"{u}}hlh{\"{a}}user, Max},
doi = {10.3115/1557769.1557806},
journal = {Proceedings of the ACL},
number = {June},
pages = {125--128},
title = {{Automatically assessing the post quality in online discussions on software}},
url = {http://atlas.tk.informatik.tu-darmstadt.de/Publications/2007/acl2007.pdf},
year = {2007}
}
@article{Rechavi2011,
abstract = {This study investigates two questions concerning question-and-answer sites. We analyzed data from "Yahoo!Answers", including 19 months and over 20 million interactions per month. The first question investigates the differences in response time and in the average number of answers between anasker's ranking of "Best Answer" (BA) and the community's BA. The second question concerns the impact of an explicit network on several implicit network activities. The results imply that askers use response time as a parameter to choose the BA, whereas the community chooses the BA with no regard to Answer Response Time (ART). Another finding implies that if the answerer is not listed in the asker's "explicit network, " it might result in longer ranking (award) time and in a slightly decreased number of answer stars (satisfaction-rate indicator).And yet, one result might be surprising. Being a "fan" of the asker implies a long response time to the question. This finding might contradict the intuition that our friends are the first to provide answers to our questions. Several explanations of this result from different research fields are suggested in the discussion.},
author = {Rechavi, Amit and Rafaeli, Sheizaf},
doi = {10.1109/PASSAT/SocialCom.2011.67},
isbn = {9780769545783},
journal = {Proceedings - 2011 IEEE International Conference on Privacy, Security, Risk and Trust and IEEE International Conference on Social Computing, PASSAT/SocialCom 2011},
keywords = {Response time,Satisfaction rates,Wisdom of the crowds,Yahoo! answers},
pages = {904--909},
title = {{Not all is gold that glitters response time {\&} satisfaction rates in Yahoo! answers}},
year = {2011}
}
@article{Harper2008,
abstract = {Question and answer (Q{\&}A) sites such as Yahoo! Answers are places where users ask questions and others answer them. In this paper, we investigate predictors of answer quality through a comparative, controlled field study of responses provided across several online Q{\&}A sites. Along with several quantitative results concerning the effects of factors such as question topic and rhetorical strategy, we present two high-level messages. First, you get what you pay for in Q{\&}A sites. Answer quality was typically higher in Google Answers (a fee-based site) than in the free sites we studied, and paying more money for an answer led to better outcomes. Second, we find that a Q{\&}A site's community of users contributes to its success. Yahoo! Answers, a Q{\&}A site where anybody can answer questions, outperformed sites that depend on specific individuals to answer questions, such as library reference services.},
author = {Harper, F Maxwell and Raban, Daphne and Rafaeli, Sheizaf and Konstan, Joseph A},
doi = {10.1145/1357054.1357191},
isbn = {9781605580111},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
keywords = {Q{\&}A,digital reference,expert services,information exchanges,information quality,knowledge networks,online community},
pages = {865--874},
title = {{Predictors of Answer Quality in Online Q{\&}A Sites}},
url = {http://dl.acm.org/citation.cfm?id=1357054.1357191},
year = {2008}
}
@article{Ravi2014,
abstract = {Asking the right question in the right way is an art (and a science). In a community question-answering setting, a good question is not just one that is found to be useful by other people—a question is good if it is also pre-sented clearly and shows prior research. Using a com-munity question-answering site that allows voting over the questions, we show that there is a notion of question quality that goes beyond mere popularity. We present techniques using latent topical models to automatically predict the quality of questions based on their content. Our best system achieves a prediction accuracy of 72{\%}, beating out strong baselines by a significant amount. We also examine the effect of question quality on the dy-namics of user behavior and the longevity of questions.},
author = {Ravi, Sujith and Pang, Bo and Rastagori, VIbhor and Kumar, Ravi},
isbn = {978-1-57735-657-8},
journal = {International AAAI Conference on Weblogs and Social Media},
keywords = {Full Papers},
number = {1},
pages = {426--435},
title = {{Great Question ! Question Quality in Community Q {\&} A}},
year = {2014}
}
